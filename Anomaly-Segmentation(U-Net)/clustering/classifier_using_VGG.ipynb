{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leetw\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('x_for_clustering.npy')\n",
    "# x.shape\n",
    "xx = []\n",
    "for im in x:\n",
    "#     print(im.shape)\n",
    "    xx.append(cv2.resize(im, (224, 224)))\n",
    "xx = np.stack(xx)\n",
    "# xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.006579\n",
       "4     0.009009\n",
       "3     0.020000\n",
       "2     0.037037\n",
       "1     0.052632\n",
       "10    0.066667\n",
       "5     0.090909\n",
       "6     0.200000\n",
       "9     0.333333\n",
       "8     0.333333\n",
       "7     0.333333\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = pd.read_csv('data_for_clustering.csv', header=None)[1]\n",
    "yy.value_counts()\n",
    "w = 1/yy.value_counts()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = OneHotEncoder()\n",
    "y = le.fit_transform(np.expand_dims(yy, axis=1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[yy.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape\n",
    "# y.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c6cc944390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leetw\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "top_model.add(Dense(512, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(11, activation='sigmoid'))\n",
    "model = Model(input=vgg16.input, output=top_model(vgg16.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 11)                4328971   \n",
      "=================================================================\n",
      "Total params: 19,043,659\n",
      "Trainable params: 19,043,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args_img = dict(samplewise_center=True,\n",
    "                     samplewise_std_normalization=True,\n",
    "                     rotation_range=90.,\n",
    "                     width_shift_range=0.2,\n",
    "                     height_shift_range=0.2,\n",
    "                     zoom_range=0.2, \n",
    "                     fill_mode=\"reflect\")\n",
    "image_datagen = ImageDataGenerator(**data_gen_args_img)\n",
    "seed = 7\n",
    "BATCH_SIZE = 8\n",
    "LR_MIN = 1e-9\n",
    "LR_INIT = 1e-3\n",
    "image_datagen.fit(x, augment=True, seed=seed)\n",
    "image_generator = image_datagen.flow(x, y, seed=seed, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model:\n",
    "model.compile(optimizer=Adam(lr = LR_INIT), loss='binary_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint('model-nuclei2018-1.h5', verbose=1, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=LR_MIN, verbose=1) # search \"learning rate\"\n",
    "def sched(epoch, lr):\n",
    "    if epoch%20==0:\n",
    "        lr = lr*0.8\n",
    "    return lr\n",
    "reduce_lr = keras.callbacks.LearningRateScheduler(sched, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 6:33 - loss: 0.8624 - acc: 0.477 - ETA: 3:15 - loss: 0.6841 - acc: 0.613 - ETA: 2:08 - loss: 0.7760 - acc: 0.625 - ETA: 1:35 - loss: 0.7010 - acc: 0.664 - ETA: 1:15 - loss: 0.6503 - acc: 0.688 - ETA: 1:02 - loss: 0.5869 - acc: 0.727 - ETA: 52s - loss: 0.5833 - acc: 0.738 - ETA: 45s - loss: 0.5560 - acc: 0.74 - ETA: 40s - loss: 0.5340 - acc: 0.76 - ETA: 35s - loss: 0.5219 - acc: 0.77 - ETA: 31s - loss: 0.5035 - acc: 0.78 - ETA: 28s - loss: 0.4847 - acc: 0.79 - ETA: 26s - loss: 0.4710 - acc: 0.80 - ETA: 23s - loss: 0.4597 - acc: 0.80 - ETA: 21s - loss: 0.4533 - acc: 0.81 - ETA: 22s - loss: 0.4451 - acc: 0.81 - ETA: 20s - loss: 0.4348 - acc: 0.82 - ETA: 19s - loss: 0.4289 - acc: 0.82 - ETA: 17s - loss: 0.4217 - acc: 0.82 - ETA: 16s - loss: 0.4168 - acc: 0.82 - ETA: 15s - loss: 0.4147 - acc: 0.82 - ETA: 14s - loss: 0.4080 - acc: 0.83 - ETA: 13s - loss: 0.4037 - acc: 0.83 - ETA: 12s - loss: 0.3970 - acc: 0.83 - ETA: 11s - loss: 0.3923 - acc: 0.83 - ETA: 10s - loss: 0.3886 - acc: 0.84 - ETA: 9s - loss: 0.3852 - acc: 0.8429 - ETA: 9s - loss: 0.3801 - acc: 0.844 - ETA: 8s - loss: 0.3746 - acc: 0.847 - ETA: 7s - loss: 0.3708 - acc: 0.849 - ETA: 7s - loss: 0.3677 - acc: 0.851 - ETA: 6s - loss: 0.3650 - acc: 0.852 - ETA: 6s - loss: 0.3616 - acc: 0.854 - ETA: 5s - loss: 0.3609 - acc: 0.856 - ETA: 5s - loss: 0.3598 - acc: 0.857 - ETA: 4s - loss: 0.3562 - acc: 0.858 - ETA: 4s - loss: 0.3525 - acc: 0.860 - ETA: 3s - loss: 0.3525 - acc: 0.860 - ETA: 3s - loss: 0.3516 - acc: 0.860 - ETA: 3s - loss: 0.3510 - acc: 0.861 - ETA: 2s - loss: 0.3494 - acc: 0.862 - ETA: 2s - loss: 0.3467 - acc: 0.862 - ETA: 1s - loss: 0.3437 - acc: 0.863 - ETA: 1s - loss: 0.3406 - acc: 0.864 - ETA: 1s - loss: 0.3399 - acc: 0.864 - ETA: 0s - loss: 0.3370 - acc: 0.865 - ETA: 0s - loss: 0.3343 - acc: 0.866 - ETA: 0s - loss: 0.3314 - acc: 0.868 - 14s 293ms/step - loss: 0.3307 - acc: 0.8682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leetw\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\callbacks.py:435: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2655 - acc: 0.886 - ETA: 4s - loss: 0.2243 - acc: 0.892 - ETA: 4s - loss: 0.2262 - acc: 0.901 - ETA: 4s - loss: 0.2196 - acc: 0.900 - ETA: 4s - loss: 0.2423 - acc: 0.897 - ETA: 4s - loss: 0.2431 - acc: 0.895 - ETA: 4s - loss: 0.2545 - acc: 0.896 - ETA: 4s - loss: 0.2648 - acc: 0.889 - ETA: 4s - loss: 0.2699 - acc: 0.890 - ETA: 4s - loss: 0.2747 - acc: 0.889 - ETA: 3s - loss: 0.2704 - acc: 0.892 - ETA: 3s - loss: 0.2690 - acc: 0.893 - ETA: 3s - loss: 0.2662 - acc: 0.892 - ETA: 3s - loss: 0.2635 - acc: 0.894 - ETA: 3s - loss: 0.2604 - acc: 0.896 - ETA: 3s - loss: 0.2598 - acc: 0.897 - ETA: 3s - loss: 0.2617 - acc: 0.897 - ETA: 3s - loss: 0.2593 - acc: 0.899 - ETA: 3s - loss: 0.2590 - acc: 0.898 - ETA: 3s - loss: 0.2526 - acc: 0.901 - ETA: 2s - loss: 0.2523 - acc: 0.902 - ETA: 2s - loss: 0.2543 - acc: 0.900 - ETA: 2s - loss: 0.2570 - acc: 0.899 - ETA: 2s - loss: 0.2564 - acc: 0.900 - ETA: 2s - loss: 0.2557 - acc: 0.900 - ETA: 2s - loss: 0.2546 - acc: 0.900 - ETA: 2s - loss: 0.2522 - acc: 0.901 - ETA: 2s - loss: 0.2561 - acc: 0.900 - ETA: 2s - loss: 0.2549 - acc: 0.899 - ETA: 1s - loss: 0.2539 - acc: 0.900 - ETA: 1s - loss: 0.2534 - acc: 0.899 - ETA: 1s - loss: 0.2541 - acc: 0.898 - ETA: 1s - loss: 0.2538 - acc: 0.899 - ETA: 1s - loss: 0.2548 - acc: 0.898 - ETA: 1s - loss: 0.2537 - acc: 0.898 - ETA: 1s - loss: 0.2530 - acc: 0.899 - ETA: 1s - loss: 0.2542 - acc: 0.899 - ETA: 1s - loss: 0.2557 - acc: 0.899 - ETA: 1s - loss: 0.2571 - acc: 0.899 - ETA: 0s - loss: 0.2572 - acc: 0.899 - ETA: 0s - loss: 0.2570 - acc: 0.899 - ETA: 0s - loss: 0.2559 - acc: 0.899 - ETA: 0s - loss: 0.2539 - acc: 0.900 - ETA: 0s - loss: 0.2540 - acc: 0.900 - ETA: 0s - loss: 0.2547 - acc: 0.900 - ETA: 0s - loss: 0.2554 - acc: 0.900 - ETA: 0s - loss: 0.2548 - acc: 0.900 - ETA: 0s - loss: 0.2550 - acc: 0.900 - 5s 104ms/step - loss: 0.2541 - acc: 0.9004\n",
      "Epoch 3/10000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2552 - acc: 0.897 - ETA: 4s - loss: 0.2261 - acc: 0.914 - ETA: 4s - loss: 0.2239 - acc: 0.912 - ETA: 4s - loss: 0.2316 - acc: 0.903 - ETA: 4s - loss: 0.2398 - acc: 0.906 - ETA: 4s - loss: 0.2307 - acc: 0.903 - ETA: 4s - loss: 0.2323 - acc: 0.902 - ETA: 4s - loss: 0.2274 - acc: 0.903 - ETA: 4s - loss: 0.2367 - acc: 0.899 - ETA: 4s - loss: 0.2376 - acc: 0.898 - ETA: 3s - loss: 0.2339 - acc: 0.901 - ETA: 3s - loss: 0.2395 - acc: 0.901 - ETA: 3s - loss: 0.2373 - acc: 0.902 - ETA: 3s - loss: 0.2376 - acc: 0.903 - ETA: 3s - loss: 0.2393 - acc: 0.901 - ETA: 3s - loss: 0.2418 - acc: 0.903 - ETA: 3s - loss: 0.2400 - acc: 0.904 - ETA: 3s - loss: 0.2393 - acc: 0.906 - ETA: 3s - loss: 0.2384 - acc: 0.906 - ETA: 3s - loss: 0.2370 - acc: 0.907 - ETA: 2s - loss: 0.2380 - acc: 0.907 - ETA: 2s - loss: 0.2379 - acc: 0.909 - ETA: 2s - loss: 0.2374 - acc: 0.909 - ETA: 2s - loss: 0.2351 - acc: 0.909 - ETA: 2s - loss: 0.2334 - acc: 0.909 - ETA: 2s - loss: 0.2329 - acc: 0.910 - ETA: 2s - loss: 0.2321 - acc: 0.909 - ETA: 2s - loss: 0.2303 - acc: 0.909 - ETA: 2s - loss: 0.2331 - acc: 0.909 - ETA: 1s - loss: 0.2342 - acc: 0.908 - ETA: 1s - loss: 0.2362 - acc: 0.908 - ETA: 1s - loss: 0.2349 - acc: 0.909 - ETA: 1s - loss: 0.2342 - acc: 0.909 - ETA: 1s - loss: 0.2329 - acc: 0.909 - ETA: 1s - loss: 0.2314 - acc: 0.909 - ETA: 1s - loss: 0.2307 - acc: 0.909 - ETA: 1s - loss: 0.2311 - acc: 0.908 - ETA: 1s - loss: 0.2310 - acc: 0.908 - ETA: 1s - loss: 0.2359 - acc: 0.906 - ETA: 0s - loss: 0.2381 - acc: 0.904 - ETA: 0s - loss: 0.2402 - acc: 0.904 - ETA: 0s - loss: 0.2402 - acc: 0.904 - ETA: 0s - loss: 0.2420 - acc: 0.904 - ETA: 0s - loss: 0.2422 - acc: 0.904 - ETA: 0s - loss: 0.2432 - acc: 0.904 - ETA: 0s - loss: 0.2440 - acc: 0.903 - ETA: 0s - loss: 0.2437 - acc: 0.903 - ETA: 0s - loss: 0.2448 - acc: 0.903 - 5s 104ms/step - loss: 0.2446 - acc: 0.9041\n",
      "Epoch 4/10000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2814 - acc: 0.897 - ETA: 4s - loss: 0.2380 - acc: 0.903 - ETA: 4s - loss: 0.2539 - acc: 0.893 - ETA: 4s - loss: 0.2615 - acc: 0.897 - ETA: 4s - loss: 0.2486 - acc: 0.900 - ETA: 4s - loss: 0.2462 - acc: 0.899 - ETA: 4s - loss: 0.2481 - acc: 0.902 - ETA: 4s - loss: 0.2346 - acc: 0.904 - ETA: 4s - loss: 0.2404 - acc: 0.902 - ETA: 4s - loss: 0.2342 - acc: 0.903 - ETA: 3s - loss: 0.2354 - acc: 0.903 - ETA: 3s - loss: 0.2353 - acc: 0.904 - ETA: 3s - loss: 0.2352 - acc: 0.903 - ETA: 3s - loss: 0.2379 - acc: 0.902 - ETA: 3s - loss: 0.2350 - acc: 0.903 - ETA: 3s - loss: 0.2378 - acc: 0.900 - ETA: 3s - loss: 0.2370 - acc: 0.901 - ETA: 3s - loss: 0.2342 - acc: 0.904 - ETA: 3s - loss: 0.2377 - acc: 0.904 - ETA: 3s - loss: 0.2373 - acc: 0.903 - ETA: 2s - loss: 0.2368 - acc: 0.904 - ETA: 2s - loss: 0.2424 - acc: 0.903 - ETA: 2s - loss: 0.2412 - acc: 0.902 - ETA: 2s - loss: 0.2433 - acc: 0.901 - ETA: 2s - loss: 0.2445 - acc: 0.902 - ETA: 2s - loss: 0.2432 - acc: 0.900 - ETA: 2s - loss: 0.2433 - acc: 0.901 - ETA: 2s - loss: 0.2426 - acc: 0.902 - ETA: 2s - loss: 0.2410 - acc: 0.902 - ETA: 1s - loss: 0.2408 - acc: 0.903 - ETA: 1s - loss: 0.2400 - acc: 0.902 - ETA: 1s - loss: 0.2402 - acc: 0.902 - ETA: 1s - loss: 0.2408 - acc: 0.902 - ETA: 1s - loss: 0.2403 - acc: 0.902 - ETA: 1s - loss: 0.2395 - acc: 0.902 - ETA: 1s - loss: 0.2388 - acc: 0.902 - ETA: 1s - loss: 0.2393 - acc: 0.902 - ETA: 1s - loss: 0.2401 - acc: 0.902 - ETA: 1s - loss: 0.2433 - acc: 0.901 - ETA: 0s - loss: 0.2429 - acc: 0.901 - ETA: 0s - loss: 0.2433 - acc: 0.901 - ETA: 0s - loss: 0.2431 - acc: 0.901 - ETA: 0s - loss: 0.2424 - acc: 0.901 - ETA: 0s - loss: 0.2419 - acc: 0.901 - ETA: 0s - loss: 0.2423 - acc: 0.901 - ETA: 0s - loss: 0.2421 - acc: 0.901 - ETA: 0s - loss: 0.2402 - acc: 0.902 - ETA: 0s - loss: 0.2392 - acc: 0.902 - 5s 104ms/step - loss: 0.2390 - acc: 0.9028\n",
      "Epoch 5/10000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2764 - acc: 0.909 - ETA: 4s - loss: 0.2385 - acc: 0.914 - ETA: 4s - loss: 0.2111 - acc: 0.916 - ETA: 4s - loss: 0.2283 - acc: 0.914 - ETA: 4s - loss: 0.2285 - acc: 0.913 - ETA: 4s - loss: 0.2205 - acc: 0.912 - ETA: 4s - loss: 0.2208 - acc: 0.909 - ETA: 4s - loss: 0.2181 - acc: 0.909 - ETA: 4s - loss: 0.2327 - acc: 0.905 - ETA: 4s - loss: 0.2252 - acc: 0.910 - ETA: 3s - loss: 0.2305 - acc: 0.910 - ETA: 3s - loss: 0.2264 - acc: 0.911 - ETA: 3s - loss: 0.2233 - acc: 0.911 - ETA: 3s - loss: 0.2228 - acc: 0.912 - ETA: 3s - loss: 0.2220 - acc: 0.912 - ETA: 3s - loss: 0.2198 - acc: 0.911 - ETA: 3s - loss: 0.2233 - acc: 0.909 - ETA: 3s - loss: 0.2260 - acc: 0.907 - ETA: 3s - loss: 0.2272 - acc: 0.908 - ETA: 2s - loss: 0.2278 - acc: 0.908 - ETA: 2s - loss: 0.2257 - acc: 0.907 - ETA: 2s - loss: 0.2254 - acc: 0.907 - ETA: 2s - loss: 0.2228 - acc: 0.908 - ETA: 2s - loss: 0.2227 - acc: 0.908 - ETA: 2s - loss: 0.2250 - acc: 0.908 - ETA: 2s - loss: 0.2233 - acc: 0.907 - ETA: 2s - loss: 0.2213 - acc: 0.907 - ETA: 2s - loss: 0.2216 - acc: 0.907 - ETA: 2s - loss: 0.2216 - acc: 0.907 - ETA: 1s - loss: 0.2208 - acc: 0.908 - ETA: 1s - loss: 0.2210 - acc: 0.907 - ETA: 1s - loss: 0.2197 - acc: 0.907 - ETA: 1s - loss: 0.2209 - acc: 0.907 - ETA: 1s - loss: 0.2190 - acc: 0.908 - ETA: 1s - loss: 0.2178 - acc: 0.908 - ETA: 1s - loss: 0.2190 - acc: 0.908 - ETA: 1s - loss: 0.2181 - acc: 0.908 - ETA: 1s - loss: 0.2185 - acc: 0.908 - ETA: 1s - loss: 0.2180 - acc: 0.907 - ETA: 0s - loss: 0.2158 - acc: 0.908 - ETA: 0s - loss: 0.2173 - acc: 0.908 - ETA: 0s - loss: 0.2190 - acc: 0.908 - ETA: 0s - loss: 0.2206 - acc: 0.908 - ETA: 0s - loss: 0.2216 - acc: 0.908 - ETA: 0s - loss: 0.2237 - acc: 0.907 - ETA: 0s - loss: 0.2243 - acc: 0.907 - ETA: 0s - loss: 0.2236 - acc: 0.908 - ETA: 0s - loss: 0.2245 - acc: 0.908 - 5s 102ms/step - loss: 0.2252 - acc: 0.9086\n",
      "Epoch 6/10000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1821 - acc: 0.931 - ETA: 4s - loss: 0.1882 - acc: 0.926 - ETA: 4s - loss: 0.1811 - acc: 0.931 - ETA: 4s - loss: 0.1982 - acc: 0.926 - ETA: 4s - loss: 0.1973 - acc: 0.922 - ETA: 4s - loss: 0.1838 - acc: 0.928 - ETA: 4s - loss: 0.1819 - acc: 0.925 - ETA: 4s - loss: 0.1913 - acc: 0.924 - ETA: 4s - loss: 0.1919 - acc: 0.921 - ETA: 4s - loss: 0.1929 - acc: 0.919 - ETA: 4s - loss: 0.1895 - acc: 0.920 - ETA: 3s - loss: 0.1925 - acc: 0.920 - ETA: 3s - loss: 0.1891 - acc: 0.922 - ETA: 3s - loss: 0.1922 - acc: 0.922 - ETA: 3s - loss: 0.1921 - acc: 0.922 - ETA: 3s - loss: 0.1907 - acc: 0.920 - ETA: 3s - loss: 0.1946 - acc: 0.919 - ETA: 3s - loss: 0.1960 - acc: 0.919 - ETA: 3s - loss: 0.1960 - acc: 0.919 - ETA: 3s - loss: 0.1947 - acc: 0.920 - ETA: 2s - loss: 0.2003 - acc: 0.917 - ETA: 2s - loss: 0.2017 - acc: 0.916 - ETA: 2s - loss: 0.2024 - acc: 0.916 - ETA: 2s - loss: 0.2013 - acc: 0.915 - ETA: 2s - loss: 0.2019 - acc: 0.916 - ETA: 2s - loss: 0.2023 - acc: 0.915 - ETA: 2s - loss: 0.2020 - acc: 0.915 - ETA: 2s - loss: 0.2007 - acc: 0.915 - ETA: 2s - loss: 0.1992 - acc: 0.916 - ETA: 1s - loss: 0.2017 - acc: 0.916 - ETA: 1s - loss: 0.2004 - acc: 0.916 - ETA: 1s - loss: 0.1992 - acc: 0.916 - ETA: 1s - loss: 0.1996 - acc: 0.915 - ETA: 1s - loss: 0.1999 - acc: 0.915 - ETA: 1s - loss: 0.2008 - acc: 0.915 - ETA: 1s - loss: 0.2014 - acc: 0.914 - ETA: 1s - loss: 0.2020 - acc: 0.915 - ETA: 1s - loss: 0.2025 - acc: 0.914 - ETA: 1s - loss: 0.2031 - acc: 0.914 - ETA: 0s - loss: 0.2043 - acc: 0.913 - ETA: 0s - loss: 0.2048 - acc: 0.913 - ETA: 0s - loss: 0.2053 - acc: 0.913 - ETA: 0s - loss: 0.2049 - acc: 0.913 - ETA: 0s - loss: 0.2052 - acc: 0.913 - ETA: 0s - loss: 0.2061 - acc: 0.914 - ETA: 0s - loss: 0.2061 - acc: 0.914 - ETA: 0s - loss: 0.2057 - acc: 0.914 - ETA: 0s - loss: 0.2080 - acc: 0.913 - 5s 104ms/step - loss: 0.2071 - acc: 0.9135\n",
      "Epoch 7/10000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2548 - acc: 0.920 - ETA: 4s - loss: 0.2579 - acc: 0.914 - ETA: 4s - loss: 0.2604 - acc: 0.905 - ETA: 4s - loss: 0.2775 - acc: 0.906 - ETA: 4s - loss: 0.2538 - acc: 0.909 - ETA: 4s - loss: 0.2548 - acc: 0.907 - ETA: 4s - loss: 0.2573 - acc: 0.907 - ETA: 4s - loss: 0.2485 - acc: 0.909 - ETA: 4s - loss: 0.2447 - acc: 0.909 - ETA: 3s - loss: 0.2527 - acc: 0.903 - ETA: 3s - loss: 0.2465 - acc: 0.903 - ETA: 3s - loss: 0.2401 - acc: 0.907 - ETA: 3s - loss: 0.2345 - acc: 0.910 - ETA: 3s - loss: 0.2342 - acc: 0.909 - ETA: 3s - loss: 0.2315 - acc: 0.909 - ETA: 3s - loss: 0.2272 - acc: 0.911 - ETA: 3s - loss: 0.2295 - acc: 0.910 - ETA: 3s - loss: 0.2274 - acc: 0.911 - ETA: 3s - loss: 0.2331 - acc: 0.908 - ETA: 2s - loss: 0.2318 - acc: 0.908 - ETA: 2s - loss: 0.2320 - acc: 0.907 - ETA: 2s - loss: 0.2323 - acc: 0.906 - ETA: 2s - loss: 0.2330 - acc: 0.906 - ETA: 2s - loss: 0.2336 - acc: 0.906 - ETA: 2s - loss: 0.2323 - acc: 0.907 - ETA: 2s - loss: 0.2313 - acc: 0.907 - ETA: 2s - loss: 0.2318 - acc: 0.907 - ETA: 2s - loss: 0.2281 - acc: 0.908 - ETA: 2s - loss: 0.2269 - acc: 0.909 - ETA: 1s - loss: 0.2328 - acc: 0.908 - ETA: 1s - loss: 0.2316 - acc: 0.908 - ETA: 1s - loss: 0.2311 - acc: 0.907 - ETA: 1s - loss: 0.2317 - acc: 0.907 - ETA: 1s - loss: 0.2304 - acc: 0.908 - ETA: 1s - loss: 0.2303 - acc: 0.909 - ETA: 1s - loss: 0.2278 - acc: 0.910 - ETA: 1s - loss: 0.2277 - acc: 0.910 - ETA: 1s - loss: 0.2270 - acc: 0.910 - ETA: 1s - loss: 0.2279 - acc: 0.909 - ETA: 0s - loss: 0.2276 - acc: 0.909 - ETA: 0s - loss: 0.2265 - acc: 0.910 - ETA: 0s - loss: 0.2253 - acc: 0.910 - ETA: 0s - loss: 0.2253 - acc: 0.910 - ETA: 0s - loss: 0.2249 - acc: 0.910 - ETA: 0s - loss: 0.2241 - acc: 0.910 - ETA: 0s - loss: 0.2239 - acc: 0.910 - ETA: 0s - loss: 0.2231 - acc: 0.910 - ETA: 0s - loss: 0.2217 - acc: 0.911 - 5s 102ms/step - loss: 0.2217 - acc: 0.9113\n",
      "Epoch 8/10000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2717 - acc: 0.909 - ETA: 4s - loss: 0.2241 - acc: 0.909 - ETA: 4s - loss: 0.2039 - acc: 0.916 - ETA: 4s - loss: 0.2082 - acc: 0.911 - ETA: 4s - loss: 0.2183 - acc: 0.904 - ETA: 4s - loss: 0.2179 - acc: 0.903 - ETA: 4s - loss: 0.2155 - acc: 0.904 - ETA: 4s - loss: 0.2144 - acc: 0.903 - ETA: 4s - loss: 0.2135 - acc: 0.906 - ETA: 4s - loss: 0.2094 - acc: 0.906 - ETA: 3s - loss: 0.2054 - acc: 0.907 - ETA: 3s - loss: 0.2043 - acc: 0.906 - ETA: 3s - loss: 0.2061 - acc: 0.906 - ETA: 3s - loss: 0.2063 - acc: 0.908 - ETA: 3s - loss: 0.2079 - acc: 0.909 - ETA: 3s - loss: 0.2031 - acc: 0.913 - ETA: 3s - loss: 0.2027 - acc: 0.913 - ETA: 3s - loss: 0.2015 - acc: 0.913 - ETA: 3s - loss: 0.2003 - acc: 0.915 - ETA: 2s - loss: 0.1992 - acc: 0.914 - ETA: 2s - loss: 0.1997 - acc: 0.913 - ETA: 2s - loss: 0.1967 - acc: 0.914 - ETA: 2s - loss: 0.1952 - acc: 0.914 - ETA: 2s - loss: 0.1930 - acc: 0.915 - ETA: 2s - loss: 0.1907 - acc: 0.917 - ETA: 2s - loss: 0.1918 - acc: 0.917 - ETA: 2s - loss: 0.1912 - acc: 0.918 - ETA: 2s - loss: 0.1930 - acc: 0.918 - ETA: 2s - loss: 0.1947 - acc: 0.918 - ETA: 1s - loss: 0.1941 - acc: 0.917 - ETA: 1s - loss: 0.1955 - acc: 0.916 - ETA: 1s - loss: 0.1981 - acc: 0.915 - ETA: 1s - loss: 0.1986 - acc: 0.915 - ETA: 1s - loss: 0.1994 - acc: 0.914 - ETA: 1s - loss: 0.1996 - acc: 0.914 - ETA: 1s - loss: 0.1984 - acc: 0.914 - ETA: 1s - loss: 0.1975 - acc: 0.915 - ETA: 1s - loss: 0.1984 - acc: 0.915 - ETA: 1s - loss: 0.2003 - acc: 0.914 - ETA: 0s - loss: 0.2000 - acc: 0.914 - ETA: 0s - loss: 0.2002 - acc: 0.915 - ETA: 0s - loss: 0.2005 - acc: 0.914 - ETA: 0s - loss: 0.1986 - acc: 0.915 - ETA: 0s - loss: 0.1982 - acc: 0.915 - ETA: 0s - loss: 0.1983 - acc: 0.914 - ETA: 0s - loss: 0.1996 - acc: 0.915 - ETA: 0s - loss: 0.2012 - acc: 0.915 - ETA: 0s - loss: 0.2007 - acc: 0.915 - 5s 102ms/step - loss: 0.2016 - acc: 0.9149\n",
      "Epoch 9/10000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2248 - acc: 0.875 - ETA: 4s - loss: 0.1926 - acc: 0.897 - ETA: 4s - loss: 0.1798 - acc: 0.905 - ETA: 4s - loss: 0.1734 - acc: 0.909 - ETA: 4s - loss: 0.1809 - acc: 0.909 - ETA: 4s - loss: 0.1915 - acc: 0.911 - ETA: 4s - loss: 0.1849 - acc: 0.912 - ETA: 4s - loss: 0.1800 - acc: 0.910 - ETA: 4s - loss: 0.1825 - acc: 0.907 - ETA: 4s - loss: 0.1785 - acc: 0.909 - ETA: 3s - loss: 0.1770 - acc: 0.907 - ETA: 3s - loss: 0.1772 - acc: 0.909 - ETA: 3s - loss: 0.1763 - acc: 0.910 - ETA: 3s - loss: 0.1772 - acc: 0.912 - ETA: 3s - loss: 0.1782 - acc: 0.914 - ETA: 3s - loss: 0.1818 - acc: 0.913 - ETA: 3s - loss: 0.1807 - acc: 0.913 - ETA: 3s - loss: 0.1813 - acc: 0.911 - ETA: 3s - loss: 0.1802 - acc: 0.912 - ETA: 2s - loss: 0.1832 - acc: 0.911 - ETA: 2s - loss: 0.1821 - acc: 0.911 - ETA: 2s - loss: 0.1816 - acc: 0.912 - ETA: 2s - loss: 0.1802 - acc: 0.914 - ETA: 2s - loss: 0.1827 - acc: 0.913 - ETA: 2s - loss: 0.1801 - acc: 0.915 - ETA: 2s - loss: 0.1778 - acc: 0.916 - ETA: 2s - loss: 0.1800 - acc: 0.915 - ETA: 2s - loss: 0.1804 - acc: 0.916 - ETA: 2s - loss: 0.1823 - acc: 0.914 - ETA: 1s - loss: 0.1833 - acc: 0.915 - ETA: 1s - loss: 0.1835 - acc: 0.915 - ETA: 1s - loss: 0.1837 - acc: 0.915 - ETA: 1s - loss: 0.1837 - acc: 0.915 - ETA: 1s - loss: 0.1831 - acc: 0.915 - ETA: 1s - loss: 0.1851 - acc: 0.914 - ETA: 1s - loss: 0.1854 - acc: 0.913 - ETA: 1s - loss: 0.1861 - acc: 0.914 - ETA: 1s - loss: 0.1858 - acc: 0.914 - ETA: 1s - loss: 0.1851 - acc: 0.914 - ETA: 0s - loss: 0.1861 - acc: 0.914 - ETA: 0s - loss: 0.1870 - acc: 0.914 - ETA: 0s - loss: 0.1865 - acc: 0.914 - ETA: 0s - loss: 0.1898 - acc: 0.913 - ETA: 0s - loss: 0.1888 - acc: 0.914 - ETA: 0s - loss: 0.1904 - acc: 0.913 - ETA: 0s - loss: 0.1920 - acc: 0.913 - ETA: 0s - loss: 0.1929 - acc: 0.913 - ETA: 0s - loss: 0.1932 - acc: 0.912 - 5s 104ms/step - loss: 0.1927 - acc: 0.9118\n",
      "Epoch 10/10000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1789 - acc: 0.897 - ETA: 4s - loss: 0.2161 - acc: 0.880 - ETA: 4s - loss: 0.1990 - acc: 0.886 - ETA: 4s - loss: 0.2104 - acc: 0.889 - ETA: 4s - loss: 0.2379 - acc: 0.881 - ETA: 4s - loss: 0.2258 - acc: 0.892 - ETA: 4s - loss: 0.2270 - acc: 0.892 - ETA: 4s - loss: 0.2184 - acc: 0.896 - ETA: 4s - loss: 0.2080 - acc: 0.900 - ETA: 4s - loss: 0.2128 - acc: 0.902 - ETA: 3s - loss: 0.2098 - acc: 0.902 - ETA: 3s - loss: 0.2209 - acc: 0.903 - ETA: 3s - loss: 0.2183 - acc: 0.904 - ETA: 3s - loss: 0.2159 - acc: 0.906 - ETA: 3s - loss: 0.2117 - acc: 0.907 - ETA: 3s - loss: 0.2100 - acc: 0.907 - ETA: 3s - loss: 0.2088 - acc: 0.907 - ETA: 3s - loss: 0.2125 - acc: 0.905 - ETA: 3s - loss: 0.2104 - acc: 0.906 - ETA: 2s - loss: 0.2076 - acc: 0.906 - ETA: 2s - loss: 0.2059 - acc: 0.906 - ETA: 2s - loss: 0.2004 - acc: 0.909 - ETA: 2s - loss: 0.2103 - acc: 0.909 - ETA: 2s - loss: 0.2109 - acc: 0.911 - ETA: 2s - loss: 0.2125 - acc: 0.912 - ETA: 2s - loss: 0.2134 - acc: 0.910 - ETA: 2s - loss: 0.2150 - acc: 0.910 - ETA: 2s - loss: 0.2148 - acc: 0.908 - ETA: 2s - loss: 0.2125 - acc: 0.910 - ETA: 1s - loss: 0.2119 - acc: 0.910 - ETA: 1s - loss: 0.2107 - acc: 0.910 - ETA: 1s - loss: 0.2105 - acc: 0.910 - ETA: 1s - loss: 0.2099 - acc: 0.910 - ETA: 1s - loss: 0.2088 - acc: 0.911 - ETA: 1s - loss: 0.2087 - acc: 0.910 - ETA: 1s - loss: 0.2094 - acc: 0.910 - ETA: 1s - loss: 0.2112 - acc: 0.909 - ETA: 1s - loss: 0.2086 - acc: 0.911 - ETA: 1s - loss: 0.2095 - acc: 0.911 - ETA: 0s - loss: 0.2110 - acc: 0.910 - ETA: 0s - loss: 0.2117 - acc: 0.910 - ETA: 0s - loss: 0.2114 - acc: 0.910 - ETA: 0s - loss: 0.2098 - acc: 0.911 - ETA: 0s - loss: 0.2113 - acc: 0.910 - ETA: 0s - loss: 0.2106 - acc: 0.910 - ETA: 0s - loss: 0.2107 - acc: 0.910 - ETA: 0s - loss: 0.2107 - acc: 0.910 - ETA: 0s - loss: 0.2104 - acc: 0.909 - 5s 103ms/step - loss: 0.2098 - acc: 0.9097\n",
      "Epoch 11/10000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1493 - acc: 0.943 - ETA: 4s - loss: 0.1502 - acc: 0.931 - ETA: 4s - loss: 0.1744 - acc: 0.920 - ETA: 4s - loss: 0.1780 - acc: 0.909 - ETA: 4s - loss: 0.1669 - acc: 0.913 - ETA: 4s - loss: 0.1709 - acc: 0.911 - ETA: 4s - loss: 0.1823 - acc: 0.905 - ETA: 4s - loss: 0.1940 - acc: 0.904 - ETA: 4s - loss: 0.1882 - acc: 0.909 - ETA: 3s - loss: 0.1895 - acc: 0.909 - ETA: 3s - loss: 0.1918 - acc: 0.910 - ETA: 3s - loss: 0.1946 - acc: 0.909 - ETA: 3s - loss: 0.1942 - acc: 0.910 - ETA: 3s - loss: 0.1918 - acc: 0.909 - ETA: 3s - loss: 0.1908 - acc: 0.912 - ETA: 3s - loss: 0.1921 - acc: 0.909 - ETA: 3s - loss: 0.1928 - acc: 0.907 - ETA: 3s - loss: 0.1944 - acc: 0.906 - ETA: 3s - loss: 0.1973 - acc: 0.907 - ETA: 2s - loss: 0.2001 - acc: 0.909 - ETA: 2s - loss: 0.2004 - acc: 0.909 - ETA: 2s - loss: 0.1992 - acc: 0.910 - ETA: 2s - loss: 0.2009 - acc: 0.909 - ETA: 2s - loss: 0.2017 - acc: 0.910 - ETA: 2s - loss: 0.2066 - acc: 0.909 - ETA: 2s - loss: 0.2111 - acc: 0.908 - ETA: 2s - loss: 0.2092 - acc: 0.909 - ETA: 2s - loss: 0.2111 - acc: 0.908 - ETA: 2s - loss: 0.2104 - acc: 0.908 - ETA: 1s - loss: 0.2127 - acc: 0.908 - ETA: 1s - loss: 0.2145 - acc: 0.908 - ETA: 1s - loss: 0.2126 - acc: 0.908 - ETA: 1s - loss: 0.2118 - acc: 0.908 - ETA: 1s - loss: 0.2131 - acc: 0.908 - ETA: 1s - loss: 0.2124 - acc: 0.909 - ETA: 1s - loss: 0.2097 - acc: 0.910 - ETA: 1s - loss: 0.2073 - acc: 0.911 - ETA: 1s - loss: 0.2058 - acc: 0.912 - ETA: 1s - loss: 0.2050 - acc: 0.912 - ETA: 0s - loss: 0.2045 - acc: 0.912 - ETA: 0s - loss: 0.2033 - acc: 0.912 - ETA: 0s - loss: 0.2020 - acc: 0.913 - ETA: 0s - loss: 0.2017 - acc: 0.913 - ETA: 0s - loss: 0.2030 - acc: 0.913 - ETA: 0s - loss: 0.2036 - acc: 0.912 - ETA: 0s - loss: 0.2025 - acc: 0.912 - ETA: 0s - loss: 0.2015 - acc: 0.913 - ETA: 0s - loss: 0.2019 - acc: 0.912 - 5s 102ms/step - loss: 0.2024 - acc: 0.9126\n",
      "Epoch 12/10000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2677 - acc: 0.886 - ETA: 4s - loss: 0.2083 - acc: 0.914 - ETA: 4s - loss: 0.2398 - acc: 0.905 - ETA: 4s - loss: 0.2155 - acc: 0.914 - ETA: 4s - loss: 0.2067 - acc: 0.918 - ETA: 4s - loss: 0.2095 - acc: 0.922 - ETA: 4s - loss: 0.2265 - acc: 0.922 - ETA: 4s - loss: 0.2165 - acc: 0.919 - ETA: 4s - loss: 0.2153 - acc: 0.915 - ETA: 3s - loss: 0.2085 - acc: 0.915 - ETA: 3s - loss: 0.2026 - acc: 0.917 - ETA: 3s - loss: 0.2015 - acc: 0.920 - ETA: 3s - loss: 0.1985 - acc: 0.918 - ETA: 3s - loss: 0.1959 - acc: 0.918 - ETA: 3s - loss: 0.1921 - acc: 0.918 - ETA: 3s - loss: 0.1897 - acc: 0.918 - ETA: 3s - loss: 0.1994 - acc: 0.917 - ETA: 3s - loss: 0.2014 - acc: 0.917 - ETA: 3s - loss: 0.2120 - acc: 0.914 - ETA: 2s - loss: 0.2077 - acc: 0.917 - ETA: 2s - loss: 0.2050 - acc: 0.918 - ETA: 2s - loss: 0.2071 - acc: 0.918 - ETA: 2s - loss: 0.2068 - acc: 0.919 - ETA: 2s - loss: 0.2045 - acc: 0.920 - ETA: 2s - loss: 0.2037 - acc: 0.920 - ETA: 2s - loss: 0.2030 - acc: 0.920 - ETA: 2s - loss: 0.2027 - acc: 0.919 - ETA: 2s - loss: 0.2015 - acc: 0.920 - ETA: 2s - loss: 0.2008 - acc: 0.920 - ETA: 1s - loss: 0.2017 - acc: 0.919 - ETA: 1s - loss: 0.2030 - acc: 0.919 - ETA: 1s - loss: 0.2027 - acc: 0.919 - ETA: 1s - loss: 0.2017 - acc: 0.919 - ETA: 1s - loss: 0.2007 - acc: 0.919 - ETA: 1s - loss: 0.1988 - acc: 0.919 - ETA: 1s - loss: 0.1972 - acc: 0.920 - ETA: 1s - loss: 0.1962 - acc: 0.920 - ETA: 1s - loss: 0.1954 - acc: 0.920 - ETA: 1s - loss: 0.1945 - acc: 0.921 - ETA: 0s - loss: 0.1935 - acc: 0.920 - ETA: 0s - loss: 0.1959 - acc: 0.920 - ETA: 0s - loss: 0.1955 - acc: 0.920 - ETA: 0s - loss: 0.1946 - acc: 0.920 - ETA: 0s - loss: 0.1930 - acc: 0.920 - ETA: 0s - loss: 0.1928 - acc: 0.920 - ETA: 0s - loss: 0.1935 - acc: 0.920 - ETA: 0s - loss: 0.1945 - acc: 0.920 - ETA: 0s - loss: 0.1944 - acc: 0.919 - 5s 103ms/step - loss: 0.1934 - acc: 0.9195\n",
      "Epoch 13/10000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2054 - acc: 0.920 - ETA: 4s - loss: 0.2197 - acc: 0.914 - ETA: 4s - loss: 0.2605 - acc: 0.916 - ETA: 4s - loss: 0.2470 - acc: 0.911 - ETA: 4s - loss: 0.2225 - acc: 0.922 - ETA: 4s - loss: 0.2392 - acc: 0.916 - ETA: 4s - loss: 0.2290 - acc: 0.918 - ETA: 4s - loss: 0.2262 - acc: 0.919 - ETA: 4s - loss: 0.2193 - acc: 0.919 - ETA: 3s - loss: 0.2138 - acc: 0.920 - ETA: 3s - loss: 0.2094 - acc: 0.918 - ETA: 3s - loss: 0.2141 - acc: 0.917 - ETA: 3s - loss: 0.2043 - acc: 0.922 - ETA: 3s - loss: 0.1983 - acc: 0.926 - ETA: 3s - loss: 0.1943 - acc: 0.926 - ETA: 3s - loss: 0.1899 - acc: 0.927 - ETA: 3s - loss: 0.1897 - acc: 0.926 - ETA: 3s - loss: 0.1888 - acc: 0.926 - ETA: 3s - loss: 0.1957 - acc: 0.922 - ETA: 2s - loss: 0.1953 - acc: 0.922 - ETA: 2s - loss: 0.1921 - acc: 0.923 - ETA: 2s - loss: 0.1908 - acc: 0.924 - ETA: 2s - loss: 0.1912 - acc: 0.924 - ETA: 2s - loss: 0.1911 - acc: 0.924 - ETA: 2s - loss: 0.1935 - acc: 0.922 - ETA: 2s - loss: 0.1931 - acc: 0.921 - ETA: 2s - loss: 0.1940 - acc: 0.920 - ETA: 2s - loss: 0.1960 - acc: 0.918 - ETA: 2s - loss: 0.1931 - acc: 0.920 - ETA: 1s - loss: 0.2013 - acc: 0.919 - ETA: 1s - loss: 0.2026 - acc: 0.919 - ETA: 1s - loss: 0.2021 - acc: 0.918 - ETA: 1s - loss: 0.2006 - acc: 0.918 - ETA: 1s - loss: 0.2011 - acc: 0.918 - ETA: 1s - loss: 0.2011 - acc: 0.917 - ETA: 1s - loss: 0.1997 - acc: 0.917 - ETA: 1s - loss: 0.1989 - acc: 0.917 - ETA: 1s - loss: 0.2009 - acc: 0.917 - ETA: 1s - loss: 0.2009 - acc: 0.917 - ETA: 0s - loss: 0.2004 - acc: 0.917 - ETA: 0s - loss: 0.2012 - acc: 0.917 - ETA: 0s - loss: 0.2014 - acc: 0.917 - ETA: 0s - loss: 0.2015 - acc: 0.916 - ETA: 0s - loss: 0.2008 - acc: 0.916 - ETA: 0s - loss: 0.2033 - acc: 0.915 - ETA: 0s - loss: 0.2023 - acc: 0.916 - ETA: 0s - loss: 0.2018 - acc: 0.916 - ETA: 0s - loss: 0.2036 - acc: 0.916 - 5s 102ms/step - loss: 0.2047 - acc: 0.9156\n",
      "Epoch 14/10000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1616 - acc: 0.920 - ETA: 4s - loss: 0.1619 - acc: 0.909 - ETA: 4s - loss: 0.1599 - acc: 0.916 - ETA: 4s - loss: 0.1658 - acc: 0.917 - ETA: 4s - loss: 0.1648 - acc: 0.918 - ETA: 4s - loss: 0.1713 - acc: 0.912 - ETA: 4s - loss: 0.1667 - acc: 0.917 - ETA: 4s - loss: 0.1666 - acc: 0.914 - ETA: 4s - loss: 0.1683 - acc: 0.912 - ETA: 3s - loss: 0.1712 - acc: 0.910 - ETA: 3s - loss: 0.1739 - acc: 0.911 - ETA: 3s - loss: 0.1713 - acc: 0.911 - ETA: 3s - loss: 0.1714 - acc: 0.913 - ETA: 3s - loss: 0.1694 - acc: 0.912 - ETA: 3s - loss: 0.1695 - acc: 0.912 - ETA: 3s - loss: 0.1692 - acc: 0.912 - ETA: 3s - loss: 0.1665 - acc: 0.913 - ETA: 3s - loss: 0.1646 - acc: 0.914 - ETA: 3s - loss: 0.1658 - acc: 0.915 - ETA: 2s - loss: 0.1654 - acc: 0.914 - ETA: 2s - loss: 0.1689 - acc: 0.914 - ETA: 2s - loss: 0.1715 - acc: 0.914 - ETA: 2s - loss: 0.1729 - acc: 0.915 - ETA: 2s - loss: 0.1717 - acc: 0.915 - ETA: 2s - loss: 0.1739 - acc: 0.915 - ETA: 2s - loss: 0.1736 - acc: 0.915 - ETA: 2s - loss: 0.1733 - acc: 0.915 - ETA: 2s - loss: 0.1770 - acc: 0.916 - ETA: 2s - loss: 0.1774 - acc: 0.914 - ETA: 1s - loss: 0.1787 - acc: 0.914 - ETA: 1s - loss: 0.1804 - acc: 0.914 - ETA: 1s - loss: 0.1814 - acc: 0.913 - ETA: 1s - loss: 0.1813 - acc: 0.914 - ETA: 1s - loss: 0.1829 - acc: 0.913 - ETA: 1s - loss: 0.1824 - acc: 0.913 - ETA: 1s - loss: 0.1826 - acc: 0.912 - ETA: 1s - loss: 0.1811 - acc: 0.912 - ETA: 1s - loss: 0.1801 - acc: 0.914 - ETA: 1s - loss: 0.1806 - acc: 0.914 - ETA: 0s - loss: 0.1813 - acc: 0.914 - ETA: 0s - loss: 0.1797 - acc: 0.916 - ETA: 0s - loss: 0.1784 - acc: 0.917 - ETA: 0s - loss: 0.1790 - acc: 0.917 - ETA: 0s - loss: 0.1787 - acc: 0.917 - ETA: 0s - loss: 0.1791 - acc: 0.917 - ETA: 0s - loss: 0.1820 - acc: 0.916 - ETA: 0s - loss: 0.1820 - acc: 0.917 - ETA: 0s - loss: 0.1831 - acc: 0.917 - 5s 102ms/step - loss: 0.1840 - acc: 0.9170\n",
      "Epoch 15/10000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2572 - acc: 0.909 - ETA: 4s - loss: 0.2294 - acc: 0.926 - ETA: 4s - loss: 0.2031 - acc: 0.931 - ETA: 4s - loss: 0.1835 - acc: 0.934 - ETA: 4s - loss: 0.1771 - acc: 0.929 - ETA: 4s - loss: 0.1849 - acc: 0.922 - ETA: 4s - loss: 0.1787 - acc: 0.926 - ETA: 4s - loss: 0.1868 - acc: 0.924 - ETA: 4s - loss: 0.1871 - acc: 0.920 - ETA: 3s - loss: 0.1864 - acc: 0.920 - ETA: 3s - loss: 0.1853 - acc: 0.920 - ETA: 3s - loss: 0.1814 - acc: 0.923 - ETA: 3s - loss: 0.1804 - acc: 0.924 - ETA: 3s - loss: 0.1822 - acc: 0.924 - ETA: 3s - loss: 0.1802 - acc: 0.926 - ETA: 3s - loss: 0.1828 - acc: 0.925 - ETA: 3s - loss: 0.1785 - acc: 0.928 - ETA: 3s - loss: 0.1763 - acc: 0.928 - ETA: 3s - loss: 0.1725 - acc: 0.929 - ETA: 2s - loss: 0.1726 - acc: 0.929 - ETA: 2s - loss: 0.1741 - acc: 0.929 - ETA: 2s - loss: 0.1753 - acc: 0.927 - ETA: 2s - loss: 0.1755 - acc: 0.927 - ETA: 2s - loss: 0.1774 - acc: 0.926 - ETA: 2s - loss: 0.1749 - acc: 0.927 - ETA: 2s - loss: 0.1728 - acc: 0.927 - ETA: 2s - loss: 0.1708 - acc: 0.926 - ETA: 2s - loss: 0.1707 - acc: 0.926 - ETA: 2s - loss: 0.1701 - acc: 0.926 - ETA: 1s - loss: 0.1725 - acc: 0.925 - ETA: 1s - loss: 0.1737 - acc: 0.924 - ETA: 1s - loss: 0.1728 - acc: 0.924 - ETA: 1s - loss: 0.1728 - acc: 0.924 - ETA: 1s - loss: 0.1716 - acc: 0.923 - ETA: 1s - loss: 0.1714 - acc: 0.924 - ETA: 1s - loss: 0.1689 - acc: 0.925 - ETA: 1s - loss: 0.1693 - acc: 0.926 - ETA: 1s - loss: 0.1683 - acc: 0.926 - ETA: 1s - loss: 0.1711 - acc: 0.925 - ETA: 0s - loss: 0.1720 - acc: 0.925 - ETA: 0s - loss: 0.1720 - acc: 0.925 - ETA: 0s - loss: 0.1720 - acc: 0.924 - ETA: 0s - loss: 0.1709 - acc: 0.925 - ETA: 0s - loss: 0.1700 - acc: 0.925 - ETA: 0s - loss: 0.1691 - acc: 0.924 - ETA: 0s - loss: 0.1688 - acc: 0.924 - ETA: 0s - loss: 0.1696 - acc: 0.924 - ETA: 0s - loss: 0.1700 - acc: 0.924 - 5s 102ms/step - loss: 0.1729 - acc: 0.9245\n",
      "Epoch 16/10000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2262 - acc: 0.897 - ETA: 4s - loss: 0.2431 - acc: 0.897 - ETA: 4s - loss: 0.2194 - acc: 0.912 - ETA: 4s - loss: 0.1919 - acc: 0.929 - ETA: 4s - loss: 0.2122 - acc: 0.925 - ETA: 4s - loss: 0.2076 - acc: 0.918 - ETA: 4s - loss: 0.2203 - acc: 0.910 - ETA: 4s - loss: 0.2068 - acc: 0.916 - ETA: 4s - loss: 0.2019 - acc: 0.917 - ETA: 3s - loss: 0.2060 - acc: 0.915 - ETA: 3s - loss: 0.2047 - acc: 0.918 - ETA: 3s - loss: 0.2123 - acc: 0.913 - ETA: 3s - loss: 0.2206 - acc: 0.910 - ETA: 3s - loss: 0.2261 - acc: 0.907 - ETA: 3s - loss: 0.2267 - acc: 0.908 - ETA: 3s - loss: 0.2250 - acc: 0.909 - ETA: 3s - loss: 0.2223 - acc: 0.911 - ETA: 3s - loss: 0.2186 - acc: 0.913 - ETA: 3s - loss: 0.2138 - acc: 0.917 - ETA: 2s - loss: 0.2145 - acc: 0.914 - ETA: 2s - loss: 0.2146 - acc: 0.914 - ETA: 2s - loss: 0.2164 - acc: 0.913 - ETA: 2s - loss: 0.2179 - acc: 0.914 - ETA: 2s - loss: 0.2153 - acc: 0.914 - ETA: 2s - loss: 0.2148 - acc: 0.915 - ETA: 2s - loss: 0.2123 - acc: 0.917 - ETA: 2s - loss: 0.2097 - acc: 0.918 - ETA: 2s - loss: 0.2064 - acc: 0.919 - ETA: 2s - loss: 0.2037 - acc: 0.920 - ETA: 1s - loss: 0.2036 - acc: 0.920 - ETA: 1s - loss: 0.2027 - acc: 0.921 - ETA: 1s - loss: 0.2015 - acc: 0.921 - ETA: 1s - loss: 0.2020 - acc: 0.921 - ETA: 1s - loss: 0.2010 - acc: 0.921 - ETA: 1s - loss: 0.2029 - acc: 0.921 - ETA: 1s - loss: 0.2014 - acc: 0.922 - ETA: 1s - loss: 0.2007 - acc: 0.922 - ETA: 1s - loss: 0.1997 - acc: 0.922 - ETA: 1s - loss: 0.1973 - acc: 0.922 - ETA: 0s - loss: 0.1952 - acc: 0.923 - ETA: 0s - loss: 0.1979 - acc: 0.923 - ETA: 0s - loss: 0.1974 - acc: 0.922 - ETA: 0s - loss: 0.1971 - acc: 0.922 - ETA: 0s - loss: 0.1975 - acc: 0.921 - ETA: 0s - loss: 0.1969 - acc: 0.921 - ETA: 0s - loss: 0.1972 - acc: 0.921 - ETA: 0s - loss: 0.1980 - acc: 0.921 - ETA: 0s - loss: 0.1992 - acc: 0.920 - 5s 102ms/step - loss: 0.1989 - acc: 0.9200\n",
      "Epoch 17/10000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1808 - acc: 0.897 - ETA: 4s - loss: 0.1667 - acc: 0.937 - ETA: 4s - loss: 0.1814 - acc: 0.924 - ETA: 4s - loss: 0.1719 - acc: 0.920 - ETA: 4s - loss: 0.1789 - acc: 0.911 - ETA: 4s - loss: 0.1963 - acc: 0.909 - ETA: 4s - loss: 0.1852 - acc: 0.915 - ETA: 4s - loss: 0.1926 - acc: 0.916 - ETA: 4s - loss: 0.1855 - acc: 0.917 - ETA: 3s - loss: 0.1945 - acc: 0.917 - ETA: 3s - loss: 0.1912 - acc: 0.919 - ETA: 3s - loss: 0.1908 - acc: 0.916 - ETA: 3s - loss: 0.1932 - acc: 0.916 - ETA: 3s - loss: 0.1945 - acc: 0.916 - ETA: 3s - loss: 0.1919 - acc: 0.918 - ETA: 3s - loss: 0.1892 - acc: 0.918 - ETA: 3s - loss: 0.1902 - acc: 0.917 - ETA: 3s - loss: 0.1908 - acc: 0.917 - ETA: 3s - loss: 0.1924 - acc: 0.916 - ETA: 2s - loss: 0.1918 - acc: 0.916 - ETA: 2s - loss: 0.1930 - acc: 0.916 - ETA: 2s - loss: 0.1922 - acc: 0.915 - ETA: 2s - loss: 0.1920 - acc: 0.914 - ETA: 2s - loss: 0.1906 - acc: 0.916 - ETA: 2s - loss: 0.1893 - acc: 0.915 - ETA: 2s - loss: 0.1904 - acc: 0.915 - ETA: 2s - loss: 0.1902 - acc: 0.915 - ETA: 2s - loss: 0.1875 - acc: 0.916 - ETA: 2s - loss: 0.1868 - acc: 0.916 - ETA: 1s - loss: 0.1865 - acc: 0.917 - ETA: 1s - loss: 0.1900 - acc: 0.915 - ETA: 1s - loss: 0.1884 - acc: 0.916 - ETA: 1s - loss: 0.1892 - acc: 0.916 - ETA: 1s - loss: 0.1869 - acc: 0.917 - ETA: 1s - loss: 0.1864 - acc: 0.917 - ETA: 1s - loss: 0.1844 - acc: 0.918 - ETA: 1s - loss: 0.1840 - acc: 0.918 - ETA: 1s - loss: 0.1828 - acc: 0.919 - ETA: 1s - loss: 0.1830 - acc: 0.919 - ETA: 0s - loss: 0.1862 - acc: 0.919 - ETA: 0s - loss: 0.1868 - acc: 0.918 - ETA: 0s - loss: 0.1863 - acc: 0.918 - ETA: 0s - loss: 0.1879 - acc: 0.918 - ETA: 0s - loss: 0.1881 - acc: 0.917 - ETA: 0s - loss: 0.1881 - acc: 0.918 - ETA: 0s - loss: 0.1891 - acc: 0.917 - ETA: 0s - loss: 0.1889 - acc: 0.918 - ETA: 0s - loss: 0.1883 - acc: 0.918 - 5s 102ms/step - loss: 0.1866 - acc: 0.9196\n",
      "Epoch 18/10000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1264 - acc: 0.920 - ETA: 4s - loss: 0.1734 - acc: 0.903 - ETA: 4s - loss: 0.1769 - acc: 0.916 - ETA: 4s - loss: 0.1792 - acc: 0.917 - ETA: 4s - loss: 0.2097 - acc: 0.915 - ETA: 4s - loss: 0.2137 - acc: 0.914 - ETA: 4s - loss: 0.2037 - acc: 0.918 - ETA: 4s - loss: 0.2148 - acc: 0.916 - ETA: 4s - loss: 0.2146 - acc: 0.914 - ETA: 3s - loss: 0.2144 - acc: 0.913 - ETA: 3s - loss: 0.2147 - acc: 0.910 - ETA: 3s - loss: 0.2091 - acc: 0.910 - ETA: 3s - loss: 0.2100 - acc: 0.910 - ETA: 3s - loss: 0.2043 - acc: 0.912 - ETA: 3s - loss: 0.2035 - acc: 0.915 - ETA: 3s - loss: 0.2014 - acc: 0.914 - ETA: 3s - loss: 0.1990 - acc: 0.914 - ETA: 3s - loss: 0.2001 - acc: 0.915 - ETA: 3s - loss: 0.2002 - acc: 0.915 - ETA: 2s - loss: 0.2011 - acc: 0.915 - ETA: 2s - loss: 0.2006 - acc: 0.915 - ETA: 2s - loss: 0.2002 - acc: 0.915 - ETA: 2s - loss: 0.1979 - acc: 0.915 - ETA: 2s - loss: 0.1975 - acc: 0.914 - ETA: 2s - loss: 0.1972 - acc: 0.914 - ETA: 2s - loss: 0.1993 - acc: 0.913 - ETA: 2s - loss: 0.1968 - acc: 0.914 - ETA: 2s - loss: 0.1969 - acc: 0.915 - ETA: 2s - loss: 0.1942 - acc: 0.916 - ETA: 1s - loss: 0.1941 - acc: 0.916 - ETA: 1s - loss: 0.1922 - acc: 0.917 - ETA: 1s - loss: 0.1901 - acc: 0.917 - ETA: 1s - loss: 0.1910 - acc: 0.917 - ETA: 1s - loss: 0.1914 - acc: 0.917 - ETA: 1s - loss: 0.1917 - acc: 0.917 - ETA: 1s - loss: 0.1907 - acc: 0.917 - ETA: 1s - loss: 0.1936 - acc: 0.917 - ETA: 1s - loss: 0.1932 - acc: 0.916 - ETA: 1s - loss: 0.1955 - acc: 0.915 - ETA: 0s - loss: 0.1941 - acc: 0.916 - ETA: 0s - loss: 0.1918 - acc: 0.918 - ETA: 0s - loss: 0.1910 - acc: 0.918 - ETA: 0s - loss: 0.1907 - acc: 0.917 - ETA: 0s - loss: 0.1913 - acc: 0.917 - ETA: 0s - loss: 0.1900 - acc: 0.917 - ETA: 0s - loss: 0.1904 - acc: 0.917 - ETA: 0s - loss: 0.1905 - acc: 0.917 - ETA: 0s - loss: 0.1906 - acc: 0.917 - 5s 101ms/step - loss: 0.1891 - acc: 0.9184\n",
      "Epoch 19/10000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1137 - acc: 0.935 - ETA: 4s - loss: 0.1794 - acc: 0.922 - ETA: 4s - loss: 0.1920 - acc: 0.914 - ETA: 4s - loss: 0.1676 - acc: 0.926 - ETA: 4s - loss: 0.1710 - acc: 0.925 - ETA: 4s - loss: 0.1735 - acc: 0.926 - ETA: 4s - loss: 0.1655 - acc: 0.929 - ETA: 4s - loss: 0.1584 - acc: 0.932 - ETA: 4s - loss: 0.1536 - acc: 0.934 - ETA: 3s - loss: 0.1543 - acc: 0.932 - ETA: 3s - loss: 0.1547 - acc: 0.931 - ETA: 3s - loss: 0.1542 - acc: 0.930 - ETA: 3s - loss: 0.1607 - acc: 0.928 - ETA: 3s - loss: 0.1650 - acc: 0.928 - ETA: 3s - loss: 0.1659 - acc: 0.928 - ETA: 3s - loss: 0.1650 - acc: 0.927 - ETA: 3s - loss: 0.1686 - acc: 0.925 - ETA: 3s - loss: 0.1745 - acc: 0.923 - ETA: 3s - loss: 0.1760 - acc: 0.922 - ETA: 2s - loss: 0.1781 - acc: 0.921 - ETA: 2s - loss: 0.1775 - acc: 0.921 - ETA: 2s - loss: 0.1743 - acc: 0.922 - ETA: 2s - loss: 0.1740 - acc: 0.923 - ETA: 2s - loss: 0.1757 - acc: 0.922 - ETA: 2s - loss: 0.1757 - acc: 0.922 - ETA: 2s - loss: 0.1743 - acc: 0.922 - ETA: 2s - loss: 0.1755 - acc: 0.923 - ETA: 2s - loss: 0.1764 - acc: 0.924 - ETA: 2s - loss: 0.1742 - acc: 0.925 - ETA: 1s - loss: 0.1752 - acc: 0.925 - ETA: 1s - loss: 0.1762 - acc: 0.926 - ETA: 1s - loss: 0.1777 - acc: 0.925 - ETA: 1s - loss: 0.1780 - acc: 0.924 - ETA: 1s - loss: 0.1780 - acc: 0.924 - ETA: 1s - loss: 0.1761 - acc: 0.926 - ETA: 1s - loss: 0.1771 - acc: 0.924 - ETA: 1s - loss: 0.1758 - acc: 0.926 - ETA: 1s - loss: 0.1746 - acc: 0.926 - ETA: 1s - loss: 0.1746 - acc: 0.926 - ETA: 0s - loss: 0.1743 - acc: 0.926 - ETA: 0s - loss: 0.1738 - acc: 0.926 - ETA: 0s - loss: 0.1734 - acc: 0.926 - ETA: 0s - loss: 0.1728 - acc: 0.926 - ETA: 0s - loss: 0.1724 - acc: 0.926 - ETA: 0s - loss: 0.1709 - acc: 0.927 - ETA: 0s - loss: 0.1734 - acc: 0.927 - ETA: 0s - loss: 0.1743 - acc: 0.927 - ETA: 0s - loss: 0.1738 - acc: 0.927 - 5s 102ms/step - loss: 0.1751 - acc: 0.9274\n",
      "Epoch 20/10000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000800000037997961.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1453 - acc: 0.965 - ETA: 4s - loss: 0.1625 - acc: 0.943 - ETA: 4s - loss: 0.1519 - acc: 0.958 - ETA: 4s - loss: 0.1353 - acc: 0.965 - ETA: 4s - loss: 0.1453 - acc: 0.954 - ETA: 4s - loss: 0.1472 - acc: 0.950 - ETA: 4s - loss: 0.1580 - acc: 0.948 - ETA: 4s - loss: 0.1548 - acc: 0.947 - ETA: 4s - loss: 0.1596 - acc: 0.944 - ETA: 4s - loss: 0.1677 - acc: 0.938 - ETA: 3s - loss: 0.1709 - acc: 0.937 - ETA: 3s - loss: 0.1808 - acc: 0.933 - ETA: 3s - loss: 0.1755 - acc: 0.936 - ETA: 3s - loss: 0.1780 - acc: 0.933 - ETA: 3s - loss: 0.1772 - acc: 0.931 - ETA: 3s - loss: 0.1820 - acc: 0.929 - ETA: 3s - loss: 0.1811 - acc: 0.929 - ETA: 3s - loss: 0.1852 - acc: 0.927 - ETA: 3s - loss: 0.1831 - acc: 0.927 - ETA: 3s - loss: 0.1897 - acc: 0.925 - ETA: 2s - loss: 0.1883 - acc: 0.925 - ETA: 2s - loss: 0.1910 - acc: 0.925 - ETA: 2s - loss: 0.1882 - acc: 0.926 - ETA: 2s - loss: 0.1869 - acc: 0.926 - ETA: 2s - loss: 0.1873 - acc: 0.925 - ETA: 2s - loss: 0.1856 - acc: 0.925 - ETA: 2s - loss: 0.1850 - acc: 0.924 - ETA: 2s - loss: 0.1856 - acc: 0.924 - ETA: 2s - loss: 0.1896 - acc: 0.923 - ETA: 1s - loss: 0.1887 - acc: 0.924 - ETA: 1s - loss: 0.1881 - acc: 0.923 - ETA: 1s - loss: 0.1883 - acc: 0.922 - ETA: 1s - loss: 0.1878 - acc: 0.922 - ETA: 1s - loss: 0.1891 - acc: 0.921 - ETA: 1s - loss: 0.1892 - acc: 0.922 - ETA: 1s - loss: 0.1892 - acc: 0.923 - ETA: 1s - loss: 0.1903 - acc: 0.923 - ETA: 1s - loss: 0.1910 - acc: 0.923 - ETA: 1s - loss: 0.1921 - acc: 0.921 - ETA: 0s - loss: 0.1895 - acc: 0.923 - ETA: 0s - loss: 0.1882 - acc: 0.924 - ETA: 0s - loss: 0.1863 - acc: 0.925 - ETA: 0s - loss: 0.1872 - acc: 0.924 - ETA: 0s - loss: 0.1857 - acc: 0.925 - ETA: 0s - loss: 0.1860 - acc: 0.924 - ETA: 0s - loss: 0.1848 - acc: 0.924 - ETA: 0s - loss: 0.1855 - acc: 0.924 - ETA: 0s - loss: 0.1850 - acc: 0.925 - 5s 104ms/step - loss: 0.1853 - acc: 0.9253\n",
      "Epoch 21/10000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0006400000303983689.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1944 - acc: 0.931 - ETA: 4s - loss: 0.1800 - acc: 0.926 - ETA: 4s - loss: 0.1618 - acc: 0.935 - ETA: 4s - loss: 0.1607 - acc: 0.934 - ETA: 4s - loss: 0.1489 - acc: 0.940 - ETA: 4s - loss: 0.1444 - acc: 0.945 - ETA: 4s - loss: 0.1570 - acc: 0.935 - ETA: 4s - loss: 0.1526 - acc: 0.937 - ETA: 4s - loss: 0.1553 - acc: 0.938 - ETA: 3s - loss: 0.1549 - acc: 0.934 - ETA: 3s - loss: 0.1520 - acc: 0.937 - ETA: 3s - loss: 0.1491 - acc: 0.936 - ETA: 3s - loss: 0.1455 - acc: 0.937 - ETA: 3s - loss: 0.1473 - acc: 0.936 - ETA: 3s - loss: 0.1462 - acc: 0.938 - ETA: 3s - loss: 0.1469 - acc: 0.937 - ETA: 3s - loss: 0.1482 - acc: 0.938 - ETA: 3s - loss: 0.1446 - acc: 0.940 - ETA: 3s - loss: 0.1441 - acc: 0.939 - ETA: 2s - loss: 0.1491 - acc: 0.939 - ETA: 2s - loss: 0.1475 - acc: 0.940 - ETA: 2s - loss: 0.1456 - acc: 0.941 - ETA: 2s - loss: 0.1458 - acc: 0.941 - ETA: 2s - loss: 0.1441 - acc: 0.941 - ETA: 2s - loss: 0.1409 - acc: 0.942 - ETA: 2s - loss: 0.1457 - acc: 0.941 - ETA: 2s - loss: 0.1448 - acc: 0.941 - ETA: 2s - loss: 0.1494 - acc: 0.939 - ETA: 2s - loss: 0.1527 - acc: 0.939 - ETA: 1s - loss: 0.1547 - acc: 0.938 - ETA: 1s - loss: 0.1545 - acc: 0.938 - ETA: 1s - loss: 0.1554 - acc: 0.938 - ETA: 1s - loss: 0.1589 - acc: 0.938 - ETA: 1s - loss: 0.1586 - acc: 0.937 - ETA: 1s - loss: 0.1575 - acc: 0.938 - ETA: 1s - loss: 0.1583 - acc: 0.937 - ETA: 1s - loss: 0.1592 - acc: 0.937 - ETA: 1s - loss: 0.1611 - acc: 0.935 - ETA: 1s - loss: 0.1644 - acc: 0.934 - ETA: 0s - loss: 0.1648 - acc: 0.933 - ETA: 0s - loss: 0.1643 - acc: 0.933 - ETA: 0s - loss: 0.1621 - acc: 0.934 - ETA: 0s - loss: 0.1641 - acc: 0.933 - ETA: 0s - loss: 0.1648 - acc: 0.933 - ETA: 0s - loss: 0.1665 - acc: 0.933 - ETA: 0s - loss: 0.1657 - acc: 0.933 - ETA: 0s - loss: 0.1689 - acc: 0.933 - ETA: 0s - loss: 0.1696 - acc: 0.932 - 5s 102ms/step - loss: 0.1700 - acc: 0.9319\n",
      "Epoch 22/10000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1704 - acc: 0.897 - ETA: 4s - loss: 0.1874 - acc: 0.903 - ETA: 4s - loss: 0.1611 - acc: 0.924 - ETA: 4s - loss: 0.1561 - acc: 0.929 - ETA: 4s - loss: 0.1703 - acc: 0.925 - ETA: 4s - loss: 0.1606 - acc: 0.933 - ETA: 4s - loss: 0.1570 - acc: 0.938 - ETA: 4s - loss: 0.1669 - acc: 0.934 - ETA: 4s - loss: 0.1666 - acc: 0.930 - ETA: 3s - loss: 0.1704 - acc: 0.927 - ETA: 3s - loss: 0.1688 - acc: 0.928 - ETA: 3s - loss: 0.1639 - acc: 0.930 - ETA: 3s - loss: 0.1641 - acc: 0.932 - ETA: 3s - loss: 0.1619 - acc: 0.933 - ETA: 3s - loss: 0.1663 - acc: 0.934 - ETA: 3s - loss: 0.1627 - acc: 0.935 - ETA: 3s - loss: 0.1635 - acc: 0.933 - ETA: 3s - loss: 0.1690 - acc: 0.931 - ETA: 3s - loss: 0.1644 - acc: 0.934 - ETA: 2s - loss: 0.1652 - acc: 0.935 - ETA: 2s - loss: 0.1643 - acc: 0.935 - ETA: 2s - loss: 0.1710 - acc: 0.932 - ETA: 2s - loss: 0.1730 - acc: 0.932 - ETA: 2s - loss: 0.1727 - acc: 0.933 - ETA: 2s - loss: 0.1741 - acc: 0.930 - ETA: 2s - loss: 0.1756 - acc: 0.930 - ETA: 2s - loss: 0.1751 - acc: 0.931 - ETA: 2s - loss: 0.1770 - acc: 0.930 - ETA: 2s - loss: 0.1781 - acc: 0.929 - ETA: 1s - loss: 0.1790 - acc: 0.929 - ETA: 1s - loss: 0.1775 - acc: 0.930 - ETA: 1s - loss: 0.1767 - acc: 0.931 - ETA: 1s - loss: 0.1751 - acc: 0.931 - ETA: 1s - loss: 0.1744 - acc: 0.932 - ETA: 1s - loss: 0.1755 - acc: 0.932 - ETA: 1s - loss: 0.1743 - acc: 0.932 - ETA: 1s - loss: 0.1741 - acc: 0.933 - ETA: 1s - loss: 0.1752 - acc: 0.933 - ETA: 1s - loss: 0.1738 - acc: 0.932 - ETA: 0s - loss: 0.1730 - acc: 0.933 - ETA: 0s - loss: 0.1713 - acc: 0.934 - ETA: 0s - loss: 0.1698 - acc: 0.934 - ETA: 0s - loss: 0.1699 - acc: 0.934 - ETA: 0s - loss: 0.1697 - acc: 0.934 - ETA: 0s - loss: 0.1680 - acc: 0.935 - ETA: 0s - loss: 0.1696 - acc: 0.934 - ETA: 0s - loss: 0.1692 - acc: 0.934 - ETA: 0s - loss: 0.1693 - acc: 0.934 - 5s 102ms/step - loss: 0.1700 - acc: 0.9347\n",
      "Epoch 23/10000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1117 - acc: 0.943 - ETA: 4s - loss: 0.1455 - acc: 0.931 - ETA: 4s - loss: 0.1558 - acc: 0.928 - ETA: 4s - loss: 0.1678 - acc: 0.917 - ETA: 4s - loss: 0.1612 - acc: 0.918 - ETA: 4s - loss: 0.1735 - acc: 0.914 - ETA: 4s - loss: 0.1644 - acc: 0.918 - ETA: 4s - loss: 0.1628 - acc: 0.917 - ETA: 4s - loss: 0.1579 - acc: 0.921 - ETA: 3s - loss: 0.1609 - acc: 0.920 - ETA: 3s - loss: 0.1630 - acc: 0.920 - ETA: 3s - loss: 0.1598 - acc: 0.923 - ETA: 3s - loss: 0.1655 - acc: 0.920 - ETA: 3s - loss: 0.1649 - acc: 0.922 - ETA: 3s - loss: 0.1685 - acc: 0.920 - ETA: 3s - loss: 0.1719 - acc: 0.919 - ETA: 3s - loss: 0.1695 - acc: 0.921 - ETA: 3s - loss: 0.1724 - acc: 0.920 - ETA: 3s - loss: 0.1726 - acc: 0.920 - ETA: 2s - loss: 0.1730 - acc: 0.922 - ETA: 2s - loss: 0.1724 - acc: 0.922 - ETA: 2s - loss: 0.1717 - acc: 0.923 - ETA: 2s - loss: 0.1743 - acc: 0.923 - ETA: 2s - loss: 0.1734 - acc: 0.922 - ETA: 2s - loss: 0.1719 - acc: 0.923 - ETA: 2s - loss: 0.1737 - acc: 0.922 - ETA: 2s - loss: 0.1729 - acc: 0.921 - ETA: 2s - loss: 0.1720 - acc: 0.922 - ETA: 2s - loss: 0.1731 - acc: 0.922 - ETA: 1s - loss: 0.1713 - acc: 0.923 - ETA: 1s - loss: 0.1732 - acc: 0.922 - ETA: 1s - loss: 0.1740 - acc: 0.921 - ETA: 1s - loss: 0.1753 - acc: 0.920 - ETA: 1s - loss: 0.1750 - acc: 0.920 - ETA: 1s - loss: 0.1733 - acc: 0.921 - ETA: 1s - loss: 0.1736 - acc: 0.922 - ETA: 1s - loss: 0.1740 - acc: 0.922 - ETA: 1s - loss: 0.1749 - acc: 0.923 - ETA: 1s - loss: 0.1743 - acc: 0.923 - ETA: 0s - loss: 0.1736 - acc: 0.924 - ETA: 0s - loss: 0.1745 - acc: 0.923 - ETA: 0s - loss: 0.1740 - acc: 0.923 - ETA: 0s - loss: 0.1736 - acc: 0.923 - ETA: 0s - loss: 0.1733 - acc: 0.923 - ETA: 0s - loss: 0.1717 - acc: 0.924 - ETA: 0s - loss: 0.1716 - acc: 0.925 - ETA: 0s - loss: 0.1704 - acc: 0.926 - ETA: 0s - loss: 0.1689 - acc: 0.926 - 5s 102ms/step - loss: 0.1685 - acc: 0.9269\n",
      "Epoch 24/10000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1520 - acc: 0.931 - ETA: 4s - loss: 0.1912 - acc: 0.914 - ETA: 4s - loss: 0.1943 - acc: 0.916 - ETA: 4s - loss: 0.2212 - acc: 0.906 - ETA: 4s - loss: 0.2054 - acc: 0.911 - ETA: 4s - loss: 0.2217 - acc: 0.907 - ETA: 4s - loss: 0.2075 - acc: 0.914 - ETA: 4s - loss: 0.2094 - acc: 0.913 - ETA: 4s - loss: 0.2118 - acc: 0.909 - ETA: 3s - loss: 0.1969 - acc: 0.917 - ETA: 3s - loss: 0.1880 - acc: 0.921 - ETA: 3s - loss: 0.1828 - acc: 0.923 - ETA: 3s - loss: 0.1805 - acc: 0.924 - ETA: 3s - loss: 0.1874 - acc: 0.924 - ETA: 3s - loss: 0.1827 - acc: 0.925 - ETA: 3s - loss: 0.1794 - acc: 0.926 - ETA: 3s - loss: 0.1788 - acc: 0.928 - ETA: 3s - loss: 0.1772 - acc: 0.929 - ETA: 3s - loss: 0.1751 - acc: 0.931 - ETA: 2s - loss: 0.1786 - acc: 0.930 - ETA: 2s - loss: 0.1789 - acc: 0.931 - ETA: 2s - loss: 0.1831 - acc: 0.930 - ETA: 2s - loss: 0.1799 - acc: 0.932 - ETA: 2s - loss: 0.1808 - acc: 0.932 - ETA: 2s - loss: 0.1786 - acc: 0.932 - ETA: 2s - loss: 0.1754 - acc: 0.932 - ETA: 2s - loss: 0.1725 - acc: 0.933 - ETA: 2s - loss: 0.1743 - acc: 0.932 - ETA: 2s - loss: 0.1809 - acc: 0.931 - ETA: 1s - loss: 0.1791 - acc: 0.931 - ETA: 1s - loss: 0.1800 - acc: 0.931 - ETA: 1s - loss: 0.1798 - acc: 0.932 - ETA: 1s - loss: 0.1805 - acc: 0.932 - ETA: 1s - loss: 0.1788 - acc: 0.933 - ETA: 1s - loss: 0.1771 - acc: 0.933 - ETA: 1s - loss: 0.1780 - acc: 0.933 - ETA: 1s - loss: 0.1777 - acc: 0.934 - ETA: 1s - loss: 0.1777 - acc: 0.933 - ETA: 1s - loss: 0.1775 - acc: 0.933 - ETA: 0s - loss: 0.1812 - acc: 0.932 - ETA: 0s - loss: 0.1818 - acc: 0.932 - ETA: 0s - loss: 0.1810 - acc: 0.932 - ETA: 0s - loss: 0.1834 - acc: 0.930 - ETA: 0s - loss: 0.1833 - acc: 0.930 - ETA: 0s - loss: 0.1821 - acc: 0.931 - ETA: 0s - loss: 0.1839 - acc: 0.931 - ETA: 0s - loss: 0.1819 - acc: 0.932 - ETA: 0s - loss: 0.1817 - acc: 0.932 - 5s 103ms/step - loss: 0.1827 - acc: 0.9311\n",
      "Epoch 25/10000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2029 - acc: 0.931 - ETA: 4s - loss: 0.1912 - acc: 0.920 - ETA: 4s - loss: 0.1843 - acc: 0.924 - ETA: 4s - loss: 0.1750 - acc: 0.931 - ETA: 4s - loss: 0.1708 - acc: 0.931 - ETA: 4s - loss: 0.1687 - acc: 0.931 - ETA: 4s - loss: 0.1625 - acc: 0.936 - ETA: 4s - loss: 0.1569 - acc: 0.936 - ETA: 4s - loss: 0.1629 - acc: 0.935 - ETA: 3s - loss: 0.1601 - acc: 0.935 - ETA: 3s - loss: 0.1552 - acc: 0.939 - ETA: 3s - loss: 0.1546 - acc: 0.937 - ETA: 3s - loss: 0.1610 - acc: 0.936 - ETA: 3s - loss: 0.1594 - acc: 0.936 - ETA: 3s - loss: 0.1621 - acc: 0.938 - ETA: 3s - loss: 0.1606 - acc: 0.939 - ETA: 3s - loss: 0.1600 - acc: 0.939 - ETA: 3s - loss: 0.1588 - acc: 0.939 - ETA: 3s - loss: 0.1582 - acc: 0.937 - ETA: 2s - loss: 0.1651 - acc: 0.935 - ETA: 2s - loss: 0.1624 - acc: 0.937 - ETA: 2s - loss: 0.1657 - acc: 0.937 - ETA: 2s - loss: 0.1662 - acc: 0.937 - ETA: 2s - loss: 0.1653 - acc: 0.938 - ETA: 2s - loss: 0.1651 - acc: 0.938 - ETA: 2s - loss: 0.1640 - acc: 0.939 - ETA: 2s - loss: 0.1670 - acc: 0.936 - ETA: 2s - loss: 0.1647 - acc: 0.938 - ETA: 2s - loss: 0.1657 - acc: 0.937 - ETA: 1s - loss: 0.1711 - acc: 0.936 - ETA: 1s - loss: 0.1701 - acc: 0.936 - ETA: 1s - loss: 0.1693 - acc: 0.937 - ETA: 1s - loss: 0.1700 - acc: 0.936 - ETA: 1s - loss: 0.1702 - acc: 0.936 - ETA: 1s - loss: 0.1694 - acc: 0.936 - ETA: 1s - loss: 0.1684 - acc: 0.936 - ETA: 1s - loss: 0.1689 - acc: 0.934 - ETA: 1s - loss: 0.1714 - acc: 0.934 - ETA: 1s - loss: 0.1744 - acc: 0.933 - ETA: 0s - loss: 0.1727 - acc: 0.934 - ETA: 0s - loss: 0.1725 - acc: 0.933 - ETA: 0s - loss: 0.1734 - acc: 0.933 - ETA: 0s - loss: 0.1726 - acc: 0.934 - ETA: 0s - loss: 0.1718 - acc: 0.933 - ETA: 0s - loss: 0.1736 - acc: 0.932 - ETA: 0s - loss: 0.1758 - acc: 0.931 - ETA: 0s - loss: 0.1745 - acc: 0.932 - ETA: 0s - loss: 0.1777 - acc: 0.932 - 5s 103ms/step - loss: 0.1764 - acc: 0.9325\n",
      "Epoch 26/10000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1306 - acc: 0.954 - ETA: 4s - loss: 0.1458 - acc: 0.943 - ETA: 4s - loss: 0.1433 - acc: 0.954 - ETA: 4s - loss: 0.1366 - acc: 0.960 - ETA: 4s - loss: 0.1452 - acc: 0.956 - ETA: 4s - loss: 0.1489 - acc: 0.952 - ETA: 4s - loss: 0.1398 - acc: 0.956 - ETA: 4s - loss: 0.1387 - acc: 0.954 - ETA: 4s - loss: 0.1326 - acc: 0.955 - ETA: 3s - loss: 0.1428 - acc: 0.953 - ETA: 3s - loss: 0.1449 - acc: 0.952 - ETA: 3s - loss: 0.1447 - acc: 0.951 - ETA: 3s - loss: 0.1438 - acc: 0.951 - ETA: 3s - loss: 0.1438 - acc: 0.950 - ETA: 3s - loss: 0.1458 - acc: 0.949 - ETA: 3s - loss: 0.1473 - acc: 0.947 - ETA: 3s - loss: 0.1438 - acc: 0.949 - ETA: 3s - loss: 0.1479 - acc: 0.947 - ETA: 3s - loss: 0.1493 - acc: 0.945 - ETA: 2s - loss: 0.1465 - acc: 0.945 - ETA: 2s - loss: 0.1464 - acc: 0.945 - ETA: 2s - loss: 0.1465 - acc: 0.945 - ETA: 2s - loss: 0.1460 - acc: 0.945 - ETA: 2s - loss: 0.1441 - acc: 0.946 - ETA: 2s - loss: 0.1483 - acc: 0.945 - ETA: 2s - loss: 0.1461 - acc: 0.946 - ETA: 2s - loss: 0.1454 - acc: 0.947 - ETA: 2s - loss: 0.1449 - acc: 0.947 - ETA: 2s - loss: 0.1456 - acc: 0.947 - ETA: 1s - loss: 0.1458 - acc: 0.945 - ETA: 1s - loss: 0.1476 - acc: 0.945 - ETA: 1s - loss: 0.1467 - acc: 0.945 - ETA: 1s - loss: 0.1454 - acc: 0.946 - ETA: 1s - loss: 0.1461 - acc: 0.945 - ETA: 1s - loss: 0.1498 - acc: 0.943 - ETA: 1s - loss: 0.1531 - acc: 0.942 - ETA: 1s - loss: 0.1559 - acc: 0.941 - ETA: 1s - loss: 0.1576 - acc: 0.941 - ETA: 1s - loss: 0.1590 - acc: 0.940 - ETA: 0s - loss: 0.1585 - acc: 0.940 - ETA: 0s - loss: 0.1571 - acc: 0.940 - ETA: 0s - loss: 0.1572 - acc: 0.940 - ETA: 0s - loss: 0.1557 - acc: 0.941 - ETA: 0s - loss: 0.1562 - acc: 0.940 - ETA: 0s - loss: 0.1560 - acc: 0.940 - ETA: 0s - loss: 0.1558 - acc: 0.940 - ETA: 0s - loss: 0.1568 - acc: 0.940 - ETA: 0s - loss: 0.1568 - acc: 0.940 - 5s 103ms/step - loss: 0.1566 - acc: 0.9406\n",
      "Epoch 27/10000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1528 - acc: 0.954 - ETA: 4s - loss: 0.1540 - acc: 0.943 - ETA: 4s - loss: 0.1387 - acc: 0.939 - ETA: 4s - loss: 0.1546 - acc: 0.929 - ETA: 4s - loss: 0.1640 - acc: 0.929 - ETA: 4s - loss: 0.1594 - acc: 0.931 - ETA: 4s - loss: 0.1597 - acc: 0.933 - ETA: 4s - loss: 0.1645 - acc: 0.930 - ETA: 4s - loss: 0.1585 - acc: 0.933 - ETA: 4s - loss: 0.1615 - acc: 0.933 - ETA: 3s - loss: 0.1786 - acc: 0.929 - ETA: 3s - loss: 0.1751 - acc: 0.931 - ETA: 3s - loss: 0.1825 - acc: 0.929 - ETA: 3s - loss: 0.1786 - acc: 0.928 - ETA: 3s - loss: 0.1772 - acc: 0.928 - ETA: 3s - loss: 0.1742 - acc: 0.927 - ETA: 3s - loss: 0.1759 - acc: 0.926 - ETA: 3s - loss: 0.1770 - acc: 0.925 - ETA: 3s - loss: 0.1817 - acc: 0.924 - ETA: 3s - loss: 0.1858 - acc: 0.923 - ETA: 2s - loss: 0.1860 - acc: 0.924 - ETA: 2s - loss: 0.1820 - acc: 0.925 - ETA: 2s - loss: 0.1807 - acc: 0.925 - ETA: 2s - loss: 0.1796 - acc: 0.925 - ETA: 2s - loss: 0.1799 - acc: 0.925 - ETA: 2s - loss: 0.1792 - acc: 0.926 - ETA: 2s - loss: 0.1764 - acc: 0.927 - ETA: 2s - loss: 0.1775 - acc: 0.926 - ETA: 2s - loss: 0.1788 - acc: 0.926 - ETA: 1s - loss: 0.1826 - acc: 0.925 - ETA: 1s - loss: 0.1805 - acc: 0.927 - ETA: 1s - loss: 0.1809 - acc: 0.927 - ETA: 1s - loss: 0.1784 - acc: 0.928 - ETA: 1s - loss: 0.1790 - acc: 0.928 - ETA: 1s - loss: 0.1802 - acc: 0.929 - ETA: 1s - loss: 0.1798 - acc: 0.929 - ETA: 1s - loss: 0.1798 - acc: 0.930 - ETA: 1s - loss: 0.1795 - acc: 0.930 - ETA: 1s - loss: 0.1834 - acc: 0.929 - ETA: 0s - loss: 0.1807 - acc: 0.931 - ETA: 0s - loss: 0.1800 - acc: 0.931 - ETA: 0s - loss: 0.1802 - acc: 0.931 - ETA: 0s - loss: 0.1779 - acc: 0.932 - ETA: 0s - loss: 0.1765 - acc: 0.933 - ETA: 0s - loss: 0.1758 - acc: 0.934 - ETA: 0s - loss: 0.1767 - acc: 0.934 - ETA: 0s - loss: 0.1754 - acc: 0.935 - ETA: 0s - loss: 0.1744 - acc: 0.936 - 5s 104ms/step - loss: 0.1762 - acc: 0.9355\n",
      "Epoch 28/10000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1397 - acc: 0.931 - ETA: 4s - loss: 0.1818 - acc: 0.931 - ETA: 4s - loss: 0.2067 - acc: 0.931 - ETA: 4s - loss: 0.2037 - acc: 0.926 - ETA: 4s - loss: 0.1938 - acc: 0.920 - ETA: 4s - loss: 0.2037 - acc: 0.922 - ETA: 4s - loss: 0.1849 - acc: 0.933 - ETA: 4s - loss: 0.1826 - acc: 0.930 - ETA: 4s - loss: 0.1792 - acc: 0.933 - ETA: 4s - loss: 0.1778 - acc: 0.935 - ETA: 3s - loss: 0.1681 - acc: 0.939 - ETA: 3s - loss: 0.1631 - acc: 0.941 - ETA: 3s - loss: 0.1635 - acc: 0.942 - ETA: 3s - loss: 0.1670 - acc: 0.943 - ETA: 3s - loss: 0.1664 - acc: 0.941 - ETA: 3s - loss: 0.1629 - acc: 0.941 - ETA: 3s - loss: 0.1632 - acc: 0.941 - ETA: 3s - loss: 0.1696 - acc: 0.937 - ETA: 3s - loss: 0.1683 - acc: 0.939 - ETA: 2s - loss: 0.1687 - acc: 0.939 - ETA: 2s - loss: 0.1680 - acc: 0.937 - ETA: 2s - loss: 0.1660 - acc: 0.939 - ETA: 2s - loss: 0.1671 - acc: 0.938 - ETA: 2s - loss: 0.1688 - acc: 0.937 - ETA: 2s - loss: 0.1666 - acc: 0.938 - ETA: 2s - loss: 0.1690 - acc: 0.938 - ETA: 2s - loss: 0.1685 - acc: 0.938 - ETA: 2s - loss: 0.1666 - acc: 0.939 - ETA: 2s - loss: 0.1658 - acc: 0.938 - ETA: 1s - loss: 0.1634 - acc: 0.939 - ETA: 1s - loss: 0.1633 - acc: 0.939 - ETA: 1s - loss: 0.1627 - acc: 0.940 - ETA: 1s - loss: 0.1603 - acc: 0.940 - ETA: 1s - loss: 0.1582 - acc: 0.940 - ETA: 1s - loss: 0.1577 - acc: 0.940 - ETA: 1s - loss: 0.1602 - acc: 0.939 - ETA: 1s - loss: 0.1615 - acc: 0.938 - ETA: 1s - loss: 0.1598 - acc: 0.939 - ETA: 1s - loss: 0.1605 - acc: 0.939 - ETA: 0s - loss: 0.1598 - acc: 0.939 - ETA: 0s - loss: 0.1606 - acc: 0.939 - ETA: 0s - loss: 0.1597 - acc: 0.940 - ETA: 0s - loss: 0.1636 - acc: 0.938 - ETA: 0s - loss: 0.1648 - acc: 0.938 - ETA: 0s - loss: 0.1643 - acc: 0.938 - ETA: 0s - loss: 0.1627 - acc: 0.939 - ETA: 0s - loss: 0.1648 - acc: 0.938 - ETA: 0s - loss: 0.1660 - acc: 0.938 - 5s 102ms/step - loss: 0.1675 - acc: 0.9376\n",
      "Epoch 29/10000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0979 - acc: 0.977 - ETA: 4s - loss: 0.1450 - acc: 0.948 - ETA: 4s - loss: 0.1722 - acc: 0.943 - ETA: 4s - loss: 0.1713 - acc: 0.940 - ETA: 4s - loss: 0.1628 - acc: 0.943 - ETA: 4s - loss: 0.1615 - acc: 0.943 - ETA: 4s - loss: 0.1615 - acc: 0.941 - ETA: 4s - loss: 0.1595 - acc: 0.940 - ETA: 4s - loss: 0.1632 - acc: 0.936 - ETA: 3s - loss: 0.1696 - acc: 0.935 - ETA: 3s - loss: 0.1741 - acc: 0.930 - ETA: 3s - loss: 0.1703 - acc: 0.931 - ETA: 3s - loss: 0.1784 - acc: 0.928 - ETA: 3s - loss: 0.1729 - acc: 0.930 - ETA: 3s - loss: 0.1711 - acc: 0.930 - ETA: 3s - loss: 0.1719 - acc: 0.931 - ETA: 3s - loss: 0.1719 - acc: 0.930 - ETA: 3s - loss: 0.1748 - acc: 0.929 - ETA: 3s - loss: 0.1754 - acc: 0.930 - ETA: 2s - loss: 0.1734 - acc: 0.932 - ETA: 2s - loss: 0.1743 - acc: 0.930 - ETA: 2s - loss: 0.1739 - acc: 0.930 - ETA: 2s - loss: 0.1720 - acc: 0.931 - ETA: 2s - loss: 0.1724 - acc: 0.931 - ETA: 2s - loss: 0.1763 - acc: 0.931 - ETA: 2s - loss: 0.1747 - acc: 0.931 - ETA: 2s - loss: 0.1747 - acc: 0.932 - ETA: 2s - loss: 0.1741 - acc: 0.931 - ETA: 2s - loss: 0.1732 - acc: 0.931 - ETA: 1s - loss: 0.1717 - acc: 0.931 - ETA: 1s - loss: 0.1754 - acc: 0.931 - ETA: 1s - loss: 0.1782 - acc: 0.929 - ETA: 1s - loss: 0.1775 - acc: 0.929 - ETA: 1s - loss: 0.1765 - acc: 0.929 - ETA: 1s - loss: 0.1763 - acc: 0.929 - ETA: 1s - loss: 0.1739 - acc: 0.930 - ETA: 1s - loss: 0.1723 - acc: 0.931 - ETA: 1s - loss: 0.1728 - acc: 0.931 - ETA: 1s - loss: 0.1733 - acc: 0.931 - ETA: 0s - loss: 0.1735 - acc: 0.930 - ETA: 0s - loss: 0.1754 - acc: 0.929 - ETA: 0s - loss: 0.1748 - acc: 0.929 - ETA: 0s - loss: 0.1747 - acc: 0.929 - ETA: 0s - loss: 0.1730 - acc: 0.930 - ETA: 0s - loss: 0.1710 - acc: 0.931 - ETA: 0s - loss: 0.1727 - acc: 0.931 - ETA: 0s - loss: 0.1703 - acc: 0.933 - ETA: 0s - loss: 0.1709 - acc: 0.933 - 5s 102ms/step - loss: 0.1711 - acc: 0.9325\n",
      "Epoch 30/10000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1008 - acc: 0.943 - ETA: 4s - loss: 0.1517 - acc: 0.926 - ETA: 4s - loss: 0.1641 - acc: 0.931 - ETA: 4s - loss: 0.1585 - acc: 0.940 - ETA: 4s - loss: 0.1440 - acc: 0.945 - ETA: 4s - loss: 0.1388 - acc: 0.948 - ETA: 4s - loss: 0.1417 - acc: 0.948 - ETA: 4s - loss: 0.1456 - acc: 0.947 - ETA: 4s - loss: 0.1579 - acc: 0.940 - ETA: 3s - loss: 0.1612 - acc: 0.937 - ETA: 3s - loss: 0.1618 - acc: 0.933 - ETA: 3s - loss: 0.1696 - acc: 0.930 - ETA: 3s - loss: 0.1662 - acc: 0.931 - ETA: 3s - loss: 0.1624 - acc: 0.935 - ETA: 3s - loss: 0.1699 - acc: 0.933 - ETA: 3s - loss: 0.1733 - acc: 0.933 - ETA: 3s - loss: 0.1739 - acc: 0.935 - ETA: 3s - loss: 0.1690 - acc: 0.938 - ETA: 3s - loss: 0.1652 - acc: 0.939 - ETA: 2s - loss: 0.1639 - acc: 0.939 - ETA: 2s - loss: 0.1615 - acc: 0.941 - ETA: 2s - loss: 0.1606 - acc: 0.941 - ETA: 2s - loss: 0.1639 - acc: 0.941 - ETA: 2s - loss: 0.1624 - acc: 0.940 - ETA: 2s - loss: 0.1583 - acc: 0.942 - ETA: 2s - loss: 0.1583 - acc: 0.942 - ETA: 2s - loss: 0.1615 - acc: 0.942 - ETA: 2s - loss: 0.1612 - acc: 0.942 - ETA: 2s - loss: 0.1612 - acc: 0.942 - ETA: 1s - loss: 0.1587 - acc: 0.943 - ETA: 1s - loss: 0.1591 - acc: 0.944 - ETA: 1s - loss: 0.1575 - acc: 0.945 - ETA: 1s - loss: 0.1567 - acc: 0.944 - ETA: 1s - loss: 0.1573 - acc: 0.945 - ETA: 1s - loss: 0.1561 - acc: 0.945 - ETA: 1s - loss: 0.1585 - acc: 0.945 - ETA: 1s - loss: 0.1581 - acc: 0.944 - ETA: 1s - loss: 0.1582 - acc: 0.944 - ETA: 1s - loss: 0.1569 - acc: 0.945 - ETA: 0s - loss: 0.1589 - acc: 0.944 - ETA: 0s - loss: 0.1583 - acc: 0.945 - ETA: 0s - loss: 0.1577 - acc: 0.944 - ETA: 0s - loss: 0.1568 - acc: 0.945 - ETA: 0s - loss: 0.1568 - acc: 0.945 - ETA: 0s - loss: 0.1564 - acc: 0.945 - ETA: 0s - loss: 0.1557 - acc: 0.946 - ETA: 0s - loss: 0.1554 - acc: 0.946 - ETA: 0s - loss: 0.1545 - acc: 0.946 - 5s 101ms/step - loss: 0.1545 - acc: 0.9465\n",
      "Epoch 31/10000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2048 - acc: 0.897 - ETA: 4s - loss: 0.2223 - acc: 0.909 - ETA: 4s - loss: 0.2183 - acc: 0.912 - ETA: 4s - loss: 0.2156 - acc: 0.923 - ETA: 4s - loss: 0.2050 - acc: 0.927 - ETA: 4s - loss: 0.1863 - acc: 0.933 - ETA: 4s - loss: 0.1842 - acc: 0.935 - ETA: 4s - loss: 0.1761 - acc: 0.938 - ETA: 4s - loss: 0.1789 - acc: 0.938 - ETA: 3s - loss: 0.1769 - acc: 0.939 - ETA: 3s - loss: 0.1791 - acc: 0.937 - ETA: 3s - loss: 0.1847 - acc: 0.934 - ETA: 3s - loss: 0.1832 - acc: 0.934 - ETA: 3s - loss: 0.1803 - acc: 0.935 - ETA: 3s - loss: 0.1828 - acc: 0.934 - ETA: 3s - loss: 0.1874 - acc: 0.931 - ETA: 3s - loss: 0.1922 - acc: 0.930 - ETA: 3s - loss: 0.1904 - acc: 0.931 - ETA: 3s - loss: 0.1851 - acc: 0.933 - ETA: 2s - loss: 0.1864 - acc: 0.933 - ETA: 2s - loss: 0.1878 - acc: 0.931 - ETA: 2s - loss: 0.1843 - acc: 0.932 - ETA: 2s - loss: 0.1900 - acc: 0.931 - ETA: 2s - loss: 0.1926 - acc: 0.930 - ETA: 2s - loss: 0.1920 - acc: 0.931 - ETA: 2s - loss: 0.1916 - acc: 0.930 - ETA: 2s - loss: 0.1905 - acc: 0.931 - ETA: 2s - loss: 0.1893 - acc: 0.931 - ETA: 2s - loss: 0.1874 - acc: 0.932 - ETA: 1s - loss: 0.1849 - acc: 0.933 - ETA: 1s - loss: 0.1823 - acc: 0.934 - ETA: 1s - loss: 0.1813 - acc: 0.934 - ETA: 1s - loss: 0.1829 - acc: 0.933 - ETA: 1s - loss: 0.1803 - acc: 0.934 - ETA: 1s - loss: 0.1790 - acc: 0.934 - ETA: 1s - loss: 0.1771 - acc: 0.935 - ETA: 1s - loss: 0.1741 - acc: 0.937 - ETA: 1s - loss: 0.1727 - acc: 0.936 - ETA: 1s - loss: 0.1705 - acc: 0.937 - ETA: 0s - loss: 0.1744 - acc: 0.936 - ETA: 0s - loss: 0.1740 - acc: 0.936 - ETA: 0s - loss: 0.1727 - acc: 0.936 - ETA: 0s - loss: 0.1739 - acc: 0.936 - ETA: 0s - loss: 0.1724 - acc: 0.936 - ETA: 0s - loss: 0.1716 - acc: 0.937 - ETA: 0s - loss: 0.1706 - acc: 0.937 - ETA: 0s - loss: 0.1713 - acc: 0.936 - ETA: 0s - loss: 0.1694 - acc: 0.937 - 5s 102ms/step - loss: 0.1723 - acc: 0.9364\n",
      "Epoch 32/10000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1350 - acc: 0.977 - ETA: 4s - loss: 0.1415 - acc: 0.954 - ETA: 4s - loss: 0.1733 - acc: 0.947 - ETA: 4s - loss: 0.1712 - acc: 0.937 - ETA: 4s - loss: 0.1519 - acc: 0.947 - ETA: 4s - loss: 0.1487 - acc: 0.947 - ETA: 4s - loss: 0.1700 - acc: 0.938 - ETA: 4s - loss: 0.1718 - acc: 0.936 - ETA: 4s - loss: 0.1714 - acc: 0.933 - ETA: 3s - loss: 0.1710 - acc: 0.935 - ETA: 3s - loss: 0.1807 - acc: 0.928 - ETA: 3s - loss: 0.1798 - acc: 0.929 - ETA: 3s - loss: 0.1781 - acc: 0.930 - ETA: 3s - loss: 0.1802 - acc: 0.930 - ETA: 3s - loss: 0.1748 - acc: 0.931 - ETA: 3s - loss: 0.1706 - acc: 0.933 - ETA: 3s - loss: 0.1723 - acc: 0.932 - ETA: 3s - loss: 0.1743 - acc: 0.932 - ETA: 3s - loss: 0.1733 - acc: 0.931 - ETA: 2s - loss: 0.1795 - acc: 0.930 - ETA: 2s - loss: 0.1824 - acc: 0.929 - ETA: 2s - loss: 0.1831 - acc: 0.929 - ETA: 2s - loss: 0.1814 - acc: 0.929 - ETA: 2s - loss: 0.1778 - acc: 0.931 - ETA: 2s - loss: 0.1778 - acc: 0.930 - ETA: 2s - loss: 0.1734 - acc: 0.932 - ETA: 2s - loss: 0.1721 - acc: 0.933 - ETA: 2s - loss: 0.1699 - acc: 0.933 - ETA: 2s - loss: 0.1762 - acc: 0.931 - ETA: 1s - loss: 0.1773 - acc: 0.931 - ETA: 1s - loss: 0.1750 - acc: 0.932 - ETA: 1s - loss: 0.1767 - acc: 0.932 - ETA: 1s - loss: 0.1768 - acc: 0.932 - ETA: 1s - loss: 0.1760 - acc: 0.932 - ETA: 1s - loss: 0.1772 - acc: 0.932 - ETA: 1s - loss: 0.1790 - acc: 0.932 - ETA: 1s - loss: 0.1777 - acc: 0.933 - ETA: 1s - loss: 0.1774 - acc: 0.933 - ETA: 1s - loss: 0.1750 - acc: 0.934 - ETA: 0s - loss: 0.1752 - acc: 0.935 - ETA: 0s - loss: 0.1752 - acc: 0.935 - ETA: 0s - loss: 0.1765 - acc: 0.934 - ETA: 0s - loss: 0.1765 - acc: 0.935 - ETA: 0s - loss: 0.1774 - acc: 0.935 - ETA: 0s - loss: 0.1785 - acc: 0.935 - ETA: 0s - loss: 0.1784 - acc: 0.935 - ETA: 0s - loss: 0.1793 - acc: 0.934 - ETA: 0s - loss: 0.1801 - acc: 0.934 - 5s 102ms/step - loss: 0.1786 - acc: 0.9355\n",
      "Epoch 33/10000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1550 - acc: 0.931 - ETA: 4s - loss: 0.2116 - acc: 0.897 - ETA: 4s - loss: 0.1952 - acc: 0.909 - ETA: 4s - loss: 0.1936 - acc: 0.909 - ETA: 4s - loss: 0.1929 - acc: 0.911 - ETA: 4s - loss: 0.1828 - acc: 0.916 - ETA: 4s - loss: 0.1857 - acc: 0.920 - ETA: 4s - loss: 0.1940 - acc: 0.916 - ETA: 4s - loss: 0.1964 - acc: 0.916 - ETA: 3s - loss: 0.1951 - acc: 0.915 - ETA: 3s - loss: 0.1863 - acc: 0.922 - ETA: 3s - loss: 0.1850 - acc: 0.922 - ETA: 3s - loss: 0.1793 - acc: 0.925 - ETA: 3s - loss: 0.1789 - acc: 0.925 - ETA: 3s - loss: 0.1731 - acc: 0.928 - ETA: 3s - loss: 0.1743 - acc: 0.926 - ETA: 3s - loss: 0.1704 - acc: 0.928 - ETA: 3s - loss: 0.1688 - acc: 0.929 - ETA: 3s - loss: 0.1658 - acc: 0.930 - ETA: 2s - loss: 0.1644 - acc: 0.930 - ETA: 2s - loss: 0.1648 - acc: 0.930 - ETA: 2s - loss: 0.1649 - acc: 0.931 - ETA: 2s - loss: 0.1693 - acc: 0.929 - ETA: 2s - loss: 0.1674 - acc: 0.930 - ETA: 2s - loss: 0.1685 - acc: 0.930 - ETA: 2s - loss: 0.1699 - acc: 0.930 - ETA: 2s - loss: 0.1701 - acc: 0.932 - ETA: 2s - loss: 0.1685 - acc: 0.933 - ETA: 2s - loss: 0.1680 - acc: 0.933 - ETA: 1s - loss: 0.1663 - acc: 0.934 - ETA: 1s - loss: 0.1639 - acc: 0.935 - ETA: 1s - loss: 0.1645 - acc: 0.935 - ETA: 1s - loss: 0.1641 - acc: 0.935 - ETA: 1s - loss: 0.1622 - acc: 0.936 - ETA: 1s - loss: 0.1608 - acc: 0.937 - ETA: 1s - loss: 0.1592 - acc: 0.938 - ETA: 1s - loss: 0.1588 - acc: 0.938 - ETA: 1s - loss: 0.1606 - acc: 0.937 - ETA: 1s - loss: 0.1590 - acc: 0.938 - ETA: 0s - loss: 0.1615 - acc: 0.937 - ETA: 0s - loss: 0.1621 - acc: 0.936 - ETA: 0s - loss: 0.1624 - acc: 0.936 - ETA: 0s - loss: 0.1634 - acc: 0.936 - ETA: 0s - loss: 0.1636 - acc: 0.936 - ETA: 0s - loss: 0.1636 - acc: 0.936 - ETA: 0s - loss: 0.1638 - acc: 0.936 - ETA: 0s - loss: 0.1639 - acc: 0.936 - ETA: 0s - loss: 0.1640 - acc: 0.936 - 5s 104ms/step - loss: 0.1647 - acc: 0.9361\n",
      "Epoch 34/10000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1621 - acc: 0.931 - ETA: 4s - loss: 0.1822 - acc: 0.937 - ETA: 4s - loss: 0.2170 - acc: 0.928 - ETA: 4s - loss: 0.2316 - acc: 0.926 - ETA: 4s - loss: 0.2071 - acc: 0.927 - ETA: 4s - loss: 0.1922 - acc: 0.934 - ETA: 4s - loss: 0.1834 - acc: 0.933 - ETA: 4s - loss: 0.1808 - acc: 0.934 - ETA: 4s - loss: 0.1804 - acc: 0.933 - ETA: 4s - loss: 0.1788 - acc: 0.933 - ETA: 3s - loss: 0.1821 - acc: 0.932 - ETA: 3s - loss: 0.1760 - acc: 0.933 - ETA: 3s - loss: 0.1808 - acc: 0.932 - ETA: 3s - loss: 0.1765 - acc: 0.933 - ETA: 3s - loss: 0.1779 - acc: 0.931 - ETA: 3s - loss: 0.1767 - acc: 0.930 - ETA: 3s - loss: 0.1753 - acc: 0.930 - ETA: 3s - loss: 0.1769 - acc: 0.927 - ETA: 3s - loss: 0.1757 - acc: 0.926 - ETA: 3s - loss: 0.1746 - acc: 0.927 - ETA: 2s - loss: 0.1755 - acc: 0.925 - ETA: 2s - loss: 0.1765 - acc: 0.924 - ETA: 2s - loss: 0.1757 - acc: 0.924 - ETA: 2s - loss: 0.1748 - acc: 0.924 - ETA: 2s - loss: 0.1725 - acc: 0.925 - ETA: 2s - loss: 0.1752 - acc: 0.924 - ETA: 2s - loss: 0.1747 - acc: 0.924 - ETA: 2s - loss: 0.1769 - acc: 0.923 - ETA: 2s - loss: 0.1759 - acc: 0.923 - ETA: 1s - loss: 0.1739 - acc: 0.923 - ETA: 1s - loss: 0.1758 - acc: 0.923 - ETA: 1s - loss: 0.1768 - acc: 0.924 - ETA: 1s - loss: 0.1784 - acc: 0.924 - ETA: 1s - loss: 0.1782 - acc: 0.923 - ETA: 1s - loss: 0.1757 - acc: 0.925 - ETA: 1s - loss: 0.1746 - acc: 0.926 - ETA: 1s - loss: 0.1745 - acc: 0.926 - ETA: 1s - loss: 0.1752 - acc: 0.925 - ETA: 1s - loss: 0.1752 - acc: 0.925 - ETA: 0s - loss: 0.1768 - acc: 0.926 - ETA: 0s - loss: 0.1777 - acc: 0.926 - ETA: 0s - loss: 0.1780 - acc: 0.926 - ETA: 0s - loss: 0.1770 - acc: 0.926 - ETA: 0s - loss: 0.1759 - acc: 0.927 - ETA: 0s - loss: 0.1759 - acc: 0.926 - ETA: 0s - loss: 0.1755 - acc: 0.926 - ETA: 0s - loss: 0.1773 - acc: 0.926 - ETA: 0s - loss: 0.1774 - acc: 0.925 - 5s 104ms/step - loss: 0.1788 - acc: 0.9256\n",
      "Epoch 35/10000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1149 - acc: 0.977 - ETA: 4s - loss: 0.1425 - acc: 0.954 - ETA: 4s - loss: 0.1781 - acc: 0.939 - ETA: 4s - loss: 0.1912 - acc: 0.923 - ETA: 4s - loss: 0.1746 - acc: 0.931 - ETA: 4s - loss: 0.1689 - acc: 0.933 - ETA: 4s - loss: 0.1791 - acc: 0.928 - ETA: 4s - loss: 0.1863 - acc: 0.923 - ETA: 4s - loss: 0.1818 - acc: 0.926 - ETA: 4s - loss: 0.1756 - acc: 0.931 - ETA: 3s - loss: 0.1820 - acc: 0.928 - ETA: 3s - loss: 0.1869 - acc: 0.927 - ETA: 3s - loss: 0.1859 - acc: 0.925 - ETA: 3s - loss: 0.1883 - acc: 0.921 - ETA: 3s - loss: 0.1912 - acc: 0.922 - ETA: 3s - loss: 0.1898 - acc: 0.921 - ETA: 3s - loss: 0.1849 - acc: 0.925 - ETA: 3s - loss: 0.1835 - acc: 0.926 - ETA: 3s - loss: 0.1864 - acc: 0.925 - ETA: 3s - loss: 0.1884 - acc: 0.923 - ETA: 2s - loss: 0.1896 - acc: 0.922 - ETA: 2s - loss: 0.1889 - acc: 0.921 - ETA: 2s - loss: 0.1859 - acc: 0.921 - ETA: 2s - loss: 0.1834 - acc: 0.922 - ETA: 2s - loss: 0.1863 - acc: 0.923 - ETA: 2s - loss: 0.1825 - acc: 0.925 - ETA: 2s - loss: 0.1816 - acc: 0.926 - ETA: 2s - loss: 0.1807 - acc: 0.926 - ETA: 2s - loss: 0.1777 - acc: 0.928 - ETA: 1s - loss: 0.1810 - acc: 0.928 - ETA: 1s - loss: 0.1813 - acc: 0.928 - ETA: 1s - loss: 0.1799 - acc: 0.929 - ETA: 1s - loss: 0.1776 - acc: 0.930 - ETA: 1s - loss: 0.1786 - acc: 0.929 - ETA: 1s - loss: 0.1815 - acc: 0.927 - ETA: 1s - loss: 0.1796 - acc: 0.927 - ETA: 1s - loss: 0.1803 - acc: 0.927 - ETA: 1s - loss: 0.1798 - acc: 0.927 - ETA: 1s - loss: 0.1810 - acc: 0.927 - ETA: 0s - loss: 0.1819 - acc: 0.927 - ETA: 0s - loss: 0.1822 - acc: 0.927 - ETA: 0s - loss: 0.1805 - acc: 0.928 - ETA: 0s - loss: 0.1795 - acc: 0.928 - ETA: 0s - loss: 0.1780 - acc: 0.928 - ETA: 0s - loss: 0.1783 - acc: 0.929 - ETA: 0s - loss: 0.1787 - acc: 0.929 - ETA: 0s - loss: 0.1781 - acc: 0.929 - ETA: 0s - loss: 0.1796 - acc: 0.928 - 5s 104ms/step - loss: 0.1785 - acc: 0.9293\n",
      "Epoch 36/10000\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2457 - acc: 0.897 - ETA: 5s - loss: 0.2182 - acc: 0.920 - ETA: 4s - loss: 0.2007 - acc: 0.928 - ETA: 4s - loss: 0.1766 - acc: 0.940 - ETA: 4s - loss: 0.1660 - acc: 0.938 - ETA: 4s - loss: 0.1636 - acc: 0.935 - ETA: 4s - loss: 0.1886 - acc: 0.930 - ETA: 4s - loss: 0.1850 - acc: 0.929 - ETA: 4s - loss: 0.1801 - acc: 0.930 - ETA: 4s - loss: 0.1846 - acc: 0.931 - ETA: 4s - loss: 0.1847 - acc: 0.932 - ETA: 3s - loss: 0.1824 - acc: 0.932 - ETA: 3s - loss: 0.1862 - acc: 0.930 - ETA: 3s - loss: 0.1842 - acc: 0.931 - ETA: 3s - loss: 0.1766 - acc: 0.935 - ETA: 3s - loss: 0.1691 - acc: 0.939 - ETA: 3s - loss: 0.1754 - acc: 0.939 - ETA: 3s - loss: 0.1728 - acc: 0.939 - ETA: 3s - loss: 0.1680 - acc: 0.940 - ETA: 3s - loss: 0.1675 - acc: 0.941 - ETA: 2s - loss: 0.1671 - acc: 0.943 - ETA: 2s - loss: 0.1671 - acc: 0.942 - ETA: 2s - loss: 0.1694 - acc: 0.940 - ETA: 2s - loss: 0.1659 - acc: 0.942 - ETA: 2s - loss: 0.1674 - acc: 0.941 - ETA: 2s - loss: 0.1672 - acc: 0.941 - ETA: 2s - loss: 0.1638 - acc: 0.943 - ETA: 2s - loss: 0.1658 - acc: 0.941 - ETA: 2s - loss: 0.1650 - acc: 0.940 - ETA: 1s - loss: 0.1616 - acc: 0.942 - ETA: 1s - loss: 0.1620 - acc: 0.942 - ETA: 1s - loss: 0.1613 - acc: 0.942 - ETA: 1s - loss: 0.1613 - acc: 0.942 - ETA: 1s - loss: 0.1624 - acc: 0.942 - ETA: 1s - loss: 0.1641 - acc: 0.941 - ETA: 1s - loss: 0.1658 - acc: 0.940 - ETA: 1s - loss: 0.1684 - acc: 0.939 - ETA: 1s - loss: 0.1683 - acc: 0.939 - ETA: 1s - loss: 0.1679 - acc: 0.940 - ETA: 0s - loss: 0.1684 - acc: 0.939 - ETA: 0s - loss: 0.1702 - acc: 0.939 - ETA: 0s - loss: 0.1707 - acc: 0.939 - ETA: 0s - loss: 0.1701 - acc: 0.940 - ETA: 0s - loss: 0.1719 - acc: 0.939 - ETA: 0s - loss: 0.1721 - acc: 0.939 - ETA: 0s - loss: 0.1715 - acc: 0.940 - ETA: 0s - loss: 0.1705 - acc: 0.940 - ETA: 0s - loss: 0.1701 - acc: 0.940 - 5s 103ms/step - loss: 0.1707 - acc: 0.9404\n",
      "Epoch 37/10000\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1362 - acc: 0.954 - ETA: 4s - loss: 0.1702 - acc: 0.931 - ETA: 4s - loss: 0.1536 - acc: 0.935 - ETA: 4s - loss: 0.1402 - acc: 0.943 - ETA: 4s - loss: 0.1408 - acc: 0.940 - ETA: 4s - loss: 0.1430 - acc: 0.939 - ETA: 4s - loss: 0.1459 - acc: 0.941 - ETA: 4s - loss: 0.1445 - acc: 0.941 - ETA: 4s - loss: 0.1570 - acc: 0.938 - ETA: 3s - loss: 0.1658 - acc: 0.936 - ETA: 3s - loss: 0.1587 - acc: 0.942 - ETA: 3s - loss: 0.1609 - acc: 0.939 - ETA: 3s - loss: 0.1609 - acc: 0.938 - ETA: 3s - loss: 0.1549 - acc: 0.941 - ETA: 3s - loss: 0.1529 - acc: 0.940 - ETA: 3s - loss: 0.1521 - acc: 0.942 - ETA: 3s - loss: 0.1543 - acc: 0.941 - ETA: 3s - loss: 0.1549 - acc: 0.941 - ETA: 3s - loss: 0.1521 - acc: 0.942 - ETA: 2s - loss: 0.1533 - acc: 0.942 - ETA: 2s - loss: 0.1538 - acc: 0.941 - ETA: 2s - loss: 0.1542 - acc: 0.940 - ETA: 2s - loss: 0.1539 - acc: 0.940 - ETA: 2s - loss: 0.1514 - acc: 0.942 - ETA: 2s - loss: 0.1523 - acc: 0.942 - ETA: 2s - loss: 0.1541 - acc: 0.939 - ETA: 2s - loss: 0.1554 - acc: 0.939 - ETA: 2s - loss: 0.1555 - acc: 0.937 - ETA: 2s - loss: 0.1553 - acc: 0.937 - ETA: 1s - loss: 0.1580 - acc: 0.936 - ETA: 1s - loss: 0.1710 - acc: 0.935 - ETA: 1s - loss: 0.1698 - acc: 0.936 - ETA: 1s - loss: 0.1694 - acc: 0.936 - ETA: 1s - loss: 0.1690 - acc: 0.936 - ETA: 1s - loss: 0.1669 - acc: 0.937 - ETA: 1s - loss: 0.1663 - acc: 0.937 - ETA: 1s - loss: 0.1659 - acc: 0.937 - ETA: 1s - loss: 0.1642 - acc: 0.937 - ETA: 1s - loss: 0.1637 - acc: 0.937 - ETA: 0s - loss: 0.1629 - acc: 0.938 - ETA: 0s - loss: 0.1652 - acc: 0.937 - ETA: 0s - loss: 0.1668 - acc: 0.937 - ETA: 0s - loss: 0.1668 - acc: 0.937 - ETA: 0s - loss: 0.1675 - acc: 0.937 - ETA: 0s - loss: 0.1664 - acc: 0.937 - ETA: 0s - loss: 0.1673 - acc: 0.937 - ETA: 0s - loss: 0.1684 - acc: 0.936 - ETA: 0s - loss: 0.1677 - acc: 0.936 - 5s 103ms/step - loss: 0.1663 - acc: 0.9369\n",
      "Epoch 38/10000\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1388 - acc: 0.909 - ETA: 4s - loss: 0.1328 - acc: 0.926 - ETA: 4s - loss: 0.1247 - acc: 0.939 - ETA: 4s - loss: 0.1347 - acc: 0.934 - ETA: 4s - loss: 0.1297 - acc: 0.940 - ETA: 4s - loss: 0.1160 - acc: 0.950 - ETA: 4s - loss: 0.1252 - acc: 0.946 - ETA: 4s - loss: 0.1377 - acc: 0.943 - ETA: 4s - loss: 0.1305 - acc: 0.947 - ETA: 3s - loss: 0.1357 - acc: 0.944 - ETA: 3s - loss: 0.1476 - acc: 0.942 - ETA: 3s - loss: 0.1452 - acc: 0.943 - ETA: 3s - loss: 0.1491 - acc: 0.941 - ETA: 3s - loss: 0.1500 - acc: 0.941 - ETA: 3s - loss: 0.1463 - acc: 0.943 - ETA: 3s - loss: 0.1469 - acc: 0.943 - ETA: 3s - loss: 0.1486 - acc: 0.945 - ETA: 3s - loss: 0.1517 - acc: 0.943 - ETA: 3s - loss: 0.1541 - acc: 0.943 - ETA: 2s - loss: 0.1493 - acc: 0.946 - ETA: 2s - loss: 0.1467 - acc: 0.946 - ETA: 2s - loss: 0.1549 - acc: 0.942 - ETA: 2s - loss: 0.1540 - acc: 0.942 - ETA: 2s - loss: 0.1564 - acc: 0.941 - ETA: 2s - loss: 0.1566 - acc: 0.940 - ETA: 2s - loss: 0.1577 - acc: 0.939 - ETA: 2s - loss: 0.1569 - acc: 0.939 - ETA: 2s - loss: 0.1575 - acc: 0.939 - ETA: 2s - loss: 0.1560 - acc: 0.939 - ETA: 1s - loss: 0.1546 - acc: 0.939 - ETA: 1s - loss: 0.1545 - acc: 0.938 - ETA: 1s - loss: 0.1590 - acc: 0.937 - ETA: 1s - loss: 0.1572 - acc: 0.938 - ETA: 1s - loss: 0.1582 - acc: 0.938 - ETA: 1s - loss: 0.1568 - acc: 0.939 - ETA: 1s - loss: 0.1573 - acc: 0.939 - ETA: 1s - loss: 0.1580 - acc: 0.939 - ETA: 1s - loss: 0.1583 - acc: 0.940 - ETA: 1s - loss: 0.1571 - acc: 0.940 - ETA: 0s - loss: 0.1558 - acc: 0.941 - ETA: 0s - loss: 0.1547 - acc: 0.941 - ETA: 0s - loss: 0.1543 - acc: 0.941 - ETA: 0s - loss: 0.1548 - acc: 0.941 - ETA: 0s - loss: 0.1551 - acc: 0.941 - ETA: 0s - loss: 0.1537 - acc: 0.942 - ETA: 0s - loss: 0.1560 - acc: 0.941 - ETA: 0s - loss: 0.1580 - acc: 0.941 - ETA: 0s - loss: 0.1573 - acc: 0.941 - 5s 103ms/step - loss: 0.1577 - acc: 0.9409\n",
      "Epoch 39/10000\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2397 - acc: 0.909 - ETA: 4s - loss: 0.1749 - acc: 0.943 - ETA: 4s - loss: 0.1830 - acc: 0.939 - ETA: 4s - loss: 0.2105 - acc: 0.931 - ETA: 4s - loss: 0.2022 - acc: 0.927 - ETA: 4s - loss: 0.2105 - acc: 0.924 - ETA: 4s - loss: 0.2048 - acc: 0.928 - ETA: 4s - loss: 0.1955 - acc: 0.931 - ETA: 4s - loss: 0.1941 - acc: 0.929 - ETA: 4s - loss: 0.1897 - acc: 0.929 - ETA: 3s - loss: 0.1883 - acc: 0.929 - ETA: 3s - loss: 0.1858 - acc: 0.929 - ETA: 3s - loss: 0.1861 - acc: 0.929 - ETA: 3s - loss: 0.1827 - acc: 0.931 - ETA: 3s - loss: 0.1769 - acc: 0.934 - ETA: 3s - loss: 0.1797 - acc: 0.933 - ETA: 3s - loss: 0.1755 - acc: 0.935 - ETA: 3s - loss: 0.1709 - acc: 0.936 - ETA: 3s - loss: 0.1665 - acc: 0.939 - ETA: 3s - loss: 0.1616 - acc: 0.941 - ETA: 2s - loss: 0.1695 - acc: 0.938 - ETA: 2s - loss: 0.1706 - acc: 0.938 - ETA: 2s - loss: 0.1699 - acc: 0.938 - ETA: 2s - loss: 0.1753 - acc: 0.935 - ETA: 2s - loss: 0.1721 - acc: 0.937 - ETA: 2s - loss: 0.1678 - acc: 0.939 - ETA: 2s - loss: 0.1660 - acc: 0.940 - ETA: 2s - loss: 0.1627 - acc: 0.941 - ETA: 2s - loss: 0.1621 - acc: 0.941 - ETA: 1s - loss: 0.1632 - acc: 0.941 - ETA: 1s - loss: 0.1618 - acc: 0.941 - ETA: 1s - loss: 0.1611 - acc: 0.941 - ETA: 1s - loss: 0.1607 - acc: 0.941 - ETA: 1s - loss: 0.1647 - acc: 0.941 - ETA: 1s - loss: 0.1683 - acc: 0.941 - ETA: 1s - loss: 0.1670 - acc: 0.941 - ETA: 1s - loss: 0.1689 - acc: 0.941 - ETA: 1s - loss: 0.1670 - acc: 0.941 - ETA: 1s - loss: 0.1671 - acc: 0.941 - ETA: 0s - loss: 0.1651 - acc: 0.942 - ETA: 0s - loss: 0.1657 - acc: 0.941 - ETA: 0s - loss: 0.1648 - acc: 0.940 - ETA: 0s - loss: 0.1628 - acc: 0.942 - ETA: 0s - loss: 0.1604 - acc: 0.942 - ETA: 0s - loss: 0.1597 - acc: 0.943 - ETA: 0s - loss: 0.1589 - acc: 0.943 - ETA: 0s - loss: 0.1578 - acc: 0.944 - ETA: 0s - loss: 0.1576 - acc: 0.944 - 5s 104ms/step - loss: 0.1572 - acc: 0.9452\n",
      "Epoch 40/10000\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.000640000042039901.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1365 - acc: 0.943 - ETA: 4s - loss: 0.1425 - acc: 0.926 - ETA: 4s - loss: 0.1221 - acc: 0.943 - ETA: 4s - loss: 0.1117 - acc: 0.951 - ETA: 4s - loss: 0.1229 - acc: 0.945 - ETA: 4s - loss: 0.1257 - acc: 0.945 - ETA: 4s - loss: 0.1181 - acc: 0.948 - ETA: 4s - loss: 0.1267 - acc: 0.946 - ETA: 4s - loss: 0.1348 - acc: 0.943 - ETA: 4s - loss: 0.1477 - acc: 0.938 - ETA: 3s - loss: 0.1497 - acc: 0.939 - ETA: 3s - loss: 0.1574 - acc: 0.937 - ETA: 3s - loss: 0.1603 - acc: 0.937 - ETA: 3s - loss: 0.1593 - acc: 0.937 - ETA: 3s - loss: 0.1554 - acc: 0.937 - ETA: 3s - loss: 0.1535 - acc: 0.938 - ETA: 3s - loss: 0.1527 - acc: 0.939 - ETA: 3s - loss: 0.1478 - acc: 0.942 - ETA: 3s - loss: 0.1526 - acc: 0.940 - ETA: 3s - loss: 0.1512 - acc: 0.941 - ETA: 2s - loss: 0.1524 - acc: 0.941 - ETA: 2s - loss: 0.1575 - acc: 0.940 - ETA: 2s - loss: 0.1584 - acc: 0.940 - ETA: 2s - loss: 0.1575 - acc: 0.941 - ETA: 2s - loss: 0.1534 - acc: 0.942 - ETA: 2s - loss: 0.1504 - acc: 0.944 - ETA: 2s - loss: 0.1512 - acc: 0.943 - ETA: 2s - loss: 0.1520 - acc: 0.942 - ETA: 2s - loss: 0.1573 - acc: 0.941 - ETA: 1s - loss: 0.1595 - acc: 0.941 - ETA: 1s - loss: 0.1613 - acc: 0.940 - ETA: 1s - loss: 0.1669 - acc: 0.939 - ETA: 1s - loss: 0.1686 - acc: 0.938 - ETA: 1s - loss: 0.1682 - acc: 0.937 - ETA: 1s - loss: 0.1706 - acc: 0.937 - ETA: 1s - loss: 0.1699 - acc: 0.937 - ETA: 1s - loss: 0.1701 - acc: 0.936 - ETA: 1s - loss: 0.1702 - acc: 0.935 - ETA: 1s - loss: 0.1700 - acc: 0.935 - ETA: 0s - loss: 0.1694 - acc: 0.935 - ETA: 0s - loss: 0.1707 - acc: 0.934 - ETA: 0s - loss: 0.1706 - acc: 0.935 - ETA: 0s - loss: 0.1693 - acc: 0.935 - ETA: 0s - loss: 0.1676 - acc: 0.936 - ETA: 0s - loss: 0.1675 - acc: 0.935 - ETA: 0s - loss: 0.1670 - acc: 0.935 - ETA: 0s - loss: 0.1652 - acc: 0.936 - ETA: 0s - loss: 0.1664 - acc: 0.936 - 5s 104ms/step - loss: 0.1662 - acc: 0.9359\n",
      "Epoch 41/10000\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0005120000336319208.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2318 - acc: 0.920 - ETA: 4s - loss: 0.1511 - acc: 0.954 - ETA: 4s - loss: 0.1712 - acc: 0.939 - ETA: 4s - loss: 0.1697 - acc: 0.940 - ETA: 4s - loss: 0.1530 - acc: 0.950 - ETA: 4s - loss: 0.1517 - acc: 0.950 - ETA: 4s - loss: 0.1459 - acc: 0.954 - ETA: 4s - loss: 0.1448 - acc: 0.956 - ETA: 4s - loss: 0.1378 - acc: 0.958 - ETA: 4s - loss: 0.1418 - acc: 0.956 - ETA: 3s - loss: 0.1408 - acc: 0.954 - ETA: 3s - loss: 0.1434 - acc: 0.952 - ETA: 3s - loss: 0.1501 - acc: 0.950 - ETA: 3s - loss: 0.1484 - acc: 0.950 - ETA: 3s - loss: 0.1475 - acc: 0.950 - ETA: 3s - loss: 0.1459 - acc: 0.948 - ETA: 3s - loss: 0.1476 - acc: 0.947 - ETA: 3s - loss: 0.1481 - acc: 0.945 - ETA: 3s - loss: 0.1464 - acc: 0.946 - ETA: 3s - loss: 0.1436 - acc: 0.948 - ETA: 2s - loss: 0.1419 - acc: 0.948 - ETA: 2s - loss: 0.1433 - acc: 0.949 - ETA: 2s - loss: 0.1441 - acc: 0.949 - ETA: 2s - loss: 0.1476 - acc: 0.947 - ETA: 2s - loss: 0.1460 - acc: 0.947 - ETA: 2s - loss: 0.1484 - acc: 0.946 - ETA: 2s - loss: 0.1549 - acc: 0.945 - ETA: 2s - loss: 0.1528 - acc: 0.946 - ETA: 2s - loss: 0.1543 - acc: 0.945 - ETA: 1s - loss: 0.1573 - acc: 0.945 - ETA: 1s - loss: 0.1565 - acc: 0.944 - ETA: 1s - loss: 0.1583 - acc: 0.943 - ETA: 1s - loss: 0.1581 - acc: 0.943 - ETA: 1s - loss: 0.1584 - acc: 0.942 - ETA: 1s - loss: 0.1579 - acc: 0.942 - ETA: 1s - loss: 0.1580 - acc: 0.941 - ETA: 1s - loss: 0.1601 - acc: 0.941 - ETA: 1s - loss: 0.1619 - acc: 0.941 - ETA: 1s - loss: 0.1613 - acc: 0.941 - ETA: 0s - loss: 0.1623 - acc: 0.939 - ETA: 0s - loss: 0.1620 - acc: 0.939 - ETA: 0s - loss: 0.1622 - acc: 0.939 - ETA: 0s - loss: 0.1612 - acc: 0.939 - ETA: 0s - loss: 0.1606 - acc: 0.939 - ETA: 0s - loss: 0.1610 - acc: 0.938 - ETA: 0s - loss: 0.1622 - acc: 0.938 - ETA: 0s - loss: 0.1616 - acc: 0.939 - ETA: 0s - loss: 0.1615 - acc: 0.939 - 5s 104ms/step - loss: 0.1625 - acc: 0.9390\n",
      "Epoch 42/10000\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1700 - acc: 0.909 - ETA: 4s - loss: 0.1658 - acc: 0.920 - ETA: 4s - loss: 0.1621 - acc: 0.935 - ETA: 4s - loss: 0.1608 - acc: 0.943 - ETA: 4s - loss: 0.1491 - acc: 0.947 - ETA: 4s - loss: 0.1471 - acc: 0.950 - ETA: 4s - loss: 0.1423 - acc: 0.952 - ETA: 4s - loss: 0.1475 - acc: 0.946 - ETA: 4s - loss: 0.1448 - acc: 0.945 - ETA: 4s - loss: 0.1475 - acc: 0.945 - ETA: 3s - loss: 0.1468 - acc: 0.942 - ETA: 3s - loss: 0.1436 - acc: 0.944 - ETA: 3s - loss: 0.1395 - acc: 0.944 - ETA: 3s - loss: 0.1416 - acc: 0.942 - ETA: 3s - loss: 0.1374 - acc: 0.943 - ETA: 3s - loss: 0.1397 - acc: 0.942 - ETA: 3s - loss: 0.1396 - acc: 0.941 - ETA: 3s - loss: 0.1378 - acc: 0.943 - ETA: 3s - loss: 0.1486 - acc: 0.942 - ETA: 3s - loss: 0.1455 - acc: 0.944 - ETA: 2s - loss: 0.1429 - acc: 0.945 - ETA: 2s - loss: 0.1465 - acc: 0.944 - ETA: 2s - loss: 0.1474 - acc: 0.944 - ETA: 2s - loss: 0.1490 - acc: 0.943 - ETA: 2s - loss: 0.1477 - acc: 0.943 - ETA: 2s - loss: 0.1496 - acc: 0.943 - ETA: 2s - loss: 0.1477 - acc: 0.943 - ETA: 2s - loss: 0.1466 - acc: 0.944 - ETA: 2s - loss: 0.1452 - acc: 0.945 - ETA: 1s - loss: 0.1512 - acc: 0.944 - ETA: 1s - loss: 0.1564 - acc: 0.943 - ETA: 1s - loss: 0.1564 - acc: 0.943 - ETA: 1s - loss: 0.1561 - acc: 0.942 - ETA: 1s - loss: 0.1569 - acc: 0.941 - ETA: 1s - loss: 0.1562 - acc: 0.940 - ETA: 1s - loss: 0.1573 - acc: 0.939 - ETA: 1s - loss: 0.1555 - acc: 0.940 - ETA: 1s - loss: 0.1549 - acc: 0.941 - ETA: 1s - loss: 0.1536 - acc: 0.942 - ETA: 0s - loss: 0.1577 - acc: 0.941 - ETA: 0s - loss: 0.1565 - acc: 0.942 - ETA: 0s - loss: 0.1561 - acc: 0.942 - ETA: 0s - loss: 0.1544 - acc: 0.943 - ETA: 0s - loss: 0.1530 - acc: 0.943 - ETA: 0s - loss: 0.1532 - acc: 0.943 - ETA: 0s - loss: 0.1521 - acc: 0.943 - ETA: 0s - loss: 0.1504 - acc: 0.944 - ETA: 0s - loss: 0.1509 - acc: 0.944 - 5s 103ms/step - loss: 0.1524 - acc: 0.9439\n",
      "Epoch 43/10000\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1539 - acc: 0.954 - ETA: 4s - loss: 0.1681 - acc: 0.937 - ETA: 4s - loss: 0.1629 - acc: 0.943 - ETA: 4s - loss: 0.1362 - acc: 0.954 - ETA: 4s - loss: 0.1245 - acc: 0.956 - ETA: 4s - loss: 0.1306 - acc: 0.956 - ETA: 4s - loss: 0.1376 - acc: 0.952 - ETA: 4s - loss: 0.1341 - acc: 0.957 - ETA: 4s - loss: 0.1450 - acc: 0.952 - ETA: 3s - loss: 0.1557 - acc: 0.946 - ETA: 3s - loss: 0.1590 - acc: 0.941 - ETA: 3s - loss: 0.1522 - acc: 0.944 - ETA: 3s - loss: 0.1523 - acc: 0.943 - ETA: 3s - loss: 0.1474 - acc: 0.944 - ETA: 3s - loss: 0.1432 - acc: 0.947 - ETA: 3s - loss: 0.1472 - acc: 0.943 - ETA: 3s - loss: 0.1461 - acc: 0.944 - ETA: 3s - loss: 0.1488 - acc: 0.941 - ETA: 3s - loss: 0.1436 - acc: 0.945 - ETA: 2s - loss: 0.1484 - acc: 0.943 - ETA: 2s - loss: 0.1462 - acc: 0.944 - ETA: 2s - loss: 0.1461 - acc: 0.944 - ETA: 2s - loss: 0.1488 - acc: 0.943 - ETA: 2s - loss: 0.1472 - acc: 0.945 - ETA: 2s - loss: 0.1477 - acc: 0.944 - ETA: 2s - loss: 0.1503 - acc: 0.944 - ETA: 2s - loss: 0.1472 - acc: 0.945 - ETA: 2s - loss: 0.1452 - acc: 0.946 - ETA: 2s - loss: 0.1487 - acc: 0.945 - ETA: 1s - loss: 0.1489 - acc: 0.945 - ETA: 1s - loss: 0.1507 - acc: 0.945 - ETA: 1s - loss: 0.1539 - acc: 0.943 - ETA: 1s - loss: 0.1542 - acc: 0.942 - ETA: 1s - loss: 0.1536 - acc: 0.942 - ETA: 1s - loss: 0.1544 - acc: 0.943 - ETA: 1s - loss: 0.1543 - acc: 0.942 - ETA: 1s - loss: 0.1540 - acc: 0.943 - ETA: 1s - loss: 0.1531 - acc: 0.943 - ETA: 1s - loss: 0.1553 - acc: 0.942 - ETA: 0s - loss: 0.1552 - acc: 0.942 - ETA: 0s - loss: 0.1573 - acc: 0.941 - ETA: 0s - loss: 0.1581 - acc: 0.941 - ETA: 0s - loss: 0.1595 - acc: 0.940 - ETA: 0s - loss: 0.1580 - acc: 0.941 - ETA: 0s - loss: 0.1565 - acc: 0.942 - ETA: 0s - loss: 0.1553 - acc: 0.942 - ETA: 0s - loss: 0.1562 - acc: 0.942 - ETA: 0s - loss: 0.1558 - acc: 0.942 - 5s 102ms/step - loss: 0.1555 - acc: 0.9416\n",
      "Epoch 44/10000\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1504 - acc: 0.954 - ETA: 4s - loss: 0.1546 - acc: 0.948 - ETA: 4s - loss: 0.1685 - acc: 0.950 - ETA: 4s - loss: 0.1588 - acc: 0.951 - ETA: 4s - loss: 0.1396 - acc: 0.959 - ETA: 4s - loss: 0.1427 - acc: 0.956 - ETA: 4s - loss: 0.1544 - acc: 0.952 - ETA: 4s - loss: 0.1576 - acc: 0.950 - ETA: 4s - loss: 0.1519 - acc: 0.948 - ETA: 3s - loss: 0.1545 - acc: 0.947 - ETA: 3s - loss: 0.1532 - acc: 0.949 - ETA: 3s - loss: 0.1517 - acc: 0.949 - ETA: 3s - loss: 0.1511 - acc: 0.951 - ETA: 3s - loss: 0.1506 - acc: 0.952 - ETA: 3s - loss: 0.1597 - acc: 0.950 - ETA: 3s - loss: 0.1622 - acc: 0.948 - ETA: 3s - loss: 0.1592 - acc: 0.949 - ETA: 3s - loss: 0.1553 - acc: 0.951 - ETA: 3s - loss: 0.1585 - acc: 0.952 - ETA: 2s - loss: 0.1575 - acc: 0.952 - ETA: 2s - loss: 0.1588 - acc: 0.950 - ETA: 2s - loss: 0.1580 - acc: 0.950 - ETA: 2s - loss: 0.1583 - acc: 0.950 - ETA: 2s - loss: 0.1556 - acc: 0.950 - ETA: 2s - loss: 0.1548 - acc: 0.950 - ETA: 2s - loss: 0.1569 - acc: 0.951 - ETA: 2s - loss: 0.1587 - acc: 0.949 - ETA: 2s - loss: 0.1574 - acc: 0.949 - ETA: 2s - loss: 0.1595 - acc: 0.948 - ETA: 1s - loss: 0.1626 - acc: 0.946 - ETA: 1s - loss: 0.1601 - acc: 0.947 - ETA: 1s - loss: 0.1593 - acc: 0.947 - ETA: 1s - loss: 0.1584 - acc: 0.948 - ETA: 1s - loss: 0.1602 - acc: 0.947 - ETA: 1s - loss: 0.1583 - acc: 0.948 - ETA: 1s - loss: 0.1578 - acc: 0.947 - ETA: 1s - loss: 0.1566 - acc: 0.948 - ETA: 1s - loss: 0.1578 - acc: 0.946 - ETA: 1s - loss: 0.1579 - acc: 0.946 - ETA: 0s - loss: 0.1569 - acc: 0.946 - ETA: 0s - loss: 0.1574 - acc: 0.946 - ETA: 0s - loss: 0.1568 - acc: 0.945 - ETA: 0s - loss: 0.1554 - acc: 0.946 - ETA: 0s - loss: 0.1546 - acc: 0.946 - ETA: 0s - loss: 0.1539 - acc: 0.947 - ETA: 0s - loss: 0.1552 - acc: 0.946 - ETA: 0s - loss: 0.1552 - acc: 0.946 - ETA: 0s - loss: 0.1546 - acc: 0.946 - 5s 103ms/step - loss: 0.1536 - acc: 0.9469\n",
      "Epoch 45/10000\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1953 - acc: 0.909 - ETA: 4s - loss: 0.1782 - acc: 0.920 - ETA: 4s - loss: 0.1848 - acc: 0.928 - ETA: 4s - loss: 0.1615 - acc: 0.934 - ETA: 4s - loss: 0.1622 - acc: 0.931 - ETA: 4s - loss: 0.1574 - acc: 0.935 - ETA: 4s - loss: 0.1577 - acc: 0.931 - ETA: 4s - loss: 0.1498 - acc: 0.936 - ETA: 4s - loss: 0.1453 - acc: 0.938 - ETA: 3s - loss: 0.1572 - acc: 0.934 - ETA: 3s - loss: 0.1670 - acc: 0.930 - ETA: 3s - loss: 0.1708 - acc: 0.928 - ETA: 3s - loss: 0.1667 - acc: 0.930 - ETA: 3s - loss: 0.1686 - acc: 0.930 - ETA: 3s - loss: 0.1659 - acc: 0.931 - ETA: 3s - loss: 0.1644 - acc: 0.933 - ETA: 3s - loss: 0.1704 - acc: 0.933 - ETA: 3s - loss: 0.1711 - acc: 0.933 - ETA: 3s - loss: 0.1737 - acc: 0.932 - ETA: 2s - loss: 0.1764 - acc: 0.933 - ETA: 2s - loss: 0.1773 - acc: 0.934 - ETA: 2s - loss: 0.1750 - acc: 0.934 - ETA: 2s - loss: 0.1730 - acc: 0.935 - ETA: 2s - loss: 0.1718 - acc: 0.934 - ETA: 2s - loss: 0.1695 - acc: 0.935 - ETA: 2s - loss: 0.1664 - acc: 0.937 - ETA: 2s - loss: 0.1685 - acc: 0.936 - ETA: 2s - loss: 0.1660 - acc: 0.937 - ETA: 2s - loss: 0.1669 - acc: 0.936 - ETA: 1s - loss: 0.1668 - acc: 0.936 - ETA: 1s - loss: 0.1679 - acc: 0.936 - ETA: 1s - loss: 0.1670 - acc: 0.936 - ETA: 1s - loss: 0.1673 - acc: 0.936 - ETA: 1s - loss: 0.1653 - acc: 0.936 - ETA: 1s - loss: 0.1645 - acc: 0.937 - ETA: 1s - loss: 0.1633 - acc: 0.938 - ETA: 1s - loss: 0.1610 - acc: 0.939 - ETA: 1s - loss: 0.1601 - acc: 0.939 - ETA: 1s - loss: 0.1587 - acc: 0.939 - ETA: 0s - loss: 0.1561 - acc: 0.940 - ETA: 0s - loss: 0.1575 - acc: 0.940 - ETA: 0s - loss: 0.1582 - acc: 0.940 - ETA: 0s - loss: 0.1600 - acc: 0.940 - ETA: 0s - loss: 0.1597 - acc: 0.939 - ETA: 0s - loss: 0.1605 - acc: 0.939 - ETA: 0s - loss: 0.1602 - acc: 0.940 - ETA: 0s - loss: 0.1607 - acc: 0.939 - ETA: 0s - loss: 0.1599 - acc: 0.939 - 5s 104ms/step - loss: 0.1593 - acc: 0.9395\n",
      "Epoch 46/10000\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1421 - acc: 0.920 - ETA: 4s - loss: 0.1762 - acc: 0.931 - ETA: 4s - loss: 0.1618 - acc: 0.943 - ETA: 4s - loss: 0.1352 - acc: 0.957 - ETA: 4s - loss: 0.1396 - acc: 0.952 - ETA: 4s - loss: 0.1389 - acc: 0.952 - ETA: 4s - loss: 0.1304 - acc: 0.952 - ETA: 4s - loss: 0.1265 - acc: 0.954 - ETA: 4s - loss: 0.1365 - acc: 0.950 - ETA: 4s - loss: 0.1412 - acc: 0.952 - ETA: 3s - loss: 0.1481 - acc: 0.948 - ETA: 3s - loss: 0.1454 - acc: 0.947 - ETA: 3s - loss: 0.1449 - acc: 0.948 - ETA: 3s - loss: 0.1442 - acc: 0.947 - ETA: 3s - loss: 0.1442 - acc: 0.947 - ETA: 3s - loss: 0.1395 - acc: 0.950 - ETA: 3s - loss: 0.1350 - acc: 0.952 - ETA: 3s - loss: 0.1343 - acc: 0.953 - ETA: 3s - loss: 0.1311 - acc: 0.954 - ETA: 3s - loss: 0.1360 - acc: 0.954 - ETA: 2s - loss: 0.1383 - acc: 0.954 - ETA: 2s - loss: 0.1391 - acc: 0.953 - ETA: 2s - loss: 0.1392 - acc: 0.952 - ETA: 2s - loss: 0.1402 - acc: 0.949 - ETA: 2s - loss: 0.1374 - acc: 0.951 - ETA: 2s - loss: 0.1358 - acc: 0.951 - ETA: 2s - loss: 0.1367 - acc: 0.951 - ETA: 2s - loss: 0.1347 - acc: 0.952 - ETA: 2s - loss: 0.1370 - acc: 0.951 - ETA: 1s - loss: 0.1367 - acc: 0.951 - ETA: 1s - loss: 0.1375 - acc: 0.951 - ETA: 1s - loss: 0.1392 - acc: 0.950 - ETA: 1s - loss: 0.1418 - acc: 0.950 - ETA: 1s - loss: 0.1448 - acc: 0.949 - ETA: 1s - loss: 0.1425 - acc: 0.950 - ETA: 1s - loss: 0.1432 - acc: 0.949 - ETA: 1s - loss: 0.1425 - acc: 0.950 - ETA: 1s - loss: 0.1432 - acc: 0.949 - ETA: 1s - loss: 0.1432 - acc: 0.948 - ETA: 0s - loss: 0.1439 - acc: 0.948 - ETA: 0s - loss: 0.1433 - acc: 0.948 - ETA: 0s - loss: 0.1429 - acc: 0.949 - ETA: 0s - loss: 0.1443 - acc: 0.948 - ETA: 0s - loss: 0.1462 - acc: 0.947 - ETA: 0s - loss: 0.1467 - acc: 0.947 - ETA: 0s - loss: 0.1453 - acc: 0.948 - ETA: 0s - loss: 0.1453 - acc: 0.948 - ETA: 0s - loss: 0.1454 - acc: 0.947 - 5s 104ms/step - loss: 0.1446 - acc: 0.9477\n",
      "Epoch 47/10000\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1124 - acc: 0.954 - ETA: 4s - loss: 0.1638 - acc: 0.943 - ETA: 4s - loss: 0.1553 - acc: 0.947 - ETA: 4s - loss: 0.1651 - acc: 0.940 - ETA: 4s - loss: 0.1493 - acc: 0.947 - ETA: 4s - loss: 0.1443 - acc: 0.947 - ETA: 4s - loss: 0.1474 - acc: 0.948 - ETA: 4s - loss: 0.1437 - acc: 0.951 - ETA: 4s - loss: 0.1498 - acc: 0.949 - ETA: 4s - loss: 0.1470 - acc: 0.948 - ETA: 3s - loss: 0.1444 - acc: 0.950 - ETA: 3s - loss: 0.1382 - acc: 0.953 - ETA: 3s - loss: 0.1353 - acc: 0.954 - ETA: 3s - loss: 0.1409 - acc: 0.949 - ETA: 3s - loss: 0.1430 - acc: 0.948 - ETA: 3s - loss: 0.1535 - acc: 0.946 - ETA: 3s - loss: 0.1546 - acc: 0.943 - ETA: 3s - loss: 0.1604 - acc: 0.941 - ETA: 3s - loss: 0.1636 - acc: 0.938 - ETA: 3s - loss: 0.1689 - acc: 0.936 - ETA: 2s - loss: 0.1707 - acc: 0.935 - ETA: 2s - loss: 0.1672 - acc: 0.936 - ETA: 2s - loss: 0.1698 - acc: 0.936 - ETA: 2s - loss: 0.1748 - acc: 0.933 - ETA: 2s - loss: 0.1739 - acc: 0.934 - ETA: 2s - loss: 0.1707 - acc: 0.935 - ETA: 2s - loss: 0.1678 - acc: 0.937 - ETA: 2s - loss: 0.1650 - acc: 0.939 - ETA: 2s - loss: 0.1641 - acc: 0.939 - ETA: 1s - loss: 0.1639 - acc: 0.938 - ETA: 1s - loss: 0.1629 - acc: 0.939 - ETA: 1s - loss: 0.1631 - acc: 0.938 - ETA: 1s - loss: 0.1630 - acc: 0.939 - ETA: 1s - loss: 0.1644 - acc: 0.938 - ETA: 1s - loss: 0.1637 - acc: 0.937 - ETA: 1s - loss: 0.1639 - acc: 0.937 - ETA: 1s - loss: 0.1616 - acc: 0.938 - ETA: 1s - loss: 0.1608 - acc: 0.939 - ETA: 1s - loss: 0.1579 - acc: 0.940 - ETA: 0s - loss: 0.1558 - acc: 0.941 - ETA: 0s - loss: 0.1578 - acc: 0.940 - ETA: 0s - loss: 0.1600 - acc: 0.939 - ETA: 0s - loss: 0.1594 - acc: 0.940 - ETA: 0s - loss: 0.1594 - acc: 0.939 - ETA: 0s - loss: 0.1606 - acc: 0.939 - ETA: 0s - loss: 0.1595 - acc: 0.939 - ETA: 0s - loss: 0.1616 - acc: 0.938 - ETA: 0s - loss: 0.1609 - acc: 0.938 - 5s 104ms/step - loss: 0.1637 - acc: 0.9371\n",
      "Epoch 48/10000\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1605 - acc: 0.954 - ETA: 4s - loss: 0.1314 - acc: 0.960 - ETA: 4s - loss: 0.1317 - acc: 0.947 - ETA: 4s - loss: 0.1650 - acc: 0.934 - ETA: 4s - loss: 0.1582 - acc: 0.936 - ETA: 4s - loss: 0.1603 - acc: 0.939 - ETA: 4s - loss: 0.1628 - acc: 0.943 - ETA: 4s - loss: 0.1659 - acc: 0.936 - ETA: 4s - loss: 0.1597 - acc: 0.939 - ETA: 4s - loss: 0.1540 - acc: 0.940 - ETA: 3s - loss: 0.1554 - acc: 0.940 - ETA: 3s - loss: 0.1533 - acc: 0.941 - ETA: 3s - loss: 0.1506 - acc: 0.943 - ETA: 3s - loss: 0.1538 - acc: 0.943 - ETA: 3s - loss: 0.1514 - acc: 0.945 - ETA: 3s - loss: 0.1492 - acc: 0.947 - ETA: 3s - loss: 0.1510 - acc: 0.946 - ETA: 3s - loss: 0.1491 - acc: 0.945 - ETA: 3s - loss: 0.1479 - acc: 0.944 - ETA: 3s - loss: 0.1479 - acc: 0.943 - ETA: 2s - loss: 0.1519 - acc: 0.942 - ETA: 2s - loss: 0.1573 - acc: 0.941 - ETA: 2s - loss: 0.1584 - acc: 0.941 - ETA: 2s - loss: 0.1554 - acc: 0.943 - ETA: 2s - loss: 0.1569 - acc: 0.942 - ETA: 2s - loss: 0.1568 - acc: 0.942 - ETA: 2s - loss: 0.1539 - acc: 0.944 - ETA: 2s - loss: 0.1510 - acc: 0.945 - ETA: 2s - loss: 0.1483 - acc: 0.946 - ETA: 1s - loss: 0.1478 - acc: 0.946 - ETA: 1s - loss: 0.1503 - acc: 0.944 - ETA: 1s - loss: 0.1521 - acc: 0.944 - ETA: 1s - loss: 0.1550 - acc: 0.943 - ETA: 1s - loss: 0.1545 - acc: 0.944 - ETA: 1s - loss: 0.1561 - acc: 0.943 - ETA: 1s - loss: 0.1550 - acc: 0.943 - ETA: 1s - loss: 0.1560 - acc: 0.943 - ETA: 1s - loss: 0.1564 - acc: 0.943 - ETA: 1s - loss: 0.1543 - acc: 0.943 - ETA: 0s - loss: 0.1531 - acc: 0.944 - ETA: 0s - loss: 0.1537 - acc: 0.944 - ETA: 0s - loss: 0.1518 - acc: 0.945 - ETA: 0s - loss: 0.1504 - acc: 0.946 - ETA: 0s - loss: 0.1506 - acc: 0.945 - ETA: 0s - loss: 0.1496 - acc: 0.946 - ETA: 0s - loss: 0.1525 - acc: 0.946 - ETA: 0s - loss: 0.1512 - acc: 0.946 - ETA: 0s - loss: 0.1524 - acc: 0.945 - 5s 105ms/step - loss: 0.1515 - acc: 0.9466\n",
      "Epoch 49/10000\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1342 - acc: 0.931 - ETA: 4s - loss: 0.1172 - acc: 0.948 - ETA: 4s - loss: 0.1284 - acc: 0.950 - ETA: 4s - loss: 0.1403 - acc: 0.940 - ETA: 4s - loss: 0.1363 - acc: 0.945 - ETA: 4s - loss: 0.1301 - acc: 0.945 - ETA: 4s - loss: 0.1317 - acc: 0.944 - ETA: 4s - loss: 0.1498 - acc: 0.942 - ETA: 4s - loss: 0.1538 - acc: 0.940 - ETA: 4s - loss: 0.1495 - acc: 0.944 - ETA: 3s - loss: 0.1481 - acc: 0.944 - ETA: 3s - loss: 0.1438 - acc: 0.947 - ETA: 3s - loss: 0.1421 - acc: 0.948 - ETA: 3s - loss: 0.1532 - acc: 0.943 - ETA: 3s - loss: 0.1614 - acc: 0.942 - ETA: 3s - loss: 0.1583 - acc: 0.943 - ETA: 3s - loss: 0.1617 - acc: 0.941 - ETA: 3s - loss: 0.1613 - acc: 0.940 - ETA: 3s - loss: 0.1600 - acc: 0.939 - ETA: 2s - loss: 0.1588 - acc: 0.939 - ETA: 2s - loss: 0.1560 - acc: 0.941 - ETA: 2s - loss: 0.1585 - acc: 0.941 - ETA: 2s - loss: 0.1595 - acc: 0.941 - ETA: 2s - loss: 0.1584 - acc: 0.941 - ETA: 2s - loss: 0.1590 - acc: 0.941 - ETA: 2s - loss: 0.1594 - acc: 0.941 - ETA: 2s - loss: 0.1615 - acc: 0.939 - ETA: 2s - loss: 0.1574 - acc: 0.942 - ETA: 2s - loss: 0.1586 - acc: 0.941 - ETA: 1s - loss: 0.1609 - acc: 0.940 - ETA: 1s - loss: 0.1612 - acc: 0.939 - ETA: 1s - loss: 0.1605 - acc: 0.940 - ETA: 1s - loss: 0.1607 - acc: 0.939 - ETA: 1s - loss: 0.1592 - acc: 0.940 - ETA: 1s - loss: 0.1581 - acc: 0.941 - ETA: 1s - loss: 0.1581 - acc: 0.941 - ETA: 1s - loss: 0.1587 - acc: 0.941 - ETA: 1s - loss: 0.1579 - acc: 0.942 - ETA: 1s - loss: 0.1580 - acc: 0.942 - ETA: 0s - loss: 0.1583 - acc: 0.942 - ETA: 0s - loss: 0.1578 - acc: 0.943 - ETA: 0s - loss: 0.1577 - acc: 0.943 - ETA: 0s - loss: 0.1570 - acc: 0.943 - ETA: 0s - loss: 0.1571 - acc: 0.943 - ETA: 0s - loss: 0.1572 - acc: 0.943 - ETA: 0s - loss: 0.1568 - acc: 0.943 - ETA: 0s - loss: 0.1555 - acc: 0.943 - ETA: 0s - loss: 0.1546 - acc: 0.943 - 5s 104ms/step - loss: 0.1556 - acc: 0.9437\n",
      "Epoch 50/10000\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1534 - acc: 0.931 - ETA: 4s - loss: 0.1391 - acc: 0.937 - ETA: 4s - loss: 0.1470 - acc: 0.939 - ETA: 4s - loss: 0.1701 - acc: 0.940 - ETA: 4s - loss: 0.1489 - acc: 0.950 - ETA: 4s - loss: 0.1637 - acc: 0.941 - ETA: 4s - loss: 0.1581 - acc: 0.943 - ETA: 4s - loss: 0.1717 - acc: 0.938 - ETA: 4s - loss: 0.1800 - acc: 0.934 - ETA: 4s - loss: 0.1790 - acc: 0.935 - ETA: 3s - loss: 0.1708 - acc: 0.939 - ETA: 3s - loss: 0.1685 - acc: 0.938 - ETA: 3s - loss: 0.1653 - acc: 0.941 - ETA: 3s - loss: 0.1620 - acc: 0.941 - ETA: 3s - loss: 0.1569 - acc: 0.944 - ETA: 3s - loss: 0.1565 - acc: 0.944 - ETA: 3s - loss: 0.1531 - acc: 0.946 - ETA: 3s - loss: 0.1508 - acc: 0.945 - ETA: 3s - loss: 0.1532 - acc: 0.944 - ETA: 3s - loss: 0.1548 - acc: 0.944 - ETA: 2s - loss: 0.1550 - acc: 0.944 - ETA: 2s - loss: 0.1590 - acc: 0.942 - ETA: 2s - loss: 0.1573 - acc: 0.943 - ETA: 2s - loss: 0.1562 - acc: 0.943 - ETA: 2s - loss: 0.1572 - acc: 0.943 - ETA: 2s - loss: 0.1558 - acc: 0.944 - ETA: 2s - loss: 0.1583 - acc: 0.944 - ETA: 2s - loss: 0.1562 - acc: 0.945 - ETA: 2s - loss: 0.1563 - acc: 0.945 - ETA: 1s - loss: 0.1559 - acc: 0.944 - ETA: 1s - loss: 0.1550 - acc: 0.944 - ETA: 1s - loss: 0.1551 - acc: 0.943 - ETA: 1s - loss: 0.1561 - acc: 0.943 - ETA: 1s - loss: 0.1541 - acc: 0.944 - ETA: 1s - loss: 0.1554 - acc: 0.944 - ETA: 1s - loss: 0.1542 - acc: 0.945 - ETA: 1s - loss: 0.1533 - acc: 0.945 - ETA: 1s - loss: 0.1518 - acc: 0.946 - ETA: 1s - loss: 0.1557 - acc: 0.944 - ETA: 0s - loss: 0.1556 - acc: 0.944 - ETA: 0s - loss: 0.1552 - acc: 0.944 - ETA: 0s - loss: 0.1545 - acc: 0.945 - ETA: 0s - loss: 0.1556 - acc: 0.944 - ETA: 0s - loss: 0.1563 - acc: 0.944 - ETA: 0s - loss: 0.1566 - acc: 0.944 - ETA: 0s - loss: 0.1550 - acc: 0.944 - ETA: 0s - loss: 0.1550 - acc: 0.943 - ETA: 0s - loss: 0.1550 - acc: 0.944 - 5s 104ms/step - loss: 0.1533 - acc: 0.9446\n",
      "Epoch 51/10000\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1760 - acc: 0.943 - ETA: 5s - loss: 0.1116 - acc: 0.965 - ETA: 5s - loss: 0.1374 - acc: 0.954 - ETA: 4s - loss: 0.1295 - acc: 0.951 - ETA: 4s - loss: 0.1600 - acc: 0.938 - ETA: 4s - loss: 0.1444 - acc: 0.945 - ETA: 4s - loss: 0.1319 - acc: 0.949 - ETA: 4s - loss: 0.1272 - acc: 0.951 - ETA: 4s - loss: 0.1263 - acc: 0.953 - ETA: 4s - loss: 0.1311 - acc: 0.952 - ETA: 4s - loss: 0.1392 - acc: 0.950 - ETA: 3s - loss: 0.1424 - acc: 0.950 - ETA: 3s - loss: 0.1467 - acc: 0.948 - ETA: 3s - loss: 0.1441 - acc: 0.948 - ETA: 3s - loss: 0.1429 - acc: 0.947 - ETA: 3s - loss: 0.1403 - acc: 0.948 - ETA: 3s - loss: 0.1462 - acc: 0.946 - ETA: 3s - loss: 0.1486 - acc: 0.946 - ETA: 3s - loss: 0.1486 - acc: 0.946 - ETA: 3s - loss: 0.1513 - acc: 0.946 - ETA: 2s - loss: 0.1497 - acc: 0.947 - ETA: 2s - loss: 0.1466 - acc: 0.948 - ETA: 2s - loss: 0.1500 - acc: 0.947 - ETA: 2s - loss: 0.1498 - acc: 0.947 - ETA: 2s - loss: 0.1500 - acc: 0.946 - ETA: 2s - loss: 0.1535 - acc: 0.945 - ETA: 2s - loss: 0.1520 - acc: 0.944 - ETA: 2s - loss: 0.1508 - acc: 0.945 - ETA: 2s - loss: 0.1486 - acc: 0.946 - ETA: 1s - loss: 0.1474 - acc: 0.945 - ETA: 1s - loss: 0.1477 - acc: 0.945 - ETA: 1s - loss: 0.1488 - acc: 0.944 - ETA: 1s - loss: 0.1483 - acc: 0.944 - ETA: 1s - loss: 0.1481 - acc: 0.944 - ETA: 1s - loss: 0.1486 - acc: 0.944 - ETA: 1s - loss: 0.1516 - acc: 0.944 - ETA: 1s - loss: 0.1507 - acc: 0.944 - ETA: 1s - loss: 0.1493 - acc: 0.945 - ETA: 1s - loss: 0.1485 - acc: 0.945 - ETA: 0s - loss: 0.1526 - acc: 0.944 - ETA: 0s - loss: 0.1513 - acc: 0.945 - ETA: 0s - loss: 0.1515 - acc: 0.945 - ETA: 0s - loss: 0.1527 - acc: 0.944 - ETA: 0s - loss: 0.1528 - acc: 0.944 - ETA: 0s - loss: 0.1506 - acc: 0.945 - ETA: 0s - loss: 0.1514 - acc: 0.944 - ETA: 0s - loss: 0.1500 - acc: 0.945 - ETA: 0s - loss: 0.1485 - acc: 0.946 - 5s 104ms/step - loss: 0.1482 - acc: 0.9457\n",
      "Epoch 52/10000\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2294 - acc: 0.920 - ETA: 4s - loss: 0.1766 - acc: 0.934 - ETA: 4s - loss: 0.1740 - acc: 0.929 - ETA: 4s - loss: 0.1613 - acc: 0.935 - ETA: 4s - loss: 0.1478 - acc: 0.944 - ETA: 4s - loss: 0.1406 - acc: 0.949 - ETA: 4s - loss: 0.1451 - acc: 0.945 - ETA: 4s - loss: 0.1373 - acc: 0.949 - ETA: 4s - loss: 0.1289 - acc: 0.953 - ETA: 4s - loss: 0.1244 - acc: 0.953 - ETA: 3s - loss: 0.1249 - acc: 0.955 - ETA: 3s - loss: 0.1250 - acc: 0.955 - ETA: 3s - loss: 0.1245 - acc: 0.955 - ETA: 3s - loss: 0.1228 - acc: 0.955 - ETA: 3s - loss: 0.1283 - acc: 0.954 - ETA: 3s - loss: 0.1307 - acc: 0.953 - ETA: 3s - loss: 0.1271 - acc: 0.955 - ETA: 3s - loss: 0.1268 - acc: 0.955 - ETA: 3s - loss: 0.1313 - acc: 0.953 - ETA: 3s - loss: 0.1319 - acc: 0.951 - ETA: 2s - loss: 0.1333 - acc: 0.950 - ETA: 2s - loss: 0.1344 - acc: 0.949 - ETA: 2s - loss: 0.1373 - acc: 0.949 - ETA: 2s - loss: 0.1439 - acc: 0.947 - ETA: 2s - loss: 0.1445 - acc: 0.946 - ETA: 2s - loss: 0.1482 - acc: 0.945 - ETA: 2s - loss: 0.1486 - acc: 0.945 - ETA: 2s - loss: 0.1506 - acc: 0.945 - ETA: 2s - loss: 0.1526 - acc: 0.944 - ETA: 1s - loss: 0.1530 - acc: 0.944 - ETA: 1s - loss: 0.1527 - acc: 0.943 - ETA: 1s - loss: 0.1537 - acc: 0.943 - ETA: 1s - loss: 0.1540 - acc: 0.943 - ETA: 1s - loss: 0.1528 - acc: 0.944 - ETA: 1s - loss: 0.1521 - acc: 0.943 - ETA: 1s - loss: 0.1509 - acc: 0.943 - ETA: 1s - loss: 0.1517 - acc: 0.943 - ETA: 1s - loss: 0.1540 - acc: 0.941 - ETA: 1s - loss: 0.1531 - acc: 0.942 - ETA: 0s - loss: 0.1526 - acc: 0.942 - ETA: 0s - loss: 0.1530 - acc: 0.942 - ETA: 0s - loss: 0.1517 - acc: 0.942 - ETA: 0s - loss: 0.1514 - acc: 0.942 - ETA: 0s - loss: 0.1512 - acc: 0.942 - ETA: 0s - loss: 0.1522 - acc: 0.942 - ETA: 0s - loss: 0.1547 - acc: 0.941 - ETA: 0s - loss: 0.1549 - acc: 0.941 - ETA: 0s - loss: 0.1541 - acc: 0.942 - 5s 104ms/step - loss: 0.1550 - acc: 0.9419\n",
      "Epoch 53/10000\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2483 - acc: 0.897 - ETA: 4s - loss: 0.2376 - acc: 0.909 - ETA: 4s - loss: 0.2116 - acc: 0.912 - ETA: 4s - loss: 0.2109 - acc: 0.914 - ETA: 4s - loss: 0.1952 - acc: 0.925 - ETA: 4s - loss: 0.1842 - acc: 0.931 - ETA: 4s - loss: 0.1713 - acc: 0.938 - ETA: 4s - loss: 0.1718 - acc: 0.940 - ETA: 4s - loss: 0.1622 - acc: 0.944 - ETA: 4s - loss: 0.1539 - acc: 0.946 - ETA: 3s - loss: 0.1579 - acc: 0.944 - ETA: 3s - loss: 0.1582 - acc: 0.943 - ETA: 3s - loss: 0.1541 - acc: 0.944 - ETA: 3s - loss: 0.1528 - acc: 0.944 - ETA: 3s - loss: 0.1559 - acc: 0.941 - ETA: 3s - loss: 0.1547 - acc: 0.941 - ETA: 3s - loss: 0.1585 - acc: 0.940 - ETA: 3s - loss: 0.1545 - acc: 0.941 - ETA: 3s - loss: 0.1558 - acc: 0.940 - ETA: 3s - loss: 0.1564 - acc: 0.940 - ETA: 2s - loss: 0.1553 - acc: 0.941 - ETA: 2s - loss: 0.1528 - acc: 0.942 - ETA: 2s - loss: 0.1530 - acc: 0.941 - ETA: 2s - loss: 0.1564 - acc: 0.940 - ETA: 2s - loss: 0.1548 - acc: 0.942 - ETA: 2s - loss: 0.1530 - acc: 0.943 - ETA: 2s - loss: 0.1542 - acc: 0.942 - ETA: 2s - loss: 0.1551 - acc: 0.942 - ETA: 2s - loss: 0.1527 - acc: 0.943 - ETA: 1s - loss: 0.1514 - acc: 0.943 - ETA: 1s - loss: 0.1502 - acc: 0.944 - ETA: 1s - loss: 0.1504 - acc: 0.944 - ETA: 1s - loss: 0.1499 - acc: 0.945 - ETA: 1s - loss: 0.1511 - acc: 0.944 - ETA: 1s - loss: 0.1506 - acc: 0.943 - ETA: 1s - loss: 0.1516 - acc: 0.943 - ETA: 1s - loss: 0.1531 - acc: 0.942 - ETA: 1s - loss: 0.1535 - acc: 0.942 - ETA: 1s - loss: 0.1541 - acc: 0.942 - ETA: 0s - loss: 0.1543 - acc: 0.942 - ETA: 0s - loss: 0.1539 - acc: 0.942 - ETA: 0s - loss: 0.1543 - acc: 0.941 - ETA: 0s - loss: 0.1537 - acc: 0.941 - ETA: 0s - loss: 0.1525 - acc: 0.942 - ETA: 0s - loss: 0.1532 - acc: 0.942 - ETA: 0s - loss: 0.1534 - acc: 0.941 - ETA: 0s - loss: 0.1546 - acc: 0.941 - ETA: 0s - loss: 0.1567 - acc: 0.940 - 5s 104ms/step - loss: 0.1564 - acc: 0.9396\n",
      "Epoch 54/10000\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1306 - acc: 0.954 - ETA: 4s - loss: 0.1258 - acc: 0.948 - ETA: 4s - loss: 0.1228 - acc: 0.947 - ETA: 4s - loss: 0.1344 - acc: 0.946 - ETA: 4s - loss: 0.1249 - acc: 0.945 - ETA: 4s - loss: 0.1299 - acc: 0.943 - ETA: 4s - loss: 0.1343 - acc: 0.944 - ETA: 4s - loss: 0.1348 - acc: 0.946 - ETA: 4s - loss: 0.1406 - acc: 0.943 - ETA: 4s - loss: 0.1428 - acc: 0.944 - ETA: 3s - loss: 0.1376 - acc: 0.947 - ETA: 3s - loss: 0.1406 - acc: 0.945 - ETA: 3s - loss: 0.1332 - acc: 0.949 - ETA: 3s - loss: 0.1294 - acc: 0.951 - ETA: 3s - loss: 0.1296 - acc: 0.950 - ETA: 3s - loss: 0.1297 - acc: 0.950 - ETA: 3s - loss: 0.1288 - acc: 0.951 - ETA: 3s - loss: 0.1306 - acc: 0.952 - ETA: 3s - loss: 0.1352 - acc: 0.951 - ETA: 3s - loss: 0.1394 - acc: 0.948 - ETA: 2s - loss: 0.1396 - acc: 0.948 - ETA: 2s - loss: 0.1426 - acc: 0.947 - ETA: 2s - loss: 0.1444 - acc: 0.946 - ETA: 2s - loss: 0.1428 - acc: 0.947 - ETA: 2s - loss: 0.1434 - acc: 0.947 - ETA: 2s - loss: 0.1417 - acc: 0.948 - ETA: 2s - loss: 0.1423 - acc: 0.947 - ETA: 2s - loss: 0.1460 - acc: 0.946 - ETA: 2s - loss: 0.1498 - acc: 0.945 - ETA: 1s - loss: 0.1532 - acc: 0.943 - ETA: 1s - loss: 0.1540 - acc: 0.943 - ETA: 1s - loss: 0.1531 - acc: 0.943 - ETA: 1s - loss: 0.1526 - acc: 0.943 - ETA: 1s - loss: 0.1519 - acc: 0.943 - ETA: 1s - loss: 0.1524 - acc: 0.943 - ETA: 1s - loss: 0.1554 - acc: 0.942 - ETA: 1s - loss: 0.1546 - acc: 0.943 - ETA: 1s - loss: 0.1544 - acc: 0.943 - ETA: 1s - loss: 0.1545 - acc: 0.943 - ETA: 0s - loss: 0.1529 - acc: 0.942 - ETA: 0s - loss: 0.1524 - acc: 0.942 - ETA: 0s - loss: 0.1552 - acc: 0.941 - ETA: 0s - loss: 0.1533 - acc: 0.942 - ETA: 0s - loss: 0.1510 - acc: 0.943 - ETA: 0s - loss: 0.1533 - acc: 0.942 - ETA: 0s - loss: 0.1525 - acc: 0.943 - ETA: 0s - loss: 0.1534 - acc: 0.943 - ETA: 0s - loss: 0.1519 - acc: 0.943 - 5s 105ms/step - loss: 0.1522 - acc: 0.9432\n",
      "Epoch 55/10000\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1847 - acc: 0.909 - ETA: 4s - loss: 0.2009 - acc: 0.920 - ETA: 4s - loss: 0.1825 - acc: 0.924 - ETA: 4s - loss: 0.1612 - acc: 0.937 - ETA: 4s - loss: 0.1725 - acc: 0.934 - ETA: 4s - loss: 0.1584 - acc: 0.937 - ETA: 4s - loss: 0.1544 - acc: 0.939 - ETA: 4s - loss: 0.1536 - acc: 0.940 - ETA: 4s - loss: 0.1530 - acc: 0.943 - ETA: 4s - loss: 0.1520 - acc: 0.945 - ETA: 3s - loss: 0.1578 - acc: 0.944 - ETA: 3s - loss: 0.1626 - acc: 0.942 - ETA: 3s - loss: 0.1592 - acc: 0.944 - ETA: 3s - loss: 0.1633 - acc: 0.944 - ETA: 3s - loss: 0.1647 - acc: 0.944 - ETA: 3s - loss: 0.1655 - acc: 0.943 - ETA: 3s - loss: 0.1661 - acc: 0.940 - ETA: 3s - loss: 0.1648 - acc: 0.940 - ETA: 3s - loss: 0.1656 - acc: 0.939 - ETA: 3s - loss: 0.1640 - acc: 0.940 - ETA: 2s - loss: 0.1604 - acc: 0.942 - ETA: 2s - loss: 0.1567 - acc: 0.944 - ETA: 2s - loss: 0.1554 - acc: 0.943 - ETA: 2s - loss: 0.1542 - acc: 0.944 - ETA: 2s - loss: 0.1545 - acc: 0.944 - ETA: 2s - loss: 0.1583 - acc: 0.941 - ETA: 2s - loss: 0.1620 - acc: 0.940 - ETA: 2s - loss: 0.1614 - acc: 0.940 - ETA: 2s - loss: 0.1633 - acc: 0.940 - ETA: 1s - loss: 0.1602 - acc: 0.941 - ETA: 1s - loss: 0.1593 - acc: 0.941 - ETA: 1s - loss: 0.1587 - acc: 0.941 - ETA: 1s - loss: 0.1588 - acc: 0.941 - ETA: 1s - loss: 0.1576 - acc: 0.942 - ETA: 1s - loss: 0.1564 - acc: 0.942 - ETA: 1s - loss: 0.1584 - acc: 0.941 - ETA: 1s - loss: 0.1572 - acc: 0.941 - ETA: 1s - loss: 0.1554 - acc: 0.941 - ETA: 1s - loss: 0.1558 - acc: 0.941 - ETA: 0s - loss: 0.1548 - acc: 0.942 - ETA: 0s - loss: 0.1558 - acc: 0.941 - ETA: 0s - loss: 0.1531 - acc: 0.942 - ETA: 0s - loss: 0.1527 - acc: 0.942 - ETA: 0s - loss: 0.1553 - acc: 0.942 - ETA: 0s - loss: 0.1556 - acc: 0.942 - ETA: 0s - loss: 0.1545 - acc: 0.943 - ETA: 0s - loss: 0.1553 - acc: 0.943 - ETA: 0s - loss: 0.1551 - acc: 0.942 - 5s 104ms/step - loss: 0.1542 - acc: 0.9431\n",
      "Epoch 56/10000\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.0727 - acc: 0.977 - ETA: 4s - loss: 0.0955 - acc: 0.977 - ETA: 4s - loss: 0.1193 - acc: 0.958 - ETA: 4s - loss: 0.1372 - acc: 0.948 - ETA: 4s - loss: 0.1382 - acc: 0.945 - ETA: 4s - loss: 0.1360 - acc: 0.945 - ETA: 4s - loss: 0.1424 - acc: 0.936 - ETA: 4s - loss: 0.1626 - acc: 0.931 - ETA: 4s - loss: 0.1714 - acc: 0.930 - ETA: 4s - loss: 0.1668 - acc: 0.933 - ETA: 3s - loss: 0.1628 - acc: 0.936 - ETA: 3s - loss: 0.1619 - acc: 0.937 - ETA: 3s - loss: 0.1564 - acc: 0.939 - ETA: 3s - loss: 0.1512 - acc: 0.941 - ETA: 3s - loss: 0.1505 - acc: 0.942 - ETA: 3s - loss: 0.1496 - acc: 0.944 - ETA: 3s - loss: 0.1464 - acc: 0.945 - ETA: 3s - loss: 0.1453 - acc: 0.946 - ETA: 3s - loss: 0.1488 - acc: 0.944 - ETA: 3s - loss: 0.1460 - acc: 0.945 - ETA: 2s - loss: 0.1433 - acc: 0.945 - ETA: 2s - loss: 0.1527 - acc: 0.943 - ETA: 2s - loss: 0.1578 - acc: 0.942 - ETA: 2s - loss: 0.1590 - acc: 0.942 - ETA: 2s - loss: 0.1567 - acc: 0.942 - ETA: 2s - loss: 0.1560 - acc: 0.941 - ETA: 2s - loss: 0.1542 - acc: 0.942 - ETA: 2s - loss: 0.1515 - acc: 0.944 - ETA: 2s - loss: 0.1490 - acc: 0.945 - ETA: 1s - loss: 0.1473 - acc: 0.946 - ETA: 1s - loss: 0.1483 - acc: 0.945 - ETA: 1s - loss: 0.1480 - acc: 0.945 - ETA: 1s - loss: 0.1499 - acc: 0.944 - ETA: 1s - loss: 0.1506 - acc: 0.944 - ETA: 1s - loss: 0.1511 - acc: 0.944 - ETA: 1s - loss: 0.1522 - acc: 0.944 - ETA: 1s - loss: 0.1519 - acc: 0.944 - ETA: 1s - loss: 0.1531 - acc: 0.944 - ETA: 1s - loss: 0.1521 - acc: 0.944 - ETA: 0s - loss: 0.1507 - acc: 0.945 - ETA: 0s - loss: 0.1512 - acc: 0.945 - ETA: 0s - loss: 0.1506 - acc: 0.945 - ETA: 0s - loss: 0.1495 - acc: 0.945 - ETA: 0s - loss: 0.1500 - acc: 0.945 - ETA: 0s - loss: 0.1499 - acc: 0.945 - ETA: 0s - loss: 0.1519 - acc: 0.944 - ETA: 0s - loss: 0.1509 - acc: 0.944 - ETA: 0s - loss: 0.1511 - acc: 0.944 - 5s 104ms/step - loss: 0.1516 - acc: 0.9444\n",
      "Epoch 57/10000\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2381 - acc: 0.920 - ETA: 4s - loss: 0.1896 - acc: 0.937 - ETA: 4s - loss: 0.1937 - acc: 0.935 - ETA: 4s - loss: 0.1771 - acc: 0.940 - ETA: 4s - loss: 0.1652 - acc: 0.943 - ETA: 4s - loss: 0.1593 - acc: 0.945 - ETA: 4s - loss: 0.1537 - acc: 0.943 - ETA: 4s - loss: 0.1549 - acc: 0.944 - ETA: 4s - loss: 0.1481 - acc: 0.947 - ETA: 4s - loss: 0.1504 - acc: 0.943 - ETA: 3s - loss: 0.1490 - acc: 0.943 - ETA: 3s - loss: 0.1477 - acc: 0.944 - ETA: 3s - loss: 0.1489 - acc: 0.945 - ETA: 3s - loss: 0.1534 - acc: 0.944 - ETA: 3s - loss: 0.1496 - acc: 0.944 - ETA: 3s - loss: 0.1511 - acc: 0.943 - ETA: 3s - loss: 0.1483 - acc: 0.944 - ETA: 3s - loss: 0.1519 - acc: 0.944 - ETA: 3s - loss: 0.1517 - acc: 0.945 - ETA: 3s - loss: 0.1523 - acc: 0.944 - ETA: 2s - loss: 0.1553 - acc: 0.943 - ETA: 2s - loss: 0.1587 - acc: 0.940 - ETA: 2s - loss: 0.1641 - acc: 0.938 - ETA: 2s - loss: 0.1642 - acc: 0.937 - ETA: 2s - loss: 0.1665 - acc: 0.936 - ETA: 2s - loss: 0.1671 - acc: 0.936 - ETA: 2s - loss: 0.1668 - acc: 0.935 - ETA: 2s - loss: 0.1676 - acc: 0.936 - ETA: 2s - loss: 0.1657 - acc: 0.937 - ETA: 1s - loss: 0.1626 - acc: 0.938 - ETA: 1s - loss: 0.1622 - acc: 0.937 - ETA: 1s - loss: 0.1625 - acc: 0.937 - ETA: 1s - loss: 0.1615 - acc: 0.937 - ETA: 1s - loss: 0.1612 - acc: 0.938 - ETA: 1s - loss: 0.1619 - acc: 0.937 - ETA: 1s - loss: 0.1617 - acc: 0.937 - ETA: 1s - loss: 0.1620 - acc: 0.937 - ETA: 1s - loss: 0.1631 - acc: 0.936 - ETA: 1s - loss: 0.1642 - acc: 0.937 - ETA: 0s - loss: 0.1649 - acc: 0.936 - ETA: 0s - loss: 0.1641 - acc: 0.937 - ETA: 0s - loss: 0.1618 - acc: 0.938 - ETA: 0s - loss: 0.1660 - acc: 0.937 - ETA: 0s - loss: 0.1674 - acc: 0.937 - ETA: 0s - loss: 0.1667 - acc: 0.938 - ETA: 0s - loss: 0.1702 - acc: 0.937 - ETA: 0s - loss: 0.1695 - acc: 0.938 - ETA: 0s - loss: 0.1685 - acc: 0.938 - 5s 104ms/step - loss: 0.1694 - acc: 0.9380\n",
      "Epoch 58/10000\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1484 - acc: 0.954 - ETA: 4s - loss: 0.1191 - acc: 0.965 - ETA: 4s - loss: 0.1015 - acc: 0.973 - ETA: 4s - loss: 0.1087 - acc: 0.968 - ETA: 4s - loss: 0.1496 - acc: 0.956 - ETA: 4s - loss: 0.1507 - acc: 0.958 - ETA: 4s - loss: 0.1464 - acc: 0.956 - ETA: 4s - loss: 0.1532 - acc: 0.950 - ETA: 4s - loss: 0.1495 - acc: 0.952 - ETA: 4s - loss: 0.1461 - acc: 0.951 - ETA: 3s - loss: 0.1521 - acc: 0.946 - ETA: 3s - loss: 0.1479 - acc: 0.948 - ETA: 3s - loss: 0.1425 - acc: 0.951 - ETA: 3s - loss: 0.1391 - acc: 0.952 - ETA: 3s - loss: 0.1337 - acc: 0.955 - ETA: 3s - loss: 0.1366 - acc: 0.953 - ETA: 3s - loss: 0.1370 - acc: 0.953 - ETA: 3s - loss: 0.1378 - acc: 0.952 - ETA: 3s - loss: 0.1354 - acc: 0.953 - ETA: 2s - loss: 0.1377 - acc: 0.952 - ETA: 2s - loss: 0.1353 - acc: 0.954 - ETA: 2s - loss: 0.1336 - acc: 0.954 - ETA: 2s - loss: 0.1351 - acc: 0.953 - ETA: 2s - loss: 0.1336 - acc: 0.953 - ETA: 2s - loss: 0.1345 - acc: 0.954 - ETA: 2s - loss: 0.1343 - acc: 0.954 - ETA: 2s - loss: 0.1344 - acc: 0.953 - ETA: 2s - loss: 0.1377 - acc: 0.953 - ETA: 2s - loss: 0.1412 - acc: 0.952 - ETA: 1s - loss: 0.1416 - acc: 0.952 - ETA: 1s - loss: 0.1401 - acc: 0.952 - ETA: 1s - loss: 0.1400 - acc: 0.952 - ETA: 1s - loss: 0.1406 - acc: 0.952 - ETA: 1s - loss: 0.1402 - acc: 0.952 - ETA: 1s - loss: 0.1392 - acc: 0.952 - ETA: 1s - loss: 0.1400 - acc: 0.952 - ETA: 1s - loss: 0.1428 - acc: 0.952 - ETA: 1s - loss: 0.1421 - acc: 0.952 - ETA: 1s - loss: 0.1421 - acc: 0.952 - ETA: 0s - loss: 0.1412 - acc: 0.951 - ETA: 0s - loss: 0.1407 - acc: 0.952 - ETA: 0s - loss: 0.1406 - acc: 0.951 - ETA: 0s - loss: 0.1440 - acc: 0.950 - ETA: 0s - loss: 0.1465 - acc: 0.949 - ETA: 0s - loss: 0.1458 - acc: 0.949 - ETA: 0s - loss: 0.1477 - acc: 0.949 - ETA: 0s - loss: 0.1475 - acc: 0.949 - ETA: 0s - loss: 0.1470 - acc: 0.948 - 5s 103ms/step - loss: 0.1491 - acc: 0.9477\n",
      "Epoch 59/10000\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1850 - acc: 0.909 - ETA: 4s - loss: 0.1717 - acc: 0.931 - ETA: 4s - loss: 0.1587 - acc: 0.943 - ETA: 4s - loss: 0.1503 - acc: 0.940 - ETA: 4s - loss: 0.1405 - acc: 0.943 - ETA: 4s - loss: 0.1364 - acc: 0.941 - ETA: 4s - loss: 0.1357 - acc: 0.944 - ETA: 4s - loss: 0.1252 - acc: 0.951 - ETA: 4s - loss: 0.1329 - acc: 0.947 - ETA: 4s - loss: 0.1244 - acc: 0.952 - ETA: 3s - loss: 0.1266 - acc: 0.952 - ETA: 3s - loss: 0.1228 - acc: 0.954 - ETA: 3s - loss: 0.1276 - acc: 0.954 - ETA: 3s - loss: 0.1266 - acc: 0.955 - ETA: 3s - loss: 0.1280 - acc: 0.953 - ETA: 3s - loss: 0.1280 - acc: 0.953 - ETA: 3s - loss: 0.1280 - acc: 0.953 - ETA: 3s - loss: 0.1272 - acc: 0.954 - ETA: 3s - loss: 0.1300 - acc: 0.954 - ETA: 2s - loss: 0.1292 - acc: 0.954 - ETA: 2s - loss: 0.1329 - acc: 0.952 - ETA: 2s - loss: 0.1308 - acc: 0.953 - ETA: 2s - loss: 0.1306 - acc: 0.952 - ETA: 2s - loss: 0.1334 - acc: 0.952 - ETA: 2s - loss: 0.1344 - acc: 0.952 - ETA: 2s - loss: 0.1406 - acc: 0.950 - ETA: 2s - loss: 0.1370 - acc: 0.952 - ETA: 2s - loss: 0.1354 - acc: 0.952 - ETA: 2s - loss: 0.1339 - acc: 0.953 - ETA: 1s - loss: 0.1369 - acc: 0.952 - ETA: 1s - loss: 0.1369 - acc: 0.952 - ETA: 1s - loss: 0.1374 - acc: 0.952 - ETA: 1s - loss: 0.1362 - acc: 0.952 - ETA: 1s - loss: 0.1352 - acc: 0.953 - ETA: 1s - loss: 0.1346 - acc: 0.953 - ETA: 1s - loss: 0.1338 - acc: 0.953 - ETA: 1s - loss: 0.1370 - acc: 0.951 - ETA: 1s - loss: 0.1348 - acc: 0.952 - ETA: 1s - loss: 0.1334 - acc: 0.953 - ETA: 0s - loss: 0.1370 - acc: 0.952 - ETA: 0s - loss: 0.1386 - acc: 0.952 - ETA: 0s - loss: 0.1380 - acc: 0.952 - ETA: 0s - loss: 0.1411 - acc: 0.951 - ETA: 0s - loss: 0.1418 - acc: 0.951 - ETA: 0s - loss: 0.1415 - acc: 0.951 - ETA: 0s - loss: 0.1432 - acc: 0.951 - ETA: 0s - loss: 0.1416 - acc: 0.951 - ETA: 0s - loss: 0.1433 - acc: 0.951 - 5s 103ms/step - loss: 0.1438 - acc: 0.9516\n",
      "Epoch 60/10000\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0005120000569149852.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2441 - acc: 0.920 - ETA: 4s - loss: 0.2074 - acc: 0.920 - ETA: 4s - loss: 0.1992 - acc: 0.920 - ETA: 4s - loss: 0.1848 - acc: 0.926 - ETA: 4s - loss: 0.1829 - acc: 0.925 - ETA: 4s - loss: 0.1731 - acc: 0.929 - ETA: 4s - loss: 0.1840 - acc: 0.926 - ETA: 4s - loss: 0.1700 - acc: 0.936 - ETA: 4s - loss: 0.1686 - acc: 0.934 - ETA: 4s - loss: 0.1660 - acc: 0.934 - ETA: 3s - loss: 0.1575 - acc: 0.939 - ETA: 3s - loss: 0.1525 - acc: 0.938 - ETA: 3s - loss: 0.1514 - acc: 0.938 - ETA: 3s - loss: 0.1550 - acc: 0.938 - ETA: 3s - loss: 0.1572 - acc: 0.939 - ETA: 3s - loss: 0.1570 - acc: 0.939 - ETA: 3s - loss: 0.1527 - acc: 0.941 - ETA: 3s - loss: 0.1576 - acc: 0.940 - ETA: 3s - loss: 0.1643 - acc: 0.936 - ETA: 3s - loss: 0.1649 - acc: 0.937 - ETA: 2s - loss: 0.1617 - acc: 0.939 - ETA: 2s - loss: 0.1609 - acc: 0.940 - ETA: 2s - loss: 0.1587 - acc: 0.941 - ETA: 2s - loss: 0.1551 - acc: 0.942 - ETA: 2s - loss: 0.1567 - acc: 0.942 - ETA: 2s - loss: 0.1554 - acc: 0.943 - ETA: 2s - loss: 0.1544 - acc: 0.944 - ETA: 2s - loss: 0.1524 - acc: 0.944 - ETA: 2s - loss: 0.1520 - acc: 0.944 - ETA: 1s - loss: 0.1539 - acc: 0.943 - ETA: 1s - loss: 0.1564 - acc: 0.942 - ETA: 1s - loss: 0.1591 - acc: 0.942 - ETA: 1s - loss: 0.1572 - acc: 0.943 - ETA: 1s - loss: 0.1555 - acc: 0.944 - ETA: 1s - loss: 0.1551 - acc: 0.944 - ETA: 1s - loss: 0.1570 - acc: 0.943 - ETA: 1s - loss: 0.1594 - acc: 0.942 - ETA: 1s - loss: 0.1581 - acc: 0.942 - ETA: 1s - loss: 0.1572 - acc: 0.943 - ETA: 0s - loss: 0.1600 - acc: 0.941 - ETA: 0s - loss: 0.1602 - acc: 0.941 - ETA: 0s - loss: 0.1598 - acc: 0.941 - ETA: 0s - loss: 0.1608 - acc: 0.941 - ETA: 0s - loss: 0.1605 - acc: 0.941 - ETA: 0s - loss: 0.1594 - acc: 0.942 - ETA: 0s - loss: 0.1587 - acc: 0.942 - ETA: 0s - loss: 0.1578 - acc: 0.942 - ETA: 0s - loss: 0.1567 - acc: 0.942 - 5s 104ms/step - loss: 0.1560 - acc: 0.9425\n",
      "Epoch 61/10000\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00040960004553198815.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1311 - acc: 0.954 - ETA: 4s - loss: 0.1808 - acc: 0.937 - ETA: 4s - loss: 0.1772 - acc: 0.935 - ETA: 4s - loss: 0.1598 - acc: 0.946 - ETA: 4s - loss: 0.1513 - acc: 0.947 - ETA: 4s - loss: 0.1493 - acc: 0.947 - ETA: 4s - loss: 0.1463 - acc: 0.948 - ETA: 4s - loss: 0.1563 - acc: 0.943 - ETA: 4s - loss: 0.1461 - acc: 0.947 - ETA: 4s - loss: 0.1423 - acc: 0.948 - ETA: 3s - loss: 0.1377 - acc: 0.951 - ETA: 3s - loss: 0.1434 - acc: 0.949 - ETA: 3s - loss: 0.1395 - acc: 0.951 - ETA: 3s - loss: 0.1349 - acc: 0.953 - ETA: 3s - loss: 0.1400 - acc: 0.950 - ETA: 3s - loss: 0.1394 - acc: 0.951 - ETA: 3s - loss: 0.1407 - acc: 0.949 - ETA: 3s - loss: 0.1404 - acc: 0.948 - ETA: 3s - loss: 0.1448 - acc: 0.948 - ETA: 3s - loss: 0.1487 - acc: 0.944 - ETA: 2s - loss: 0.1516 - acc: 0.944 - ETA: 2s - loss: 0.1489 - acc: 0.944 - ETA: 2s - loss: 0.1511 - acc: 0.944 - ETA: 2s - loss: 0.1491 - acc: 0.945 - ETA: 2s - loss: 0.1471 - acc: 0.945 - ETA: 2s - loss: 0.1462 - acc: 0.946 - ETA: 2s - loss: 0.1434 - acc: 0.948 - ETA: 2s - loss: 0.1415 - acc: 0.948 - ETA: 2s - loss: 0.1427 - acc: 0.947 - ETA: 1s - loss: 0.1422 - acc: 0.947 - ETA: 1s - loss: 0.1420 - acc: 0.947 - ETA: 1s - loss: 0.1393 - acc: 0.948 - ETA: 1s - loss: 0.1372 - acc: 0.950 - ETA: 1s - loss: 0.1365 - acc: 0.950 - ETA: 1s - loss: 0.1395 - acc: 0.949 - ETA: 1s - loss: 0.1380 - acc: 0.950 - ETA: 1s - loss: 0.1368 - acc: 0.950 - ETA: 1s - loss: 0.1359 - acc: 0.950 - ETA: 1s - loss: 0.1352 - acc: 0.950 - ETA: 0s - loss: 0.1369 - acc: 0.950 - ETA: 0s - loss: 0.1362 - acc: 0.950 - ETA: 0s - loss: 0.1365 - acc: 0.950 - ETA: 0s - loss: 0.1366 - acc: 0.949 - ETA: 0s - loss: 0.1384 - acc: 0.949 - ETA: 0s - loss: 0.1404 - acc: 0.948 - ETA: 0s - loss: 0.1423 - acc: 0.948 - ETA: 0s - loss: 0.1416 - acc: 0.948 - ETA: 0s - loss: 0.1423 - acc: 0.948 - 5s 104ms/step - loss: 0.1430 - acc: 0.9479\n",
      "Epoch 62/10000\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1326 - acc: 0.943 - ETA: 4s - loss: 0.1751 - acc: 0.926 - ETA: 4s - loss: 0.2012 - acc: 0.916 - ETA: 4s - loss: 0.1718 - acc: 0.931 - ETA: 4s - loss: 0.1647 - acc: 0.931 - ETA: 4s - loss: 0.1578 - acc: 0.939 - ETA: 4s - loss: 0.1682 - acc: 0.933 - ETA: 4s - loss: 0.1604 - acc: 0.937 - ETA: 4s - loss: 0.1643 - acc: 0.938 - ETA: 4s - loss: 0.1640 - acc: 0.938 - ETA: 3s - loss: 0.1694 - acc: 0.937 - ETA: 3s - loss: 0.1744 - acc: 0.934 - ETA: 3s - loss: 0.1704 - acc: 0.935 - ETA: 3s - loss: 0.1707 - acc: 0.936 - ETA: 3s - loss: 0.1644 - acc: 0.939 - ETA: 3s - loss: 0.1611 - acc: 0.941 - ETA: 3s - loss: 0.1619 - acc: 0.941 - ETA: 3s - loss: 0.1569 - acc: 0.943 - ETA: 3s - loss: 0.1561 - acc: 0.943 - ETA: 3s - loss: 0.1572 - acc: 0.943 - ETA: 2s - loss: 0.1536 - acc: 0.944 - ETA: 2s - loss: 0.1514 - acc: 0.945 - ETA: 2s - loss: 0.1503 - acc: 0.945 - ETA: 2s - loss: 0.1509 - acc: 0.945 - ETA: 2s - loss: 0.1509 - acc: 0.946 - ETA: 2s - loss: 0.1482 - acc: 0.947 - ETA: 2s - loss: 0.1475 - acc: 0.947 - ETA: 2s - loss: 0.1467 - acc: 0.948 - ETA: 2s - loss: 0.1482 - acc: 0.947 - ETA: 1s - loss: 0.1494 - acc: 0.946 - ETA: 1s - loss: 0.1481 - acc: 0.947 - ETA: 1s - loss: 0.1459 - acc: 0.949 - ETA: 1s - loss: 0.1499 - acc: 0.948 - ETA: 1s - loss: 0.1484 - acc: 0.949 - ETA: 1s - loss: 0.1484 - acc: 0.948 - ETA: 1s - loss: 0.1487 - acc: 0.948 - ETA: 1s - loss: 0.1510 - acc: 0.948 - ETA: 1s - loss: 0.1501 - acc: 0.948 - ETA: 1s - loss: 0.1490 - acc: 0.948 - ETA: 0s - loss: 0.1523 - acc: 0.947 - ETA: 0s - loss: 0.1517 - acc: 0.947 - ETA: 0s - loss: 0.1515 - acc: 0.946 - ETA: 0s - loss: 0.1502 - acc: 0.947 - ETA: 0s - loss: 0.1511 - acc: 0.946 - ETA: 0s - loss: 0.1504 - acc: 0.947 - ETA: 0s - loss: 0.1531 - acc: 0.946 - ETA: 0s - loss: 0.1523 - acc: 0.946 - ETA: 0s - loss: 0.1513 - acc: 0.947 - 5s 104ms/step - loss: 0.1524 - acc: 0.9473\n",
      "Epoch 63/10000\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1565 - acc: 0.920 - ETA: 4s - loss: 0.1229 - acc: 0.948 - ETA: 4s - loss: 0.1656 - acc: 0.928 - ETA: 4s - loss: 0.1464 - acc: 0.940 - ETA: 4s - loss: 0.1536 - acc: 0.936 - ETA: 4s - loss: 0.1421 - acc: 0.943 - ETA: 4s - loss: 0.1459 - acc: 0.944 - ETA: 4s - loss: 0.1501 - acc: 0.943 - ETA: 4s - loss: 0.1536 - acc: 0.941 - ETA: 4s - loss: 0.1568 - acc: 0.939 - ETA: 3s - loss: 0.1576 - acc: 0.940 - ETA: 3s - loss: 0.1529 - acc: 0.941 - ETA: 3s - loss: 0.1490 - acc: 0.942 - ETA: 3s - loss: 0.1455 - acc: 0.944 - ETA: 3s - loss: 0.1450 - acc: 0.945 - ETA: 3s - loss: 0.1442 - acc: 0.945 - ETA: 3s - loss: 0.1497 - acc: 0.944 - ETA: 3s - loss: 0.1569 - acc: 0.943 - ETA: 3s - loss: 0.1534 - acc: 0.945 - ETA: 3s - loss: 0.1515 - acc: 0.946 - ETA: 2s - loss: 0.1515 - acc: 0.945 - ETA: 2s - loss: 0.1503 - acc: 0.946 - ETA: 2s - loss: 0.1522 - acc: 0.945 - ETA: 2s - loss: 0.1517 - acc: 0.946 - ETA: 2s - loss: 0.1518 - acc: 0.946 - ETA: 2s - loss: 0.1502 - acc: 0.946 - ETA: 2s - loss: 0.1486 - acc: 0.947 - ETA: 2s - loss: 0.1472 - acc: 0.948 - ETA: 2s - loss: 0.1500 - acc: 0.947 - ETA: 1s - loss: 0.1473 - acc: 0.948 - ETA: 1s - loss: 0.1483 - acc: 0.947 - ETA: 1s - loss: 0.1518 - acc: 0.946 - ETA: 1s - loss: 0.1530 - acc: 0.945 - ETA: 1s - loss: 0.1519 - acc: 0.946 - ETA: 1s - loss: 0.1527 - acc: 0.945 - ETA: 1s - loss: 0.1542 - acc: 0.944 - ETA: 1s - loss: 0.1538 - acc: 0.945 - ETA: 1s - loss: 0.1533 - acc: 0.945 - ETA: 1s - loss: 0.1522 - acc: 0.946 - ETA: 0s - loss: 0.1510 - acc: 0.946 - ETA: 0s - loss: 0.1498 - acc: 0.947 - ETA: 0s - loss: 0.1513 - acc: 0.947 - ETA: 0s - loss: 0.1513 - acc: 0.947 - ETA: 0s - loss: 0.1497 - acc: 0.948 - ETA: 0s - loss: 0.1497 - acc: 0.948 - ETA: 0s - loss: 0.1483 - acc: 0.948 - ETA: 0s - loss: 0.1478 - acc: 0.948 - ETA: 0s - loss: 0.1458 - acc: 0.949 - 5s 104ms/step - loss: 0.1467 - acc: 0.9499\n",
      "Epoch 64/10000\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2206 - acc: 0.943 - ETA: 4s - loss: 0.1989 - acc: 0.931 - ETA: 4s - loss: 0.1871 - acc: 0.939 - ETA: 4s - loss: 0.1867 - acc: 0.937 - ETA: 4s - loss: 0.1639 - acc: 0.947 - ETA: 4s - loss: 0.1682 - acc: 0.943 - ETA: 4s - loss: 0.1642 - acc: 0.939 - ETA: 4s - loss: 0.1669 - acc: 0.936 - ETA: 4s - loss: 0.1688 - acc: 0.936 - ETA: 3s - loss: 0.1671 - acc: 0.938 - ETA: 3s - loss: 0.1644 - acc: 0.938 - ETA: 3s - loss: 0.1632 - acc: 0.938 - ETA: 3s - loss: 0.1600 - acc: 0.941 - ETA: 3s - loss: 0.1577 - acc: 0.942 - ETA: 3s - loss: 0.1520 - acc: 0.945 - ETA: 3s - loss: 0.1514 - acc: 0.945 - ETA: 3s - loss: 0.1472 - acc: 0.946 - ETA: 3s - loss: 0.1436 - acc: 0.947 - ETA: 3s - loss: 0.1400 - acc: 0.950 - ETA: 3s - loss: 0.1393 - acc: 0.950 - ETA: 2s - loss: 0.1432 - acc: 0.948 - ETA: 2s - loss: 0.1444 - acc: 0.947 - ETA: 2s - loss: 0.1428 - acc: 0.949 - ETA: 2s - loss: 0.1454 - acc: 0.948 - ETA: 2s - loss: 0.1441 - acc: 0.949 - ETA: 2s - loss: 0.1441 - acc: 0.949 - ETA: 2s - loss: 0.1422 - acc: 0.950 - ETA: 2s - loss: 0.1458 - acc: 0.948 - ETA: 2s - loss: 0.1443 - acc: 0.949 - ETA: 1s - loss: 0.1426 - acc: 0.949 - ETA: 1s - loss: 0.1416 - acc: 0.950 - ETA: 1s - loss: 0.1433 - acc: 0.948 - ETA: 1s - loss: 0.1424 - acc: 0.949 - ETA: 1s - loss: 0.1423 - acc: 0.949 - ETA: 1s - loss: 0.1402 - acc: 0.951 - ETA: 1s - loss: 0.1395 - acc: 0.950 - ETA: 1s - loss: 0.1411 - acc: 0.950 - ETA: 1s - loss: 0.1448 - acc: 0.948 - ETA: 1s - loss: 0.1451 - acc: 0.949 - ETA: 0s - loss: 0.1443 - acc: 0.948 - ETA: 0s - loss: 0.1447 - acc: 0.949 - ETA: 0s - loss: 0.1458 - acc: 0.948 - ETA: 0s - loss: 0.1432 - acc: 0.949 - ETA: 0s - loss: 0.1450 - acc: 0.949 - ETA: 0s - loss: 0.1443 - acc: 0.949 - ETA: 0s - loss: 0.1433 - acc: 0.949 - ETA: 0s - loss: 0.1436 - acc: 0.949 - ETA: 0s - loss: 0.1425 - acc: 0.949 - 5s 104ms/step - loss: 0.1437 - acc: 0.9490\n",
      "Epoch 65/10000\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1501 - acc: 0.965 - ETA: 4s - loss: 0.1361 - acc: 0.960 - ETA: 4s - loss: 0.1240 - acc: 0.965 - ETA: 4s - loss: 0.1486 - acc: 0.954 - ETA: 4s - loss: 0.1407 - acc: 0.954 - ETA: 4s - loss: 0.1286 - acc: 0.960 - ETA: 4s - loss: 0.1212 - acc: 0.965 - ETA: 4s - loss: 0.1174 - acc: 0.965 - ETA: 4s - loss: 0.1438 - acc: 0.957 - ETA: 4s - loss: 0.1558 - acc: 0.950 - ETA: 3s - loss: 0.1627 - acc: 0.946 - ETA: 3s - loss: 0.1634 - acc: 0.945 - ETA: 3s - loss: 0.1652 - acc: 0.944 - ETA: 3s - loss: 0.1674 - acc: 0.943 - ETA: 3s - loss: 0.1716 - acc: 0.942 - ETA: 3s - loss: 0.1668 - acc: 0.944 - ETA: 3s - loss: 0.1650 - acc: 0.943 - ETA: 3s - loss: 0.1610 - acc: 0.945 - ETA: 3s - loss: 0.1582 - acc: 0.945 - ETA: 3s - loss: 0.1569 - acc: 0.944 - ETA: 2s - loss: 0.1559 - acc: 0.945 - ETA: 2s - loss: 0.1515 - acc: 0.946 - ETA: 2s - loss: 0.1497 - acc: 0.946 - ETA: 2s - loss: 0.1518 - acc: 0.945 - ETA: 2s - loss: 0.1494 - acc: 0.945 - ETA: 2s - loss: 0.1476 - acc: 0.947 - ETA: 2s - loss: 0.1473 - acc: 0.946 - ETA: 2s - loss: 0.1491 - acc: 0.945 - ETA: 2s - loss: 0.1461 - acc: 0.947 - ETA: 1s - loss: 0.1499 - acc: 0.947 - ETA: 1s - loss: 0.1479 - acc: 0.947 - ETA: 1s - loss: 0.1471 - acc: 0.948 - ETA: 1s - loss: 0.1462 - acc: 0.948 - ETA: 1s - loss: 0.1455 - acc: 0.949 - ETA: 1s - loss: 0.1443 - acc: 0.949 - ETA: 1s - loss: 0.1450 - acc: 0.949 - ETA: 1s - loss: 0.1435 - acc: 0.949 - ETA: 1s - loss: 0.1454 - acc: 0.949 - ETA: 1s - loss: 0.1479 - acc: 0.948 - ETA: 0s - loss: 0.1471 - acc: 0.947 - ETA: 0s - loss: 0.1497 - acc: 0.947 - ETA: 0s - loss: 0.1500 - acc: 0.946 - ETA: 0s - loss: 0.1524 - acc: 0.946 - ETA: 0s - loss: 0.1512 - acc: 0.946 - ETA: 0s - loss: 0.1510 - acc: 0.946 - ETA: 0s - loss: 0.1500 - acc: 0.946 - ETA: 0s - loss: 0.1519 - acc: 0.945 - ETA: 0s - loss: 0.1535 - acc: 0.944 - 5s 104ms/step - loss: 0.1528 - acc: 0.9447\n",
      "Epoch 66/10000\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1516 - acc: 0.943 - ETA: 4s - loss: 0.2042 - acc: 0.920 - ETA: 4s - loss: 0.2254 - acc: 0.912 - ETA: 4s - loss: 0.2038 - acc: 0.923 - ETA: 4s - loss: 0.1826 - acc: 0.931 - ETA: 4s - loss: 0.1816 - acc: 0.929 - ETA: 4s - loss: 0.1789 - acc: 0.931 - ETA: 4s - loss: 0.1712 - acc: 0.931 - ETA: 4s - loss: 0.1573 - acc: 0.939 - ETA: 4s - loss: 0.1503 - acc: 0.942 - ETA: 3s - loss: 0.1457 - acc: 0.943 - ETA: 3s - loss: 0.1444 - acc: 0.944 - ETA: 3s - loss: 0.1483 - acc: 0.945 - ETA: 3s - loss: 0.1446 - acc: 0.947 - ETA: 3s - loss: 0.1491 - acc: 0.946 - ETA: 3s - loss: 0.1459 - acc: 0.948 - ETA: 3s - loss: 0.1458 - acc: 0.947 - ETA: 3s - loss: 0.1436 - acc: 0.948 - ETA: 3s - loss: 0.1480 - acc: 0.946 - ETA: 3s - loss: 0.1479 - acc: 0.946 - ETA: 2s - loss: 0.1482 - acc: 0.945 - ETA: 2s - loss: 0.1462 - acc: 0.946 - ETA: 2s - loss: 0.1468 - acc: 0.945 - ETA: 2s - loss: 0.1462 - acc: 0.946 - ETA: 2s - loss: 0.1471 - acc: 0.945 - ETA: 2s - loss: 0.1473 - acc: 0.946 - ETA: 2s - loss: 0.1483 - acc: 0.946 - ETA: 2s - loss: 0.1465 - acc: 0.946 - ETA: 2s - loss: 0.1458 - acc: 0.946 - ETA: 2s - loss: 0.1435 - acc: 0.948 - ETA: 1s - loss: 0.1413 - acc: 0.949 - ETA: 1s - loss: 0.1417 - acc: 0.948 - ETA: 1s - loss: 0.1437 - acc: 0.948 - ETA: 1s - loss: 0.1426 - acc: 0.947 - ETA: 1s - loss: 0.1432 - acc: 0.947 - ETA: 1s - loss: 0.1411 - acc: 0.947 - ETA: 1s - loss: 0.1403 - acc: 0.948 - ETA: 1s - loss: 0.1407 - acc: 0.947 - ETA: 1s - loss: 0.1389 - acc: 0.948 - ETA: 0s - loss: 0.1409 - acc: 0.947 - ETA: 0s - loss: 0.1397 - acc: 0.947 - ETA: 0s - loss: 0.1404 - acc: 0.947 - ETA: 0s - loss: 0.1394 - acc: 0.947 - ETA: 0s - loss: 0.1425 - acc: 0.946 - ETA: 0s - loss: 0.1416 - acc: 0.946 - ETA: 0s - loss: 0.1409 - acc: 0.946 - ETA: 0s - loss: 0.1407 - acc: 0.946 - ETA: 0s - loss: 0.1398 - acc: 0.947 - 5s 106ms/step - loss: 0.1416 - acc: 0.9462\n",
      "Epoch 67/10000\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0928 - acc: 0.943 - ETA: 5s - loss: 0.1325 - acc: 0.954 - ETA: 4s - loss: 0.1570 - acc: 0.947 - ETA: 4s - loss: 0.1474 - acc: 0.948 - ETA: 4s - loss: 0.1605 - acc: 0.943 - ETA: 4s - loss: 0.1505 - acc: 0.947 - ETA: 4s - loss: 0.1505 - acc: 0.944 - ETA: 4s - loss: 0.1633 - acc: 0.941 - ETA: 4s - loss: 0.1761 - acc: 0.938 - ETA: 4s - loss: 0.1690 - acc: 0.938 - ETA: 4s - loss: 0.1609 - acc: 0.942 - ETA: 3s - loss: 0.1652 - acc: 0.938 - ETA: 3s - loss: 0.1573 - acc: 0.942 - ETA: 3s - loss: 0.1592 - acc: 0.941 - ETA: 3s - loss: 0.1550 - acc: 0.942 - ETA: 3s - loss: 0.1556 - acc: 0.941 - ETA: 3s - loss: 0.1493 - acc: 0.945 - ETA: 3s - loss: 0.1508 - acc: 0.946 - ETA: 3s - loss: 0.1490 - acc: 0.946 - ETA: 3s - loss: 0.1473 - acc: 0.947 - ETA: 2s - loss: 0.1425 - acc: 0.949 - ETA: 2s - loss: 0.1426 - acc: 0.949 - ETA: 2s - loss: 0.1414 - acc: 0.950 - ETA: 2s - loss: 0.1390 - acc: 0.951 - ETA: 2s - loss: 0.1399 - acc: 0.951 - ETA: 2s - loss: 0.1390 - acc: 0.951 - ETA: 2s - loss: 0.1421 - acc: 0.951 - ETA: 2s - loss: 0.1465 - acc: 0.950 - ETA: 2s - loss: 0.1477 - acc: 0.949 - ETA: 2s - loss: 0.1518 - acc: 0.948 - ETA: 1s - loss: 0.1498 - acc: 0.948 - ETA: 1s - loss: 0.1482 - acc: 0.949 - ETA: 1s - loss: 0.1455 - acc: 0.950 - ETA: 1s - loss: 0.1471 - acc: 0.949 - ETA: 1s - loss: 0.1465 - acc: 0.949 - ETA: 1s - loss: 0.1441 - acc: 0.950 - ETA: 1s - loss: 0.1427 - acc: 0.951 - ETA: 1s - loss: 0.1458 - acc: 0.949 - ETA: 1s - loss: 0.1459 - acc: 0.949 - ETA: 0s - loss: 0.1441 - acc: 0.950 - ETA: 0s - loss: 0.1449 - acc: 0.950 - ETA: 0s - loss: 0.1455 - acc: 0.949 - ETA: 0s - loss: 0.1441 - acc: 0.950 - ETA: 0s - loss: 0.1450 - acc: 0.949 - ETA: 0s - loss: 0.1475 - acc: 0.948 - ETA: 0s - loss: 0.1485 - acc: 0.947 - ETA: 0s - loss: 0.1490 - acc: 0.947 - ETA: 0s - loss: 0.1500 - acc: 0.946 - 5s 106ms/step - loss: 0.1500 - acc: 0.9467\n",
      "Epoch 68/10000\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2176 - acc: 0.920 - ETA: 5s - loss: 0.1789 - acc: 0.926 - ETA: 4s - loss: 0.1640 - acc: 0.931 - ETA: 4s - loss: 0.1625 - acc: 0.934 - ETA: 4s - loss: 0.1604 - acc: 0.934 - ETA: 4s - loss: 0.1740 - acc: 0.926 - ETA: 4s - loss: 0.1684 - acc: 0.931 - ETA: 4s - loss: 0.1600 - acc: 0.936 - ETA: 4s - loss: 0.1594 - acc: 0.936 - ETA: 4s - loss: 0.1510 - acc: 0.942 - ETA: 3s - loss: 0.1518 - acc: 0.940 - ETA: 3s - loss: 0.1490 - acc: 0.943 - ETA: 3s - loss: 0.1444 - acc: 0.945 - ETA: 3s - loss: 0.1431 - acc: 0.947 - ETA: 3s - loss: 0.1431 - acc: 0.947 - ETA: 3s - loss: 0.1386 - acc: 0.950 - ETA: 3s - loss: 0.1395 - acc: 0.949 - ETA: 3s - loss: 0.1425 - acc: 0.947 - ETA: 3s - loss: 0.1478 - acc: 0.944 - ETA: 3s - loss: 0.1476 - acc: 0.944 - ETA: 2s - loss: 0.1462 - acc: 0.944 - ETA: 2s - loss: 0.1448 - acc: 0.945 - ETA: 2s - loss: 0.1422 - acc: 0.947 - ETA: 2s - loss: 0.1417 - acc: 0.945 - ETA: 2s - loss: 0.1407 - acc: 0.946 - ETA: 2s - loss: 0.1368 - acc: 0.948 - ETA: 2s - loss: 0.1404 - acc: 0.946 - ETA: 2s - loss: 0.1416 - acc: 0.946 - ETA: 2s - loss: 0.1449 - acc: 0.944 - ETA: 1s - loss: 0.1427 - acc: 0.944 - ETA: 1s - loss: 0.1428 - acc: 0.944 - ETA: 1s - loss: 0.1421 - acc: 0.944 - ETA: 1s - loss: 0.1415 - acc: 0.944 - ETA: 1s - loss: 0.1438 - acc: 0.943 - ETA: 1s - loss: 0.1425 - acc: 0.943 - ETA: 1s - loss: 0.1424 - acc: 0.943 - ETA: 1s - loss: 0.1416 - acc: 0.944 - ETA: 1s - loss: 0.1424 - acc: 0.943 - ETA: 1s - loss: 0.1416 - acc: 0.944 - ETA: 0s - loss: 0.1419 - acc: 0.944 - ETA: 0s - loss: 0.1404 - acc: 0.945 - ETA: 0s - loss: 0.1399 - acc: 0.945 - ETA: 0s - loss: 0.1411 - acc: 0.944 - ETA: 0s - loss: 0.1393 - acc: 0.945 - ETA: 0s - loss: 0.1397 - acc: 0.945 - ETA: 0s - loss: 0.1389 - acc: 0.945 - ETA: 0s - loss: 0.1381 - acc: 0.946 - ETA: 0s - loss: 0.1379 - acc: 0.947 - 5s 106ms/step - loss: 0.1371 - acc: 0.9474\n",
      "Epoch 69/10000\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2742 - acc: 0.909 - ETA: 4s - loss: 0.2504 - acc: 0.914 - ETA: 4s - loss: 0.2138 - acc: 0.928 - ETA: 4s - loss: 0.2228 - acc: 0.931 - ETA: 4s - loss: 0.2148 - acc: 0.929 - ETA: 4s - loss: 0.1987 - acc: 0.935 - ETA: 4s - loss: 0.1844 - acc: 0.939 - ETA: 4s - loss: 0.1727 - acc: 0.940 - ETA: 4s - loss: 0.1707 - acc: 0.941 - ETA: 4s - loss: 0.1609 - acc: 0.945 - ETA: 3s - loss: 0.1618 - acc: 0.948 - ETA: 3s - loss: 0.1582 - acc: 0.949 - ETA: 3s - loss: 0.1629 - acc: 0.948 - ETA: 3s - loss: 0.1582 - acc: 0.948 - ETA: 3s - loss: 0.1571 - acc: 0.949 - ETA: 3s - loss: 0.1545 - acc: 0.948 - ETA: 3s - loss: 0.1559 - acc: 0.946 - ETA: 3s - loss: 0.1620 - acc: 0.944 - ETA: 3s - loss: 0.1595 - acc: 0.946 - ETA: 3s - loss: 0.1580 - acc: 0.946 - ETA: 2s - loss: 0.1542 - acc: 0.947 - ETA: 2s - loss: 0.1573 - acc: 0.946 - ETA: 2s - loss: 0.1601 - acc: 0.946 - ETA: 2s - loss: 0.1605 - acc: 0.946 - ETA: 2s - loss: 0.1582 - acc: 0.946 - ETA: 2s - loss: 0.1551 - acc: 0.948 - ETA: 2s - loss: 0.1517 - acc: 0.950 - ETA: 2s - loss: 0.1498 - acc: 0.950 - ETA: 2s - loss: 0.1483 - acc: 0.951 - ETA: 1s - loss: 0.1480 - acc: 0.951 - ETA: 1s - loss: 0.1463 - acc: 0.952 - ETA: 1s - loss: 0.1493 - acc: 0.951 - ETA: 1s - loss: 0.1465 - acc: 0.952 - ETA: 1s - loss: 0.1496 - acc: 0.951 - ETA: 1s - loss: 0.1478 - acc: 0.951 - ETA: 1s - loss: 0.1503 - acc: 0.950 - ETA: 1s - loss: 0.1528 - acc: 0.949 - ETA: 1s - loss: 0.1555 - acc: 0.948 - ETA: 1s - loss: 0.1537 - acc: 0.949 - ETA: 0s - loss: 0.1550 - acc: 0.948 - ETA: 0s - loss: 0.1549 - acc: 0.948 - ETA: 0s - loss: 0.1535 - acc: 0.948 - ETA: 0s - loss: 0.1528 - acc: 0.948 - ETA: 0s - loss: 0.1544 - acc: 0.948 - ETA: 0s - loss: 0.1552 - acc: 0.947 - ETA: 0s - loss: 0.1560 - acc: 0.947 - ETA: 0s - loss: 0.1568 - acc: 0.945 - ETA: 0s - loss: 0.1548 - acc: 0.947 - 5s 104ms/step - loss: 0.1561 - acc: 0.9469\n",
      "Epoch 70/10000\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1096 - acc: 0.954 - ETA: 4s - loss: 0.1212 - acc: 0.957 - ETA: 4s - loss: 0.1424 - acc: 0.956 - ETA: 4s - loss: 0.1640 - acc: 0.950 - ETA: 4s - loss: 0.1697 - acc: 0.944 - ETA: 4s - loss: 0.1725 - acc: 0.936 - ETA: 4s - loss: 0.1598 - acc: 0.942 - ETA: 4s - loss: 0.1561 - acc: 0.944 - ETA: 4s - loss: 0.1486 - acc: 0.945 - ETA: 4s - loss: 0.1432 - acc: 0.948 - ETA: 3s - loss: 0.1405 - acc: 0.948 - ETA: 3s - loss: 0.1366 - acc: 0.949 - ETA: 3s - loss: 0.1379 - acc: 0.949 - ETA: 3s - loss: 0.1375 - acc: 0.950 - ETA: 3s - loss: 0.1412 - acc: 0.947 - ETA: 3s - loss: 0.1384 - acc: 0.947 - ETA: 3s - loss: 0.1379 - acc: 0.946 - ETA: 3s - loss: 0.1423 - acc: 0.946 - ETA: 3s - loss: 0.1380 - acc: 0.948 - ETA: 2s - loss: 0.1368 - acc: 0.949 - ETA: 2s - loss: 0.1412 - acc: 0.947 - ETA: 2s - loss: 0.1397 - acc: 0.947 - ETA: 2s - loss: 0.1381 - acc: 0.948 - ETA: 2s - loss: 0.1356 - acc: 0.949 - ETA: 2s - loss: 0.1362 - acc: 0.948 - ETA: 2s - loss: 0.1360 - acc: 0.949 - ETA: 2s - loss: 0.1347 - acc: 0.949 - ETA: 2s - loss: 0.1388 - acc: 0.947 - ETA: 2s - loss: 0.1382 - acc: 0.948 - ETA: 1s - loss: 0.1361 - acc: 0.949 - ETA: 1s - loss: 0.1367 - acc: 0.948 - ETA: 1s - loss: 0.1376 - acc: 0.948 - ETA: 1s - loss: 0.1416 - acc: 0.946 - ETA: 1s - loss: 0.1399 - acc: 0.946 - ETA: 1s - loss: 0.1411 - acc: 0.946 - ETA: 1s - loss: 0.1418 - acc: 0.946 - ETA: 1s - loss: 0.1413 - acc: 0.947 - ETA: 1s - loss: 0.1407 - acc: 0.947 - ETA: 1s - loss: 0.1385 - acc: 0.948 - ETA: 0s - loss: 0.1376 - acc: 0.948 - ETA: 0s - loss: 0.1382 - acc: 0.948 - ETA: 0s - loss: 0.1388 - acc: 0.948 - ETA: 0s - loss: 0.1382 - acc: 0.949 - ETA: 0s - loss: 0.1381 - acc: 0.949 - ETA: 0s - loss: 0.1380 - acc: 0.949 - ETA: 0s - loss: 0.1365 - acc: 0.949 - ETA: 0s - loss: 0.1407 - acc: 0.947 - ETA: 0s - loss: 0.1404 - acc: 0.948 - 5s 103ms/step - loss: 0.1416 - acc: 0.9469\n",
      "Epoch 71/10000\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1891 - acc: 0.920 - ETA: 4s - loss: 0.1378 - acc: 0.943 - ETA: 4s - loss: 0.1278 - acc: 0.943 - ETA: 4s - loss: 0.1384 - acc: 0.937 - ETA: 4s - loss: 0.1503 - acc: 0.934 - ETA: 4s - loss: 0.1617 - acc: 0.931 - ETA: 4s - loss: 0.1601 - acc: 0.935 - ETA: 4s - loss: 0.1658 - acc: 0.931 - ETA: 4s - loss: 0.1614 - acc: 0.934 - ETA: 4s - loss: 0.1584 - acc: 0.937 - ETA: 3s - loss: 0.1508 - acc: 0.940 - ETA: 3s - loss: 0.1515 - acc: 0.940 - ETA: 3s - loss: 0.1484 - acc: 0.942 - ETA: 3s - loss: 0.1456 - acc: 0.943 - ETA: 3s - loss: 0.1461 - acc: 0.942 - ETA: 3s - loss: 0.1463 - acc: 0.942 - ETA: 3s - loss: 0.1489 - acc: 0.939 - ETA: 3s - loss: 0.1553 - acc: 0.938 - ETA: 3s - loss: 0.1535 - acc: 0.939 - ETA: 3s - loss: 0.1552 - acc: 0.940 - ETA: 2s - loss: 0.1518 - acc: 0.942 - ETA: 2s - loss: 0.1608 - acc: 0.940 - ETA: 2s - loss: 0.1574 - acc: 0.941 - ETA: 2s - loss: 0.1563 - acc: 0.941 - ETA: 2s - loss: 0.1554 - acc: 0.943 - ETA: 2s - loss: 0.1531 - acc: 0.944 - ETA: 2s - loss: 0.1540 - acc: 0.944 - ETA: 2s - loss: 0.1546 - acc: 0.944 - ETA: 2s - loss: 0.1539 - acc: 0.945 - ETA: 2s - loss: 0.1524 - acc: 0.946 - ETA: 1s - loss: 0.1522 - acc: 0.946 - ETA: 1s - loss: 0.1513 - acc: 0.946 - ETA: 1s - loss: 0.1496 - acc: 0.947 - ETA: 1s - loss: 0.1511 - acc: 0.946 - ETA: 1s - loss: 0.1498 - acc: 0.947 - ETA: 1s - loss: 0.1487 - acc: 0.947 - ETA: 1s - loss: 0.1478 - acc: 0.947 - ETA: 1s - loss: 0.1490 - acc: 0.946 - ETA: 1s - loss: 0.1471 - acc: 0.946 - ETA: 0s - loss: 0.1476 - acc: 0.946 - ETA: 0s - loss: 0.1468 - acc: 0.947 - ETA: 0s - loss: 0.1471 - acc: 0.947 - ETA: 0s - loss: 0.1453 - acc: 0.948 - ETA: 0s - loss: 0.1460 - acc: 0.948 - ETA: 0s - loss: 0.1457 - acc: 0.948 - ETA: 0s - loss: 0.1473 - acc: 0.947 - ETA: 0s - loss: 0.1467 - acc: 0.947 - ETA: 0s - loss: 0.1492 - acc: 0.947 - 5s 107ms/step - loss: 0.1478 - acc: 0.9478\n",
      "Epoch 72/10000\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2186 - acc: 0.909 - ETA: 5s - loss: 0.1376 - acc: 0.948 - ETA: 5s - loss: 0.1575 - acc: 0.939 - ETA: 4s - loss: 0.1635 - acc: 0.943 - ETA: 4s - loss: 0.1432 - acc: 0.950 - ETA: 4s - loss: 0.1494 - acc: 0.948 - ETA: 4s - loss: 0.1493 - acc: 0.949 - ETA: 4s - loss: 0.1452 - acc: 0.950 - ETA: 4s - loss: 0.1396 - acc: 0.952 - ETA: 4s - loss: 0.1447 - acc: 0.950 - ETA: 4s - loss: 0.1511 - acc: 0.945 - ETA: 4s - loss: 0.1537 - acc: 0.941 - ETA: 3s - loss: 0.1577 - acc: 0.942 - ETA: 3s - loss: 0.1567 - acc: 0.940 - ETA: 3s - loss: 0.1559 - acc: 0.941 - ETA: 3s - loss: 0.1605 - acc: 0.941 - ETA: 3s - loss: 0.1593 - acc: 0.943 - ETA: 3s - loss: 0.1564 - acc: 0.945 - ETA: 3s - loss: 0.1534 - acc: 0.947 - ETA: 3s - loss: 0.1514 - acc: 0.948 - ETA: 3s - loss: 0.1481 - acc: 0.949 - ETA: 3s - loss: 0.1514 - acc: 0.948 - ETA: 2s - loss: 0.1496 - acc: 0.947 - ETA: 2s - loss: 0.1459 - acc: 0.949 - ETA: 2s - loss: 0.1435 - acc: 0.950 - ETA: 2s - loss: 0.1452 - acc: 0.951 - ETA: 2s - loss: 0.1423 - acc: 0.952 - ETA: 2s - loss: 0.1430 - acc: 0.952 - ETA: 2s - loss: 0.1436 - acc: 0.951 - ETA: 2s - loss: 0.1424 - acc: 0.952 - ETA: 2s - loss: 0.1440 - acc: 0.951 - ETA: 1s - loss: 0.1443 - acc: 0.950 - ETA: 1s - loss: 0.1433 - acc: 0.950 - ETA: 1s - loss: 0.1450 - acc: 0.950 - ETA: 1s - loss: 0.1453 - acc: 0.949 - ETA: 1s - loss: 0.1424 - acc: 0.950 - ETA: 1s - loss: 0.1435 - acc: 0.949 - ETA: 1s - loss: 0.1425 - acc: 0.950 - ETA: 1s - loss: 0.1435 - acc: 0.949 - ETA: 1s - loss: 0.1431 - acc: 0.950 - ETA: 0s - loss: 0.1433 - acc: 0.950 - ETA: 0s - loss: 0.1431 - acc: 0.951 - ETA: 0s - loss: 0.1435 - acc: 0.951 - ETA: 0s - loss: 0.1431 - acc: 0.951 - ETA: 0s - loss: 0.1445 - acc: 0.950 - ETA: 0s - loss: 0.1428 - acc: 0.951 - ETA: 0s - loss: 0.1430 - acc: 0.950 - ETA: 0s - loss: 0.1438 - acc: 0.950 - 6s 123ms/step - loss: 0.1439 - acc: 0.9508\n",
      "Epoch 73/10000\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 7s - loss: 0.1298 - acc: 0.948 - ETA: 7s - loss: 0.1990 - acc: 0.934 - ETA: 6s - loss: 0.2053 - acc: 0.925 - ETA: 7s - loss: 0.1805 - acc: 0.933 - ETA: 8s - loss: 0.1760 - acc: 0.941 - ETA: 7s - loss: 0.1757 - acc: 0.938 - ETA: 7s - loss: 0.1841 - acc: 0.934 - ETA: 6s - loss: 0.1763 - acc: 0.938 - ETA: 6s - loss: 0.1778 - acc: 0.933 - ETA: 6s - loss: 0.1792 - acc: 0.931 - ETA: 6s - loss: 0.1768 - acc: 0.932 - ETA: 5s - loss: 0.1751 - acc: 0.932 - ETA: 5s - loss: 0.1709 - acc: 0.932 - ETA: 5s - loss: 0.1704 - acc: 0.933 - ETA: 5s - loss: 0.1645 - acc: 0.936 - ETA: 5s - loss: 0.1666 - acc: 0.937 - ETA: 5s - loss: 0.1673 - acc: 0.936 - ETA: 4s - loss: 0.1607 - acc: 0.938 - ETA: 4s - loss: 0.1585 - acc: 0.939 - ETA: 4s - loss: 0.1575 - acc: 0.940 - ETA: 4s - loss: 0.1525 - acc: 0.942 - ETA: 4s - loss: 0.1488 - acc: 0.944 - ETA: 4s - loss: 0.1464 - acc: 0.945 - ETA: 4s - loss: 0.1446 - acc: 0.946 - ETA: 4s - loss: 0.1438 - acc: 0.947 - ETA: 4s - loss: 0.1440 - acc: 0.947 - ETA: 4s - loss: 0.1436 - acc: 0.948 - ETA: 4s - loss: 0.1423 - acc: 0.949 - ETA: 3s - loss: 0.1495 - acc: 0.947 - ETA: 3s - loss: 0.1512 - acc: 0.947 - ETA: 3s - loss: 0.1521 - acc: 0.947 - ETA: 3s - loss: 0.1529 - acc: 0.946 - ETA: 3s - loss: 0.1516 - acc: 0.947 - ETA: 2s - loss: 0.1522 - acc: 0.947 - ETA: 2s - loss: 0.1528 - acc: 0.947 - ETA: 2s - loss: 0.1522 - acc: 0.947 - ETA: 2s - loss: 0.1491 - acc: 0.948 - ETA: 2s - loss: 0.1480 - acc: 0.948 - ETA: 2s - loss: 0.1497 - acc: 0.948 - ETA: 1s - loss: 0.1480 - acc: 0.948 - ETA: 1s - loss: 0.1474 - acc: 0.948 - ETA: 1s - loss: 0.1450 - acc: 0.950 - ETA: 1s - loss: 0.1445 - acc: 0.949 - ETA: 1s - loss: 0.1445 - acc: 0.949 - ETA: 0s - loss: 0.1423 - acc: 0.950 - ETA: 0s - loss: 0.1415 - acc: 0.950 - ETA: 0s - loss: 0.1423 - acc: 0.951 - ETA: 0s - loss: 0.1409 - acc: 0.951 - 10s 214ms/step - loss: 0.1405 - acc: 0.9514\n",
      "Epoch 74/10000\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 6s - loss: 0.2135 - acc: 0.954 - ETA: 7s - loss: 0.2421 - acc: 0.920 - ETA: 9s - loss: 0.2098 - acc: 0.920 - ETA: 8s - loss: 0.1965 - acc: 0.923 - ETA: 7s - loss: 0.1775 - acc: 0.931 - ETA: 7s - loss: 0.1664 - acc: 0.935 - ETA: 7s - loss: 0.1723 - acc: 0.935 - ETA: 6s - loss: 0.1813 - acc: 0.933 - ETA: 7s - loss: 0.1711 - acc: 0.935 - ETA: 6s - loss: 0.1643 - acc: 0.938 - ETA: 6s - loss: 0.1639 - acc: 0.937 - ETA: 5s - loss: 0.1602 - acc: 0.940 - ETA: 5s - loss: 0.1590 - acc: 0.939 - ETA: 5s - loss: 0.1557 - acc: 0.939 - ETA: 5s - loss: 0.1481 - acc: 0.943 - ETA: 5s - loss: 0.1556 - acc: 0.941 - ETA: 5s - loss: 0.1562 - acc: 0.941 - ETA: 4s - loss: 0.1556 - acc: 0.942 - ETA: 4s - loss: 0.1552 - acc: 0.943 - ETA: 4s - loss: 0.1536 - acc: 0.942 - ETA: 4s - loss: 0.1516 - acc: 0.944 - ETA: 3s - loss: 0.1525 - acc: 0.943 - ETA: 3s - loss: 0.1503 - acc: 0.944 - ETA: 3s - loss: 0.1497 - acc: 0.944 - ETA: 3s - loss: 0.1469 - acc: 0.946 - ETA: 3s - loss: 0.1451 - acc: 0.946 - ETA: 3s - loss: 0.1479 - acc: 0.943 - ETA: 2s - loss: 0.1489 - acc: 0.942 - ETA: 2s - loss: 0.1466 - acc: 0.943 - ETA: 2s - loss: 0.1440 - acc: 0.945 - ETA: 2s - loss: 0.1420 - acc: 0.946 - ETA: 2s - loss: 0.1454 - acc: 0.945 - ETA: 2s - loss: 0.1445 - acc: 0.945 - ETA: 2s - loss: 0.1450 - acc: 0.946 - ETA: 1s - loss: 0.1451 - acc: 0.945 - ETA: 1s - loss: 0.1452 - acc: 0.946 - ETA: 1s - loss: 0.1447 - acc: 0.946 - ETA: 1s - loss: 0.1433 - acc: 0.947 - ETA: 1s - loss: 0.1452 - acc: 0.946 - ETA: 1s - loss: 0.1490 - acc: 0.945 - ETA: 1s - loss: 0.1511 - acc: 0.944 - ETA: 0s - loss: 0.1548 - acc: 0.942 - ETA: 0s - loss: 0.1531 - acc: 0.943 - ETA: 0s - loss: 0.1526 - acc: 0.943 - ETA: 0s - loss: 0.1525 - acc: 0.943 - ETA: 0s - loss: 0.1510 - acc: 0.943 - ETA: 0s - loss: 0.1516 - acc: 0.943 - ETA: 0s - loss: 0.1500 - acc: 0.943 - 7s 138ms/step - loss: 0.1480 - acc: 0.9448\n",
      "Epoch 75/10000\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1125 - acc: 0.965 - ETA: 4s - loss: 0.1290 - acc: 0.954 - ETA: 4s - loss: 0.1286 - acc: 0.954 - ETA: 4s - loss: 0.1364 - acc: 0.954 - ETA: 4s - loss: 0.1324 - acc: 0.950 - ETA: 4s - loss: 0.1326 - acc: 0.950 - ETA: 4s - loss: 0.1234 - acc: 0.954 - ETA: 4s - loss: 0.1294 - acc: 0.954 - ETA: 4s - loss: 0.1423 - acc: 0.949 - ETA: 4s - loss: 0.1546 - acc: 0.946 - ETA: 4s - loss: 0.1535 - acc: 0.945 - ETA: 3s - loss: 0.1530 - acc: 0.946 - ETA: 3s - loss: 0.1562 - acc: 0.942 - ETA: 3s - loss: 0.1520 - acc: 0.944 - ETA: 3s - loss: 0.1488 - acc: 0.946 - ETA: 3s - loss: 0.1529 - acc: 0.945 - ETA: 3s - loss: 0.1545 - acc: 0.945 - ETA: 3s - loss: 0.1559 - acc: 0.943 - ETA: 3s - loss: 0.1601 - acc: 0.942 - ETA: 3s - loss: 0.1575 - acc: 0.944 - ETA: 2s - loss: 0.1549 - acc: 0.945 - ETA: 2s - loss: 0.1566 - acc: 0.945 - ETA: 2s - loss: 0.1530 - acc: 0.946 - ETA: 2s - loss: 0.1500 - acc: 0.947 - ETA: 2s - loss: 0.1502 - acc: 0.948 - ETA: 2s - loss: 0.1476 - acc: 0.949 - ETA: 2s - loss: 0.1480 - acc: 0.949 - ETA: 2s - loss: 0.1477 - acc: 0.949 - ETA: 2s - loss: 0.1471 - acc: 0.949 - ETA: 2s - loss: 0.1461 - acc: 0.950 - ETA: 1s - loss: 0.1468 - acc: 0.949 - ETA: 1s - loss: 0.1469 - acc: 0.950 - ETA: 1s - loss: 0.1464 - acc: 0.950 - ETA: 1s - loss: 0.1458 - acc: 0.949 - ETA: 1s - loss: 0.1444 - acc: 0.949 - ETA: 1s - loss: 0.1464 - acc: 0.949 - ETA: 1s - loss: 0.1467 - acc: 0.948 - ETA: 1s - loss: 0.1483 - acc: 0.947 - ETA: 1s - loss: 0.1479 - acc: 0.947 - ETA: 0s - loss: 0.1476 - acc: 0.947 - ETA: 0s - loss: 0.1471 - acc: 0.946 - ETA: 0s - loss: 0.1457 - acc: 0.947 - ETA: 0s - loss: 0.1464 - acc: 0.947 - ETA: 0s - loss: 0.1470 - acc: 0.947 - ETA: 0s - loss: 0.1482 - acc: 0.946 - ETA: 0s - loss: 0.1482 - acc: 0.946 - ETA: 0s - loss: 0.1491 - acc: 0.945 - ETA: 0s - loss: 0.1473 - acc: 0.946 - 5s 107ms/step - loss: 0.1472 - acc: 0.9470\n",
      "Epoch 76/10000\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1159 - acc: 0.977 - ETA: 4s - loss: 0.0874 - acc: 0.983 - ETA: 4s - loss: 0.0868 - acc: 0.981 - ETA: 4s - loss: 0.1052 - acc: 0.974 - ETA: 4s - loss: 0.1145 - acc: 0.970 - ETA: 4s - loss: 0.1097 - acc: 0.973 - ETA: 4s - loss: 0.1120 - acc: 0.972 - ETA: 4s - loss: 0.1081 - acc: 0.971 - ETA: 4s - loss: 0.1202 - acc: 0.967 - ETA: 4s - loss: 0.1298 - acc: 0.961 - ETA: 4s - loss: 0.1311 - acc: 0.957 - ETA: 3s - loss: 0.1307 - acc: 0.959 - ETA: 3s - loss: 0.1317 - acc: 0.958 - ETA: 3s - loss: 0.1292 - acc: 0.959 - ETA: 3s - loss: 0.1294 - acc: 0.958 - ETA: 3s - loss: 0.1269 - acc: 0.959 - ETA: 3s - loss: 0.1240 - acc: 0.959 - ETA: 3s - loss: 0.1256 - acc: 0.959 - ETA: 3s - loss: 0.1224 - acc: 0.959 - ETA: 3s - loss: 0.1215 - acc: 0.959 - ETA: 2s - loss: 0.1199 - acc: 0.960 - ETA: 2s - loss: 0.1196 - acc: 0.960 - ETA: 2s - loss: 0.1193 - acc: 0.960 - ETA: 2s - loss: 0.1255 - acc: 0.956 - ETA: 2s - loss: 0.1303 - acc: 0.954 - ETA: 2s - loss: 0.1308 - acc: 0.954 - ETA: 2s - loss: 0.1288 - acc: 0.954 - ETA: 2s - loss: 0.1329 - acc: 0.952 - ETA: 2s - loss: 0.1338 - acc: 0.951 - ETA: 2s - loss: 0.1328 - acc: 0.951 - ETA: 1s - loss: 0.1322 - acc: 0.951 - ETA: 1s - loss: 0.1343 - acc: 0.950 - ETA: 1s - loss: 0.1318 - acc: 0.951 - ETA: 1s - loss: 0.1321 - acc: 0.950 - ETA: 1s - loss: 0.1309 - acc: 0.950 - ETA: 1s - loss: 0.1334 - acc: 0.950 - ETA: 1s - loss: 0.1324 - acc: 0.950 - ETA: 1s - loss: 0.1319 - acc: 0.951 - ETA: 1s - loss: 0.1309 - acc: 0.950 - ETA: 0s - loss: 0.1305 - acc: 0.950 - ETA: 0s - loss: 0.1292 - acc: 0.951 - ETA: 0s - loss: 0.1287 - acc: 0.951 - ETA: 0s - loss: 0.1279 - acc: 0.951 - ETA: 0s - loss: 0.1289 - acc: 0.951 - ETA: 0s - loss: 0.1289 - acc: 0.951 - ETA: 0s - loss: 0.1277 - acc: 0.951 - ETA: 0s - loss: 0.1283 - acc: 0.951 - ETA: 0s - loss: 0.1263 - acc: 0.952 - 5s 111ms/step - loss: 0.1276 - acc: 0.9518\n",
      "Epoch 77/10000\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.0752 - acc: 0.977 - ETA: 4s - loss: 0.1047 - acc: 0.965 - ETA: 4s - loss: 0.0891 - acc: 0.973 - ETA: 4s - loss: 0.1056 - acc: 0.963 - ETA: 4s - loss: 0.1081 - acc: 0.961 - ETA: 4s - loss: 0.1029 - acc: 0.965 - ETA: 4s - loss: 0.1105 - acc: 0.962 - ETA: 4s - loss: 0.1036 - acc: 0.964 - ETA: 4s - loss: 0.1064 - acc: 0.963 - ETA: 4s - loss: 0.1067 - acc: 0.962 - ETA: 4s - loss: 0.1033 - acc: 0.964 - ETA: 3s - loss: 0.1045 - acc: 0.961 - ETA: 3s - loss: 0.1062 - acc: 0.960 - ETA: 3s - loss: 0.1223 - acc: 0.955 - ETA: 3s - loss: 0.1233 - acc: 0.955 - ETA: 3s - loss: 0.1314 - acc: 0.954 - ETA: 3s - loss: 0.1324 - acc: 0.953 - ETA: 3s - loss: 0.1372 - acc: 0.950 - ETA: 3s - loss: 0.1428 - acc: 0.947 - ETA: 3s - loss: 0.1469 - acc: 0.945 - ETA: 3s - loss: 0.1483 - acc: 0.944 - ETA: 3s - loss: 0.1507 - acc: 0.943 - ETA: 2s - loss: 0.1514 - acc: 0.941 - ETA: 2s - loss: 0.1549 - acc: 0.939 - ETA: 2s - loss: 0.1526 - acc: 0.941 - ETA: 2s - loss: 0.1533 - acc: 0.941 - ETA: 2s - loss: 0.1517 - acc: 0.942 - ETA: 2s - loss: 0.1497 - acc: 0.943 - ETA: 2s - loss: 0.1508 - acc: 0.942 - ETA: 2s - loss: 0.1525 - acc: 0.941 - ETA: 2s - loss: 0.1506 - acc: 0.942 - ETA: 2s - loss: 0.1504 - acc: 0.942 - ETA: 1s - loss: 0.1564 - acc: 0.941 - ETA: 1s - loss: 0.1554 - acc: 0.941 - ETA: 1s - loss: 0.1548 - acc: 0.942 - ETA: 1s - loss: 0.1522 - acc: 0.943 - ETA: 1s - loss: 0.1502 - acc: 0.944 - ETA: 1s - loss: 0.1485 - acc: 0.945 - ETA: 1s - loss: 0.1489 - acc: 0.945 - ETA: 1s - loss: 0.1475 - acc: 0.946 - ETA: 1s - loss: 0.1485 - acc: 0.945 - ETA: 1s - loss: 0.1505 - acc: 0.945 - ETA: 0s - loss: 0.1496 - acc: 0.945 - ETA: 0s - loss: 0.1504 - acc: 0.945 - ETA: 0s - loss: 0.1495 - acc: 0.945 - ETA: 0s - loss: 0.1485 - acc: 0.945 - ETA: 0s - loss: 0.1476 - acc: 0.946 - ETA: 0s - loss: 0.1500 - acc: 0.945 - 7s 147ms/step - loss: 0.1509 - acc: 0.9452\n",
      "Epoch 78/10000\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 7s - loss: 0.3840 - acc: 0.875 - ETA: 7s - loss: 0.2218 - acc: 0.937 - ETA: 6s - loss: 0.1913 - acc: 0.943 - ETA: 6s - loss: 0.1760 - acc: 0.943 - ETA: 6s - loss: 0.1613 - acc: 0.947 - ETA: 7s - loss: 0.1431 - acc: 0.956 - ETA: 8s - loss: 0.1365 - acc: 0.959 - ETA: 8s - loss: 0.1525 - acc: 0.954 - ETA: 7s - loss: 0.1462 - acc: 0.957 - ETA: 7s - loss: 0.1399 - acc: 0.958 - ETA: 7s - loss: 0.1362 - acc: 0.958 - ETA: 7s - loss: 0.1335 - acc: 0.958 - ETA: 6s - loss: 0.1350 - acc: 0.955 - ETA: 6s - loss: 0.1376 - acc: 0.955 - ETA: 6s - loss: 0.1354 - acc: 0.956 - ETA: 5s - loss: 0.1378 - acc: 0.955 - ETA: 5s - loss: 0.1351 - acc: 0.954 - ETA: 6s - loss: 0.1376 - acc: 0.953 - ETA: 5s - loss: 0.1373 - acc: 0.953 - ETA: 5s - loss: 0.1386 - acc: 0.953 - ETA: 5s - loss: 0.1369 - acc: 0.954 - ETA: 4s - loss: 0.1380 - acc: 0.954 - ETA: 4s - loss: 0.1365 - acc: 0.955 - ETA: 4s - loss: 0.1367 - acc: 0.955 - ETA: 4s - loss: 0.1378 - acc: 0.954 - ETA: 3s - loss: 0.1363 - acc: 0.954 - ETA: 3s - loss: 0.1367 - acc: 0.953 - ETA: 3s - loss: 0.1356 - acc: 0.954 - ETA: 3s - loss: 0.1393 - acc: 0.953 - ETA: 3s - loss: 0.1405 - acc: 0.952 - ETA: 2s - loss: 0.1390 - acc: 0.953 - ETA: 2s - loss: 0.1401 - acc: 0.953 - ETA: 2s - loss: 0.1413 - acc: 0.951 - ETA: 2s - loss: 0.1400 - acc: 0.951 - ETA: 2s - loss: 0.1395 - acc: 0.951 - ETA: 2s - loss: 0.1414 - acc: 0.950 - ETA: 1s - loss: 0.1447 - acc: 0.948 - ETA: 1s - loss: 0.1434 - acc: 0.948 - ETA: 1s - loss: 0.1420 - acc: 0.949 - ETA: 1s - loss: 0.1401 - acc: 0.950 - ETA: 1s - loss: 0.1404 - acc: 0.950 - ETA: 1s - loss: 0.1401 - acc: 0.950 - ETA: 0s - loss: 0.1390 - acc: 0.951 - ETA: 0s - loss: 0.1395 - acc: 0.950 - ETA: 0s - loss: 0.1390 - acc: 0.950 - ETA: 0s - loss: 0.1393 - acc: 0.950 - ETA: 0s - loss: 0.1401 - acc: 0.949 - ETA: 0s - loss: 0.1410 - acc: 0.949 - 7s 141ms/step - loss: 0.1416 - acc: 0.9482\n",
      "Epoch 79/10000\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1277 - acc: 0.954 - ETA: 4s - loss: 0.1434 - acc: 0.960 - ETA: 4s - loss: 0.1294 - acc: 0.965 - ETA: 4s - loss: 0.1341 - acc: 0.963 - ETA: 4s - loss: 0.1285 - acc: 0.961 - ETA: 4s - loss: 0.1371 - acc: 0.958 - ETA: 4s - loss: 0.1300 - acc: 0.959 - ETA: 4s - loss: 0.1213 - acc: 0.963 - ETA: 4s - loss: 0.1172 - acc: 0.964 - ETA: 4s - loss: 0.1123 - acc: 0.967 - ETA: 3s - loss: 0.1156 - acc: 0.966 - ETA: 3s - loss: 0.1149 - acc: 0.965 - ETA: 3s - loss: 0.1134 - acc: 0.965 - ETA: 3s - loss: 0.1136 - acc: 0.963 - ETA: 3s - loss: 0.1176 - acc: 0.962 - ETA: 3s - loss: 0.1259 - acc: 0.959 - ETA: 3s - loss: 0.1252 - acc: 0.960 - ETA: 3s - loss: 0.1267 - acc: 0.959 - ETA: 3s - loss: 0.1306 - acc: 0.959 - ETA: 2s - loss: 0.1315 - acc: 0.957 - ETA: 2s - loss: 0.1309 - acc: 0.958 - ETA: 2s - loss: 0.1296 - acc: 0.959 - ETA: 2s - loss: 0.1337 - acc: 0.958 - ETA: 2s - loss: 0.1353 - acc: 0.955 - ETA: 2s - loss: 0.1330 - acc: 0.955 - ETA: 2s - loss: 0.1364 - acc: 0.955 - ETA: 2s - loss: 0.1383 - acc: 0.954 - ETA: 2s - loss: 0.1359 - acc: 0.955 - ETA: 2s - loss: 0.1359 - acc: 0.955 - ETA: 1s - loss: 0.1352 - acc: 0.954 - ETA: 1s - loss: 0.1364 - acc: 0.954 - ETA: 1s - loss: 0.1371 - acc: 0.954 - ETA: 1s - loss: 0.1387 - acc: 0.953 - ETA: 1s - loss: 0.1415 - acc: 0.952 - ETA: 1s - loss: 0.1397 - acc: 0.953 - ETA: 1s - loss: 0.1385 - acc: 0.953 - ETA: 1s - loss: 0.1383 - acc: 0.953 - ETA: 1s - loss: 0.1376 - acc: 0.954 - ETA: 1s - loss: 0.1380 - acc: 0.953 - ETA: 0s - loss: 0.1385 - acc: 0.953 - ETA: 0s - loss: 0.1377 - acc: 0.953 - ETA: 0s - loss: 0.1384 - acc: 0.952 - ETA: 0s - loss: 0.1361 - acc: 0.953 - ETA: 0s - loss: 0.1359 - acc: 0.953 - ETA: 0s - loss: 0.1359 - acc: 0.952 - ETA: 0s - loss: 0.1359 - acc: 0.952 - ETA: 0s - loss: 0.1340 - acc: 0.953 - ETA: 0s - loss: 0.1333 - acc: 0.954 - 5s 104ms/step - loss: 0.1333 - acc: 0.9540\n",
      "Epoch 80/10000\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.00040960003389045596.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0609 - acc: 0.977 - ETA: 4s - loss: 0.1415 - acc: 0.954 - ETA: 4s - loss: 0.1095 - acc: 0.969 - ETA: 4s - loss: 0.1280 - acc: 0.954 - ETA: 4s - loss: 0.1278 - acc: 0.956 - ETA: 4s - loss: 0.1232 - acc: 0.954 - ETA: 4s - loss: 0.1261 - acc: 0.952 - ETA: 4s - loss: 0.1292 - acc: 0.954 - ETA: 4s - loss: 0.1442 - acc: 0.950 - ETA: 4s - loss: 0.1434 - acc: 0.950 - ETA: 3s - loss: 0.1400 - acc: 0.949 - ETA: 3s - loss: 0.1347 - acc: 0.951 - ETA: 3s - loss: 0.1272 - acc: 0.954 - ETA: 3s - loss: 0.1232 - acc: 0.955 - ETA: 3s - loss: 0.1218 - acc: 0.955 - ETA: 3s - loss: 0.1341 - acc: 0.952 - ETA: 3s - loss: 0.1326 - acc: 0.953 - ETA: 3s - loss: 0.1324 - acc: 0.954 - ETA: 3s - loss: 0.1353 - acc: 0.954 - ETA: 2s - loss: 0.1371 - acc: 0.953 - ETA: 2s - loss: 0.1471 - acc: 0.952 - ETA: 2s - loss: 0.1461 - acc: 0.951 - ETA: 2s - loss: 0.1459 - acc: 0.951 - ETA: 2s - loss: 0.1457 - acc: 0.951 - ETA: 2s - loss: 0.1440 - acc: 0.951 - ETA: 2s - loss: 0.1447 - acc: 0.951 - ETA: 2s - loss: 0.1445 - acc: 0.950 - ETA: 2s - loss: 0.1454 - acc: 0.948 - ETA: 2s - loss: 0.1455 - acc: 0.948 - ETA: 1s - loss: 0.1478 - acc: 0.947 - ETA: 1s - loss: 0.1495 - acc: 0.946 - ETA: 1s - loss: 0.1477 - acc: 0.947 - ETA: 1s - loss: 0.1533 - acc: 0.945 - ETA: 1s - loss: 0.1523 - acc: 0.945 - ETA: 1s - loss: 0.1507 - acc: 0.946 - ETA: 1s - loss: 0.1492 - acc: 0.946 - ETA: 1s - loss: 0.1524 - acc: 0.944 - ETA: 1s - loss: 0.1508 - acc: 0.945 - ETA: 1s - loss: 0.1485 - acc: 0.946 - ETA: 0s - loss: 0.1475 - acc: 0.947 - ETA: 0s - loss: 0.1479 - acc: 0.946 - ETA: 0s - loss: 0.1467 - acc: 0.947 - ETA: 0s - loss: 0.1481 - acc: 0.946 - ETA: 0s - loss: 0.1476 - acc: 0.946 - ETA: 0s - loss: 0.1493 - acc: 0.945 - ETA: 0s - loss: 0.1488 - acc: 0.945 - ETA: 0s - loss: 0.1474 - acc: 0.946 - ETA: 0s - loss: 0.1482 - acc: 0.946 - 5s 103ms/step - loss: 0.1479 - acc: 0.9459\n",
      "Epoch 81/10000\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1512 - acc: 0.943 - ETA: 4s - loss: 0.1650 - acc: 0.937 - ETA: 4s - loss: 0.1582 - acc: 0.939 - ETA: 4s - loss: 0.1515 - acc: 0.943 - ETA: 4s - loss: 0.1643 - acc: 0.938 - ETA: 4s - loss: 0.1523 - acc: 0.945 - ETA: 4s - loss: 0.1530 - acc: 0.941 - ETA: 4s - loss: 0.1502 - acc: 0.940 - ETA: 4s - loss: 0.1429 - acc: 0.944 - ETA: 4s - loss: 0.1359 - acc: 0.948 - ETA: 3s - loss: 0.1283 - acc: 0.952 - ETA: 3s - loss: 0.1285 - acc: 0.950 - ETA: 3s - loss: 0.1306 - acc: 0.949 - ETA: 3s - loss: 0.1303 - acc: 0.948 - ETA: 3s - loss: 0.1305 - acc: 0.949 - ETA: 3s - loss: 0.1284 - acc: 0.951 - ETA: 3s - loss: 0.1284 - acc: 0.951 - ETA: 3s - loss: 0.1299 - acc: 0.950 - ETA: 3s - loss: 0.1310 - acc: 0.949 - ETA: 2s - loss: 0.1364 - acc: 0.947 - ETA: 2s - loss: 0.1366 - acc: 0.948 - ETA: 2s - loss: 0.1392 - acc: 0.946 - ETA: 2s - loss: 0.1410 - acc: 0.946 - ETA: 2s - loss: 0.1382 - acc: 0.947 - ETA: 2s - loss: 0.1371 - acc: 0.948 - ETA: 2s - loss: 0.1353 - acc: 0.948 - ETA: 2s - loss: 0.1330 - acc: 0.949 - ETA: 2s - loss: 0.1348 - acc: 0.950 - ETA: 2s - loss: 0.1350 - acc: 0.949 - ETA: 1s - loss: 0.1357 - acc: 0.949 - ETA: 1s - loss: 0.1353 - acc: 0.950 - ETA: 1s - loss: 0.1337 - acc: 0.950 - ETA: 1s - loss: 0.1316 - acc: 0.951 - ETA: 1s - loss: 0.1294 - acc: 0.952 - ETA: 1s - loss: 0.1326 - acc: 0.951 - ETA: 1s - loss: 0.1340 - acc: 0.949 - ETA: 1s - loss: 0.1322 - acc: 0.950 - ETA: 1s - loss: 0.1331 - acc: 0.950 - ETA: 1s - loss: 0.1335 - acc: 0.950 - ETA: 0s - loss: 0.1354 - acc: 0.949 - ETA: 0s - loss: 0.1336 - acc: 0.950 - ETA: 0s - loss: 0.1363 - acc: 0.950 - ETA: 0s - loss: 0.1344 - acc: 0.951 - ETA: 0s - loss: 0.1369 - acc: 0.949 - ETA: 0s - loss: 0.1380 - acc: 0.948 - ETA: 0s - loss: 0.1390 - acc: 0.947 - ETA: 0s - loss: 0.1373 - acc: 0.948 - ETA: 0s - loss: 0.1367 - acc: 0.948 - 5s 102ms/step - loss: 0.1381 - acc: 0.9484\n",
      "Epoch 82/10000\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.2091 - acc: 0.920 - ETA: 4s - loss: 0.1458 - acc: 0.948 - ETA: 4s - loss: 0.1210 - acc: 0.962 - ETA: 4s - loss: 0.1166 - acc: 0.957 - ETA: 4s - loss: 0.1148 - acc: 0.956 - ETA: 4s - loss: 0.1332 - acc: 0.954 - ETA: 4s - loss: 0.1224 - acc: 0.957 - ETA: 4s - loss: 0.1252 - acc: 0.957 - ETA: 4s - loss: 0.1314 - acc: 0.958 - ETA: 4s - loss: 0.1349 - acc: 0.953 - ETA: 3s - loss: 0.1357 - acc: 0.953 - ETA: 3s - loss: 0.1431 - acc: 0.950 - ETA: 3s - loss: 0.1451 - acc: 0.948 - ETA: 3s - loss: 0.1389 - acc: 0.950 - ETA: 3s - loss: 0.1383 - acc: 0.950 - ETA: 3s - loss: 0.1361 - acc: 0.953 - ETA: 3s - loss: 0.1349 - acc: 0.953 - ETA: 3s - loss: 0.1352 - acc: 0.953 - ETA: 3s - loss: 0.1324 - acc: 0.955 - ETA: 2s - loss: 0.1331 - acc: 0.955 - ETA: 2s - loss: 0.1307 - acc: 0.956 - ETA: 2s - loss: 0.1325 - acc: 0.955 - ETA: 2s - loss: 0.1342 - acc: 0.954 - ETA: 2s - loss: 0.1379 - acc: 0.953 - ETA: 2s - loss: 0.1369 - acc: 0.953 - ETA: 2s - loss: 0.1366 - acc: 0.953 - ETA: 2s - loss: 0.1464 - acc: 0.950 - ETA: 2s - loss: 0.1432 - acc: 0.951 - ETA: 2s - loss: 0.1434 - acc: 0.951 - ETA: 1s - loss: 0.1431 - acc: 0.951 - ETA: 1s - loss: 0.1450 - acc: 0.949 - ETA: 1s - loss: 0.1453 - acc: 0.948 - ETA: 1s - loss: 0.1474 - acc: 0.947 - ETA: 1s - loss: 0.1466 - acc: 0.948 - ETA: 1s - loss: 0.1448 - acc: 0.949 - ETA: 1s - loss: 0.1463 - acc: 0.949 - ETA: 1s - loss: 0.1454 - acc: 0.949 - ETA: 1s - loss: 0.1450 - acc: 0.950 - ETA: 1s - loss: 0.1439 - acc: 0.950 - ETA: 0s - loss: 0.1417 - acc: 0.951 - ETA: 0s - loss: 0.1405 - acc: 0.952 - ETA: 0s - loss: 0.1403 - acc: 0.952 - ETA: 0s - loss: 0.1405 - acc: 0.952 - ETA: 0s - loss: 0.1407 - acc: 0.952 - ETA: 0s - loss: 0.1405 - acc: 0.953 - ETA: 0s - loss: 0.1398 - acc: 0.953 - ETA: 0s - loss: 0.1396 - acc: 0.953 - ETA: 0s - loss: 0.1404 - acc: 0.952 - 5s 103ms/step - loss: 0.1411 - acc: 0.9525\n",
      "Epoch 83/10000\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0854 - acc: 0.977 - ETA: 4s - loss: 0.1231 - acc: 0.954 - ETA: 4s - loss: 0.1035 - acc: 0.965 - ETA: 4s - loss: 0.0984 - acc: 0.968 - ETA: 4s - loss: 0.1115 - acc: 0.965 - ETA: 4s - loss: 0.1172 - acc: 0.964 - ETA: 4s - loss: 0.1105 - acc: 0.964 - ETA: 4s - loss: 0.1167 - acc: 0.960 - ETA: 4s - loss: 0.1270 - acc: 0.955 - ETA: 4s - loss: 0.1376 - acc: 0.954 - ETA: 3s - loss: 0.1518 - acc: 0.951 - ETA: 3s - loss: 0.1450 - acc: 0.954 - ETA: 3s - loss: 0.1359 - acc: 0.958 - ETA: 3s - loss: 0.1295 - acc: 0.961 - ETA: 3s - loss: 0.1276 - acc: 0.962 - ETA: 3s - loss: 0.1268 - acc: 0.962 - ETA: 3s - loss: 0.1261 - acc: 0.961 - ETA: 3s - loss: 0.1276 - acc: 0.961 - ETA: 3s - loss: 0.1318 - acc: 0.959 - ETA: 2s - loss: 0.1295 - acc: 0.961 - ETA: 2s - loss: 0.1318 - acc: 0.959 - ETA: 2s - loss: 0.1363 - acc: 0.957 - ETA: 2s - loss: 0.1389 - acc: 0.956 - ETA: 2s - loss: 0.1387 - acc: 0.955 - ETA: 2s - loss: 0.1412 - acc: 0.953 - ETA: 2s - loss: 0.1397 - acc: 0.954 - ETA: 2s - loss: 0.1423 - acc: 0.953 - ETA: 2s - loss: 0.1433 - acc: 0.951 - ETA: 2s - loss: 0.1426 - acc: 0.951 - ETA: 1s - loss: 0.1440 - acc: 0.951 - ETA: 1s - loss: 0.1455 - acc: 0.949 - ETA: 1s - loss: 0.1448 - acc: 0.949 - ETA: 1s - loss: 0.1440 - acc: 0.949 - ETA: 1s - loss: 0.1460 - acc: 0.948 - ETA: 1s - loss: 0.1447 - acc: 0.948 - ETA: 1s - loss: 0.1461 - acc: 0.948 - ETA: 1s - loss: 0.1452 - acc: 0.947 - ETA: 1s - loss: 0.1470 - acc: 0.946 - ETA: 1s - loss: 0.1471 - acc: 0.947 - ETA: 0s - loss: 0.1489 - acc: 0.947 - ETA: 0s - loss: 0.1506 - acc: 0.945 - ETA: 0s - loss: 0.1508 - acc: 0.945 - ETA: 0s - loss: 0.1488 - acc: 0.947 - ETA: 0s - loss: 0.1490 - acc: 0.946 - ETA: 0s - loss: 0.1480 - acc: 0.947 - ETA: 0s - loss: 0.1458 - acc: 0.948 - ETA: 0s - loss: 0.1447 - acc: 0.948 - ETA: 0s - loss: 0.1444 - acc: 0.949 - 5s 103ms/step - loss: 0.1434 - acc: 0.9495\n",
      "Epoch 84/10000\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1066 - acc: 0.954 - ETA: 4s - loss: 0.1269 - acc: 0.948 - ETA: 4s - loss: 0.1181 - acc: 0.947 - ETA: 4s - loss: 0.1068 - acc: 0.960 - ETA: 4s - loss: 0.1115 - acc: 0.959 - ETA: 4s - loss: 0.1138 - acc: 0.960 - ETA: 4s - loss: 0.1116 - acc: 0.964 - ETA: 4s - loss: 0.1056 - acc: 0.965 - ETA: 4s - loss: 0.1051 - acc: 0.967 - ETA: 4s - loss: 0.1159 - acc: 0.963 - ETA: 3s - loss: 0.1136 - acc: 0.964 - ETA: 3s - loss: 0.1141 - acc: 0.963 - ETA: 3s - loss: 0.1161 - acc: 0.963 - ETA: 3s - loss: 0.1140 - acc: 0.963 - ETA: 3s - loss: 0.1131 - acc: 0.962 - ETA: 3s - loss: 0.1209 - acc: 0.960 - ETA: 3s - loss: 0.1194 - acc: 0.960 - ETA: 3s - loss: 0.1210 - acc: 0.958 - ETA: 3s - loss: 0.1265 - acc: 0.958 - ETA: 2s - loss: 0.1289 - acc: 0.957 - ETA: 2s - loss: 0.1317 - acc: 0.956 - ETA: 2s - loss: 0.1332 - acc: 0.954 - ETA: 2s - loss: 0.1345 - acc: 0.953 - ETA: 2s - loss: 0.1336 - acc: 0.953 - ETA: 2s - loss: 0.1301 - acc: 0.954 - ETA: 2s - loss: 0.1289 - acc: 0.955 - ETA: 2s - loss: 0.1292 - acc: 0.955 - ETA: 2s - loss: 0.1338 - acc: 0.953 - ETA: 2s - loss: 0.1334 - acc: 0.953 - ETA: 1s - loss: 0.1354 - acc: 0.953 - ETA: 1s - loss: 0.1345 - acc: 0.953 - ETA: 1s - loss: 0.1346 - acc: 0.951 - ETA: 1s - loss: 0.1367 - acc: 0.950 - ETA: 1s - loss: 0.1374 - acc: 0.951 - ETA: 1s - loss: 0.1391 - acc: 0.950 - ETA: 1s - loss: 0.1399 - acc: 0.949 - ETA: 1s - loss: 0.1390 - acc: 0.949 - ETA: 1s - loss: 0.1373 - acc: 0.950 - ETA: 1s - loss: 0.1370 - acc: 0.950 - ETA: 0s - loss: 0.1376 - acc: 0.950 - ETA: 0s - loss: 0.1358 - acc: 0.951 - ETA: 0s - loss: 0.1362 - acc: 0.951 - ETA: 0s - loss: 0.1358 - acc: 0.950 - ETA: 0s - loss: 0.1365 - acc: 0.950 - ETA: 0s - loss: 0.1366 - acc: 0.950 - ETA: 0s - loss: 0.1374 - acc: 0.950 - ETA: 0s - loss: 0.1364 - acc: 0.950 - ETA: 0s - loss: 0.1349 - acc: 0.951 - 5s 102ms/step - loss: 0.1340 - acc: 0.9515\n",
      "Epoch 85/10000\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1615 - acc: 0.920 - ETA: 4s - loss: 0.1963 - acc: 0.914 - ETA: 4s - loss: 0.1744 - acc: 0.924 - ETA: 4s - loss: 0.1523 - acc: 0.937 - ETA: 4s - loss: 0.1516 - acc: 0.945 - ETA: 4s - loss: 0.1409 - acc: 0.948 - ETA: 4s - loss: 0.1539 - acc: 0.946 - ETA: 4s - loss: 0.1524 - acc: 0.946 - ETA: 4s - loss: 0.1524 - acc: 0.948 - ETA: 4s - loss: 0.1445 - acc: 0.951 - ETA: 4s - loss: 0.1395 - acc: 0.952 - ETA: 3s - loss: 0.1441 - acc: 0.950 - ETA: 3s - loss: 0.1451 - acc: 0.950 - ETA: 3s - loss: 0.1512 - acc: 0.947 - ETA: 3s - loss: 0.1516 - acc: 0.947 - ETA: 3s - loss: 0.1468 - acc: 0.951 - ETA: 3s - loss: 0.1516 - acc: 0.949 - ETA: 3s - loss: 0.1518 - acc: 0.948 - ETA: 3s - loss: 0.1482 - acc: 0.951 - ETA: 3s - loss: 0.1441 - acc: 0.952 - ETA: 2s - loss: 0.1443 - acc: 0.952 - ETA: 2s - loss: 0.1423 - acc: 0.954 - ETA: 2s - loss: 0.1468 - acc: 0.952 - ETA: 2s - loss: 0.1457 - acc: 0.952 - ETA: 2s - loss: 0.1466 - acc: 0.951 - ETA: 2s - loss: 0.1442 - acc: 0.951 - ETA: 2s - loss: 0.1439 - acc: 0.951 - ETA: 2s - loss: 0.1413 - acc: 0.952 - ETA: 2s - loss: 0.1427 - acc: 0.952 - ETA: 1s - loss: 0.1456 - acc: 0.951 - ETA: 1s - loss: 0.1461 - acc: 0.950 - ETA: 1s - loss: 0.1457 - acc: 0.950 - ETA: 1s - loss: 0.1434 - acc: 0.951 - ETA: 1s - loss: 0.1412 - acc: 0.952 - ETA: 1s - loss: 0.1402 - acc: 0.952 - ETA: 1s - loss: 0.1402 - acc: 0.952 - ETA: 1s - loss: 0.1408 - acc: 0.952 - ETA: 1s - loss: 0.1400 - acc: 0.952 - ETA: 1s - loss: 0.1406 - acc: 0.951 - ETA: 0s - loss: 0.1387 - acc: 0.952 - ETA: 0s - loss: 0.1381 - acc: 0.952 - ETA: 0s - loss: 0.1394 - acc: 0.952 - ETA: 0s - loss: 0.1410 - acc: 0.951 - ETA: 0s - loss: 0.1411 - acc: 0.951 - ETA: 0s - loss: 0.1406 - acc: 0.951 - ETA: 0s - loss: 0.1404 - acc: 0.951 - ETA: 0s - loss: 0.1410 - acc: 0.951 - ETA: 0s - loss: 0.1411 - acc: 0.951 - 5s 104ms/step - loss: 0.1414 - acc: 0.9509\n",
      "Epoch 86/10000\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1976 - acc: 0.943 - ETA: 4s - loss: 0.1652 - acc: 0.960 - ETA: 4s - loss: 0.1562 - acc: 0.962 - ETA: 4s - loss: 0.1386 - acc: 0.965 - ETA: 4s - loss: 0.1600 - acc: 0.961 - ETA: 4s - loss: 0.1596 - acc: 0.956 - ETA: 4s - loss: 0.1573 - acc: 0.954 - ETA: 4s - loss: 0.1575 - acc: 0.951 - ETA: 4s - loss: 0.1578 - acc: 0.950 - ETA: 4s - loss: 0.1571 - acc: 0.947 - ETA: 3s - loss: 0.1577 - acc: 0.946 - ETA: 3s - loss: 0.1548 - acc: 0.947 - ETA: 3s - loss: 0.1589 - acc: 0.944 - ETA: 3s - loss: 0.1534 - acc: 0.945 - ETA: 3s - loss: 0.1522 - acc: 0.947 - ETA: 3s - loss: 0.1501 - acc: 0.948 - ETA: 3s - loss: 0.1462 - acc: 0.949 - ETA: 3s - loss: 0.1430 - acc: 0.951 - ETA: 3s - loss: 0.1423 - acc: 0.951 - ETA: 2s - loss: 0.1400 - acc: 0.952 - ETA: 2s - loss: 0.1383 - acc: 0.952 - ETA: 2s - loss: 0.1388 - acc: 0.951 - ETA: 2s - loss: 0.1435 - acc: 0.949 - ETA: 2s - loss: 0.1422 - acc: 0.950 - ETA: 2s - loss: 0.1401 - acc: 0.950 - ETA: 2s - loss: 0.1395 - acc: 0.949 - ETA: 2s - loss: 0.1430 - acc: 0.948 - ETA: 2s - loss: 0.1403 - acc: 0.949 - ETA: 2s - loss: 0.1401 - acc: 0.949 - ETA: 1s - loss: 0.1384 - acc: 0.950 - ETA: 1s - loss: 0.1370 - acc: 0.951 - ETA: 1s - loss: 0.1362 - acc: 0.951 - ETA: 1s - loss: 0.1389 - acc: 0.950 - ETA: 1s - loss: 0.1387 - acc: 0.950 - ETA: 1s - loss: 0.1370 - acc: 0.951 - ETA: 1s - loss: 0.1343 - acc: 0.952 - ETA: 1s - loss: 0.1343 - acc: 0.952 - ETA: 1s - loss: 0.1356 - acc: 0.952 - ETA: 1s - loss: 0.1361 - acc: 0.952 - ETA: 0s - loss: 0.1362 - acc: 0.952 - ETA: 0s - loss: 0.1340 - acc: 0.953 - ETA: 0s - loss: 0.1365 - acc: 0.952 - ETA: 0s - loss: 0.1362 - acc: 0.951 - ETA: 0s - loss: 0.1370 - acc: 0.951 - ETA: 0s - loss: 0.1358 - acc: 0.952 - ETA: 0s - loss: 0.1346 - acc: 0.952 - ETA: 0s - loss: 0.1342 - acc: 0.952 - ETA: 0s - loss: 0.1325 - acc: 0.953 - 5s 103ms/step - loss: 0.1325 - acc: 0.9534\n",
      "Epoch 87/10000\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1741 - acc: 0.920 - ETA: 4s - loss: 0.2086 - acc: 0.931 - ETA: 4s - loss: 0.1724 - acc: 0.943 - ETA: 4s - loss: 0.1749 - acc: 0.940 - ETA: 4s - loss: 0.1705 - acc: 0.945 - ETA: 4s - loss: 0.1654 - acc: 0.943 - ETA: 4s - loss: 0.1756 - acc: 0.936 - ETA: 4s - loss: 0.1779 - acc: 0.933 - ETA: 4s - loss: 0.1645 - acc: 0.936 - ETA: 4s - loss: 0.1563 - acc: 0.939 - ETA: 3s - loss: 0.1530 - acc: 0.943 - ETA: 3s - loss: 0.1523 - acc: 0.943 - ETA: 3s - loss: 0.1570 - acc: 0.941 - ETA: 3s - loss: 0.1508 - acc: 0.944 - ETA: 3s - loss: 0.1508 - acc: 0.942 - ETA: 3s - loss: 0.1495 - acc: 0.943 - ETA: 3s - loss: 0.1483 - acc: 0.943 - ETA: 3s - loss: 0.1462 - acc: 0.944 - ETA: 3s - loss: 0.1446 - acc: 0.945 - ETA: 2s - loss: 0.1416 - acc: 0.946 - ETA: 2s - loss: 0.1482 - acc: 0.945 - ETA: 2s - loss: 0.1510 - acc: 0.943 - ETA: 2s - loss: 0.1546 - acc: 0.941 - ETA: 2s - loss: 0.1508 - acc: 0.943 - ETA: 2s - loss: 0.1503 - acc: 0.943 - ETA: 2s - loss: 0.1515 - acc: 0.943 - ETA: 2s - loss: 0.1497 - acc: 0.944 - ETA: 2s - loss: 0.1489 - acc: 0.944 - ETA: 2s - loss: 0.1493 - acc: 0.944 - ETA: 1s - loss: 0.1492 - acc: 0.943 - ETA: 1s - loss: 0.1511 - acc: 0.941 - ETA: 1s - loss: 0.1522 - acc: 0.940 - ETA: 1s - loss: 0.1517 - acc: 0.941 - ETA: 1s - loss: 0.1561 - acc: 0.939 - ETA: 1s - loss: 0.1559 - acc: 0.939 - ETA: 1s - loss: 0.1549 - acc: 0.940 - ETA: 1s - loss: 0.1546 - acc: 0.940 - ETA: 1s - loss: 0.1548 - acc: 0.940 - ETA: 1s - loss: 0.1525 - acc: 0.942 - ETA: 0s - loss: 0.1522 - acc: 0.941 - ETA: 0s - loss: 0.1510 - acc: 0.942 - ETA: 0s - loss: 0.1509 - acc: 0.942 - ETA: 0s - loss: 0.1510 - acc: 0.942 - ETA: 0s - loss: 0.1518 - acc: 0.941 - ETA: 0s - loss: 0.1507 - acc: 0.942 - ETA: 0s - loss: 0.1502 - acc: 0.942 - ETA: 0s - loss: 0.1502 - acc: 0.942 - ETA: 0s - loss: 0.1490 - acc: 0.943 - 5s 103ms/step - loss: 0.1478 - acc: 0.9436\n",
      "Epoch 88/10000\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1147 - acc: 0.943 - ETA: 4s - loss: 0.1371 - acc: 0.943 - ETA: 4s - loss: 0.1460 - acc: 0.943 - ETA: 4s - loss: 0.1411 - acc: 0.943 - ETA: 4s - loss: 0.1380 - acc: 0.945 - ETA: 4s - loss: 0.1297 - acc: 0.950 - ETA: 4s - loss: 0.1207 - acc: 0.956 - ETA: 4s - loss: 0.1253 - acc: 0.954 - ETA: 4s - loss: 0.1263 - acc: 0.954 - ETA: 3s - loss: 0.1279 - acc: 0.956 - ETA: 3s - loss: 0.1262 - acc: 0.957 - ETA: 3s - loss: 0.1299 - acc: 0.955 - ETA: 3s - loss: 0.1346 - acc: 0.954 - ETA: 3s - loss: 0.1342 - acc: 0.953 - ETA: 3s - loss: 0.1414 - acc: 0.949 - ETA: 3s - loss: 0.1384 - acc: 0.949 - ETA: 3s - loss: 0.1396 - acc: 0.948 - ETA: 3s - loss: 0.1387 - acc: 0.947 - ETA: 3s - loss: 0.1438 - acc: 0.947 - ETA: 2s - loss: 0.1478 - acc: 0.944 - ETA: 2s - loss: 0.1466 - acc: 0.943 - ETA: 2s - loss: 0.1482 - acc: 0.943 - ETA: 2s - loss: 0.1452 - acc: 0.945 - ETA: 2s - loss: 0.1481 - acc: 0.944 - ETA: 2s - loss: 0.1532 - acc: 0.943 - ETA: 2s - loss: 0.1497 - acc: 0.944 - ETA: 2s - loss: 0.1503 - acc: 0.945 - ETA: 2s - loss: 0.1550 - acc: 0.942 - ETA: 2s - loss: 0.1536 - acc: 0.943 - ETA: 1s - loss: 0.1537 - acc: 0.942 - ETA: 1s - loss: 0.1558 - acc: 0.941 - ETA: 1s - loss: 0.1560 - acc: 0.941 - ETA: 1s - loss: 0.1558 - acc: 0.941 - ETA: 1s - loss: 0.1526 - acc: 0.943 - ETA: 1s - loss: 0.1516 - acc: 0.943 - ETA: 1s - loss: 0.1514 - acc: 0.943 - ETA: 1s - loss: 0.1493 - acc: 0.944 - ETA: 1s - loss: 0.1513 - acc: 0.943 - ETA: 1s - loss: 0.1497 - acc: 0.945 - ETA: 0s - loss: 0.1497 - acc: 0.945 - ETA: 0s - loss: 0.1481 - acc: 0.946 - ETA: 0s - loss: 0.1483 - acc: 0.945 - ETA: 0s - loss: 0.1481 - acc: 0.945 - ETA: 0s - loss: 0.1496 - acc: 0.944 - ETA: 0s - loss: 0.1519 - acc: 0.943 - ETA: 0s - loss: 0.1506 - acc: 0.944 - ETA: 0s - loss: 0.1493 - acc: 0.945 - ETA: 0s - loss: 0.1498 - acc: 0.944 - 5s 103ms/step - loss: 0.1502 - acc: 0.9439\n",
      "Epoch 89/10000\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.2106 - acc: 0.943 - ETA: 4s - loss: 0.1400 - acc: 0.965 - ETA: 4s - loss: 0.1292 - acc: 0.958 - ETA: 4s - loss: 0.1288 - acc: 0.957 - ETA: 4s - loss: 0.1476 - acc: 0.950 - ETA: 4s - loss: 0.1446 - acc: 0.947 - ETA: 4s - loss: 0.1359 - acc: 0.948 - ETA: 4s - loss: 0.1300 - acc: 0.951 - ETA: 4s - loss: 0.1283 - acc: 0.953 - ETA: 4s - loss: 0.1263 - acc: 0.954 - ETA: 3s - loss: 0.1296 - acc: 0.953 - ETA: 3s - loss: 0.1323 - acc: 0.951 - ETA: 3s - loss: 0.1274 - acc: 0.952 - ETA: 3s - loss: 0.1241 - acc: 0.954 - ETA: 3s - loss: 0.1223 - acc: 0.956 - ETA: 3s - loss: 0.1223 - acc: 0.956 - ETA: 3s - loss: 0.1215 - acc: 0.956 - ETA: 3s - loss: 0.1231 - acc: 0.955 - ETA: 3s - loss: 0.1223 - acc: 0.955 - ETA: 2s - loss: 0.1257 - acc: 0.955 - ETA: 2s - loss: 0.1286 - acc: 0.954 - ETA: 2s - loss: 0.1245 - acc: 0.956 - ETA: 2s - loss: 0.1249 - acc: 0.956 - ETA: 2s - loss: 0.1258 - acc: 0.956 - ETA: 2s - loss: 0.1262 - acc: 0.955 - ETA: 2s - loss: 0.1247 - acc: 0.955 - ETA: 2s - loss: 0.1246 - acc: 0.955 - ETA: 2s - loss: 0.1275 - acc: 0.954 - ETA: 2s - loss: 0.1257 - acc: 0.955 - ETA: 1s - loss: 0.1273 - acc: 0.953 - ETA: 1s - loss: 0.1275 - acc: 0.954 - ETA: 1s - loss: 0.1257 - acc: 0.955 - ETA: 1s - loss: 0.1242 - acc: 0.956 - ETA: 1s - loss: 0.1260 - acc: 0.956 - ETA: 1s - loss: 0.1246 - acc: 0.956 - ETA: 1s - loss: 0.1268 - acc: 0.956 - ETA: 1s - loss: 0.1291 - acc: 0.955 - ETA: 1s - loss: 0.1286 - acc: 0.956 - ETA: 1s - loss: 0.1280 - acc: 0.956 - ETA: 0s - loss: 0.1264 - acc: 0.956 - ETA: 0s - loss: 0.1267 - acc: 0.956 - ETA: 0s - loss: 0.1246 - acc: 0.957 - ETA: 0s - loss: 0.1251 - acc: 0.957 - ETA: 0s - loss: 0.1249 - acc: 0.957 - ETA: 0s - loss: 0.1238 - acc: 0.958 - ETA: 0s - loss: 0.1219 - acc: 0.959 - ETA: 0s - loss: 0.1224 - acc: 0.959 - ETA: 0s - loss: 0.1232 - acc: 0.958 - 5s 103ms/step - loss: 0.1218 - acc: 0.9594\n",
      "Epoch 90/10000\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1128 - acc: 0.965 - ETA: 4s - loss: 0.1464 - acc: 0.937 - ETA: 4s - loss: 0.1238 - acc: 0.947 - ETA: 4s - loss: 0.1399 - acc: 0.943 - ETA: 4s - loss: 0.1265 - acc: 0.950 - ETA: 4s - loss: 0.1400 - acc: 0.950 - ETA: 4s - loss: 0.1386 - acc: 0.951 - ETA: 4s - loss: 0.1417 - acc: 0.950 - ETA: 4s - loss: 0.1387 - acc: 0.950 - ETA: 4s - loss: 0.1299 - acc: 0.954 - ETA: 3s - loss: 0.1275 - acc: 0.955 - ETA: 3s - loss: 0.1346 - acc: 0.951 - ETA: 3s - loss: 0.1319 - acc: 0.952 - ETA: 3s - loss: 0.1260 - acc: 0.956 - ETA: 3s - loss: 0.1251 - acc: 0.955 - ETA: 3s - loss: 0.1294 - acc: 0.953 - ETA: 3s - loss: 0.1328 - acc: 0.951 - ETA: 3s - loss: 0.1370 - acc: 0.949 - ETA: 3s - loss: 0.1418 - acc: 0.948 - ETA: 2s - loss: 0.1408 - acc: 0.948 - ETA: 2s - loss: 0.1395 - acc: 0.949 - ETA: 2s - loss: 0.1388 - acc: 0.948 - ETA: 2s - loss: 0.1394 - acc: 0.948 - ETA: 2s - loss: 0.1423 - acc: 0.947 - ETA: 2s - loss: 0.1462 - acc: 0.946 - ETA: 2s - loss: 0.1439 - acc: 0.947 - ETA: 2s - loss: 0.1430 - acc: 0.947 - ETA: 2s - loss: 0.1412 - acc: 0.947 - ETA: 2s - loss: 0.1430 - acc: 0.947 - ETA: 1s - loss: 0.1449 - acc: 0.946 - ETA: 1s - loss: 0.1427 - acc: 0.947 - ETA: 1s - loss: 0.1415 - acc: 0.948 - ETA: 1s - loss: 0.1407 - acc: 0.949 - ETA: 1s - loss: 0.1421 - acc: 0.948 - ETA: 1s - loss: 0.1434 - acc: 0.947 - ETA: 1s - loss: 0.1433 - acc: 0.947 - ETA: 1s - loss: 0.1424 - acc: 0.947 - ETA: 1s - loss: 0.1430 - acc: 0.947 - ETA: 1s - loss: 0.1445 - acc: 0.946 - ETA: 0s - loss: 0.1429 - acc: 0.947 - ETA: 0s - loss: 0.1407 - acc: 0.948 - ETA: 0s - loss: 0.1404 - acc: 0.949 - ETA: 0s - loss: 0.1394 - acc: 0.949 - ETA: 0s - loss: 0.1383 - acc: 0.950 - ETA: 0s - loss: 0.1377 - acc: 0.950 - ETA: 0s - loss: 0.1375 - acc: 0.950 - ETA: 0s - loss: 0.1380 - acc: 0.950 - ETA: 0s - loss: 0.1391 - acc: 0.949 - 5s 103ms/step - loss: 0.1380 - acc: 0.9502\n",
      "Epoch 91/10000\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.2172 - acc: 0.920 - ETA: 4s - loss: 0.1406 - acc: 0.948 - ETA: 4s - loss: 0.1294 - acc: 0.954 - ETA: 4s - loss: 0.1183 - acc: 0.960 - ETA: 4s - loss: 0.1257 - acc: 0.961 - ETA: 4s - loss: 0.1184 - acc: 0.962 - ETA: 4s - loss: 0.1335 - acc: 0.959 - ETA: 4s - loss: 0.1447 - acc: 0.951 - ETA: 4s - loss: 0.1494 - acc: 0.945 - ETA: 3s - loss: 0.1502 - acc: 0.942 - ETA: 3s - loss: 0.1618 - acc: 0.937 - ETA: 3s - loss: 0.1562 - acc: 0.939 - ETA: 3s - loss: 0.1499 - acc: 0.942 - ETA: 3s - loss: 0.1465 - acc: 0.944 - ETA: 3s - loss: 0.1431 - acc: 0.945 - ETA: 3s - loss: 0.1430 - acc: 0.946 - ETA: 3s - loss: 0.1407 - acc: 0.946 - ETA: 3s - loss: 0.1409 - acc: 0.946 - ETA: 3s - loss: 0.1499 - acc: 0.942 - ETA: 2s - loss: 0.1542 - acc: 0.939 - ETA: 2s - loss: 0.1550 - acc: 0.939 - ETA: 2s - loss: 0.1518 - acc: 0.940 - ETA: 2s - loss: 0.1538 - acc: 0.939 - ETA: 2s - loss: 0.1505 - acc: 0.940 - ETA: 2s - loss: 0.1464 - acc: 0.943 - ETA: 2s - loss: 0.1430 - acc: 0.944 - ETA: 2s - loss: 0.1422 - acc: 0.945 - ETA: 2s - loss: 0.1481 - acc: 0.944 - ETA: 2s - loss: 0.1499 - acc: 0.943 - ETA: 1s - loss: 0.1479 - acc: 0.943 - ETA: 1s - loss: 0.1456 - acc: 0.945 - ETA: 1s - loss: 0.1453 - acc: 0.946 - ETA: 1s - loss: 0.1480 - acc: 0.945 - ETA: 1s - loss: 0.1477 - acc: 0.945 - ETA: 1s - loss: 0.1474 - acc: 0.945 - ETA: 1s - loss: 0.1478 - acc: 0.945 - ETA: 1s - loss: 0.1464 - acc: 0.946 - ETA: 1s - loss: 0.1448 - acc: 0.947 - ETA: 1s - loss: 0.1450 - acc: 0.946 - ETA: 0s - loss: 0.1462 - acc: 0.946 - ETA: 0s - loss: 0.1467 - acc: 0.945 - ETA: 0s - loss: 0.1452 - acc: 0.946 - ETA: 0s - loss: 0.1462 - acc: 0.946 - ETA: 0s - loss: 0.1453 - acc: 0.946 - ETA: 0s - loss: 0.1443 - acc: 0.946 - ETA: 0s - loss: 0.1428 - acc: 0.947 - ETA: 0s - loss: 0.1434 - acc: 0.947 - ETA: 0s - loss: 0.1421 - acc: 0.947 - 5s 102ms/step - loss: 0.1447 - acc: 0.9468\n",
      "Epoch 92/10000\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0867 - acc: 0.965 - ETA: 4s - loss: 0.1232 - acc: 0.948 - ETA: 4s - loss: 0.0931 - acc: 0.965 - ETA: 4s - loss: 0.0974 - acc: 0.968 - ETA: 4s - loss: 0.1003 - acc: 0.965 - ETA: 4s - loss: 0.1004 - acc: 0.967 - ETA: 4s - loss: 0.1145 - acc: 0.959 - ETA: 4s - loss: 0.1162 - acc: 0.957 - ETA: 4s - loss: 0.1201 - acc: 0.955 - ETA: 4s - loss: 0.1194 - acc: 0.954 - ETA: 3s - loss: 0.1185 - acc: 0.953 - ETA: 3s - loss: 0.1178 - acc: 0.955 - ETA: 3s - loss: 0.1201 - acc: 0.954 - ETA: 3s - loss: 0.1177 - acc: 0.955 - ETA: 3s - loss: 0.1179 - acc: 0.954 - ETA: 3s - loss: 0.1167 - acc: 0.954 - ETA: 3s - loss: 0.1176 - acc: 0.952 - ETA: 3s - loss: 0.1166 - acc: 0.953 - ETA: 3s - loss: 0.1180 - acc: 0.953 - ETA: 2s - loss: 0.1194 - acc: 0.953 - ETA: 2s - loss: 0.1226 - acc: 0.953 - ETA: 2s - loss: 0.1253 - acc: 0.952 - ETA: 2s - loss: 0.1246 - acc: 0.952 - ETA: 2s - loss: 0.1249 - acc: 0.953 - ETA: 2s - loss: 0.1307 - acc: 0.950 - ETA: 2s - loss: 0.1341 - acc: 0.949 - ETA: 2s - loss: 0.1377 - acc: 0.948 - ETA: 2s - loss: 0.1362 - acc: 0.948 - ETA: 2s - loss: 0.1408 - acc: 0.947 - ETA: 1s - loss: 0.1406 - acc: 0.947 - ETA: 1s - loss: 0.1452 - acc: 0.946 - ETA: 1s - loss: 0.1462 - acc: 0.946 - ETA: 1s - loss: 0.1477 - acc: 0.946 - ETA: 1s - loss: 0.1466 - acc: 0.947 - ETA: 1s - loss: 0.1469 - acc: 0.946 - ETA: 1s - loss: 0.1449 - acc: 0.947 - ETA: 1s - loss: 0.1428 - acc: 0.948 - ETA: 1s - loss: 0.1435 - acc: 0.947 - ETA: 1s - loss: 0.1426 - acc: 0.948 - ETA: 0s - loss: 0.1428 - acc: 0.948 - ETA: 0s - loss: 0.1420 - acc: 0.948 - ETA: 0s - loss: 0.1411 - acc: 0.949 - ETA: 0s - loss: 0.1406 - acc: 0.949 - ETA: 0s - loss: 0.1406 - acc: 0.948 - ETA: 0s - loss: 0.1391 - acc: 0.949 - ETA: 0s - loss: 0.1405 - acc: 0.948 - ETA: 0s - loss: 0.1413 - acc: 0.947 - ETA: 0s - loss: 0.1411 - acc: 0.947 - 5s 103ms/step - loss: 0.1400 - acc: 0.9484\n",
      "Epoch 93/10000\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1482 - acc: 0.909 - ETA: 4s - loss: 0.1118 - acc: 0.943 - ETA: 4s - loss: 0.1333 - acc: 0.943 - ETA: 4s - loss: 0.1204 - acc: 0.944 - ETA: 4s - loss: 0.1160 - acc: 0.948 - ETA: 4s - loss: 0.1061 - acc: 0.957 - ETA: 4s - loss: 0.1124 - acc: 0.955 - ETA: 4s - loss: 0.1199 - acc: 0.950 - ETA: 4s - loss: 0.1127 - acc: 0.953 - ETA: 4s - loss: 0.1145 - acc: 0.952 - ETA: 3s - loss: 0.1099 - acc: 0.955 - ETA: 3s - loss: 0.1046 - acc: 0.957 - ETA: 3s - loss: 0.1063 - acc: 0.957 - ETA: 3s - loss: 0.1133 - acc: 0.954 - ETA: 3s - loss: 0.1166 - acc: 0.954 - ETA: 3s - loss: 0.1131 - acc: 0.955 - ETA: 3s - loss: 0.1144 - acc: 0.954 - ETA: 3s - loss: 0.1109 - acc: 0.956 - ETA: 3s - loss: 0.1098 - acc: 0.957 - ETA: 2s - loss: 0.1158 - acc: 0.955 - ETA: 2s - loss: 0.1196 - acc: 0.954 - ETA: 2s - loss: 0.1223 - acc: 0.953 - ETA: 2s - loss: 0.1264 - acc: 0.952 - ETA: 2s - loss: 0.1261 - acc: 0.952 - ETA: 2s - loss: 0.1259 - acc: 0.952 - ETA: 2s - loss: 0.1248 - acc: 0.953 - ETA: 2s - loss: 0.1225 - acc: 0.954 - ETA: 2s - loss: 0.1218 - acc: 0.955 - ETA: 2s - loss: 0.1203 - acc: 0.955 - ETA: 1s - loss: 0.1186 - acc: 0.956 - ETA: 1s - loss: 0.1241 - acc: 0.955 - ETA: 1s - loss: 0.1265 - acc: 0.953 - ETA: 1s - loss: 0.1262 - acc: 0.953 - ETA: 1s - loss: 0.1296 - acc: 0.952 - ETA: 1s - loss: 0.1302 - acc: 0.951 - ETA: 1s - loss: 0.1346 - acc: 0.949 - ETA: 1s - loss: 0.1337 - acc: 0.950 - ETA: 1s - loss: 0.1323 - acc: 0.950 - ETA: 1s - loss: 0.1340 - acc: 0.949 - ETA: 0s - loss: 0.1326 - acc: 0.950 - ETA: 0s - loss: 0.1329 - acc: 0.949 - ETA: 0s - loss: 0.1332 - acc: 0.950 - ETA: 0s - loss: 0.1340 - acc: 0.950 - ETA: 0s - loss: 0.1339 - acc: 0.950 - ETA: 0s - loss: 0.1338 - acc: 0.950 - ETA: 0s - loss: 0.1338 - acc: 0.950 - ETA: 0s - loss: 0.1320 - acc: 0.951 - ETA: 0s - loss: 0.1324 - acc: 0.951 - 5s 103ms/step - loss: 0.1314 - acc: 0.9516\n",
      "Epoch 94/10000\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.2473 - acc: 0.896 - ETA: 4s - loss: 0.1842 - acc: 0.931 - ETA: 4s - loss: 0.1649 - acc: 0.938 - ETA: 4s - loss: 0.1635 - acc: 0.942 - ETA: 4s - loss: 0.1538 - acc: 0.945 - ETA: 4s - loss: 0.1483 - acc: 0.944 - ETA: 4s - loss: 0.1473 - acc: 0.946 - ETA: 4s - loss: 0.1473 - acc: 0.943 - ETA: 4s - loss: 0.1475 - acc: 0.944 - ETA: 4s - loss: 0.1397 - acc: 0.948 - ETA: 3s - loss: 0.1503 - acc: 0.944 - ETA: 3s - loss: 0.1459 - acc: 0.945 - ETA: 3s - loss: 0.1504 - acc: 0.943 - ETA: 3s - loss: 0.1449 - acc: 0.946 - ETA: 3s - loss: 0.1445 - acc: 0.946 - ETA: 3s - loss: 0.1407 - acc: 0.947 - ETA: 3s - loss: 0.1386 - acc: 0.948 - ETA: 3s - loss: 0.1400 - acc: 0.948 - ETA: 3s - loss: 0.1386 - acc: 0.947 - ETA: 2s - loss: 0.1351 - acc: 0.948 - ETA: 2s - loss: 0.1351 - acc: 0.949 - ETA: 2s - loss: 0.1338 - acc: 0.949 - ETA: 2s - loss: 0.1386 - acc: 0.948 - ETA: 2s - loss: 0.1404 - acc: 0.947 - ETA: 2s - loss: 0.1400 - acc: 0.947 - ETA: 2s - loss: 0.1389 - acc: 0.948 - ETA: 2s - loss: 0.1390 - acc: 0.949 - ETA: 2s - loss: 0.1362 - acc: 0.950 - ETA: 2s - loss: 0.1388 - acc: 0.949 - ETA: 1s - loss: 0.1369 - acc: 0.949 - ETA: 1s - loss: 0.1374 - acc: 0.949 - ETA: 1s - loss: 0.1356 - acc: 0.950 - ETA: 1s - loss: 0.1346 - acc: 0.950 - ETA: 1s - loss: 0.1335 - acc: 0.951 - ETA: 1s - loss: 0.1311 - acc: 0.952 - ETA: 1s - loss: 0.1294 - acc: 0.954 - ETA: 1s - loss: 0.1328 - acc: 0.953 - ETA: 1s - loss: 0.1326 - acc: 0.953 - ETA: 1s - loss: 0.1309 - acc: 0.953 - ETA: 0s - loss: 0.1327 - acc: 0.953 - ETA: 0s - loss: 0.1333 - acc: 0.953 - ETA: 0s - loss: 0.1328 - acc: 0.953 - ETA: 0s - loss: 0.1345 - acc: 0.953 - ETA: 0s - loss: 0.1348 - acc: 0.953 - ETA: 0s - loss: 0.1360 - acc: 0.952 - ETA: 0s - loss: 0.1343 - acc: 0.953 - ETA: 0s - loss: 0.1339 - acc: 0.953 - ETA: 0s - loss: 0.1340 - acc: 0.952 - 5s 103ms/step - loss: 0.1348 - acc: 0.9524\n",
      "Epoch 95/10000\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0731 - acc: 0.965 - ETA: 4s - loss: 0.0841 - acc: 0.971 - ETA: 4s - loss: 0.1436 - acc: 0.947 - ETA: 4s - loss: 0.1270 - acc: 0.954 - ETA: 4s - loss: 0.1310 - acc: 0.956 - ETA: 4s - loss: 0.1256 - acc: 0.958 - ETA: 4s - loss: 0.1274 - acc: 0.954 - ETA: 4s - loss: 0.1211 - acc: 0.956 - ETA: 4s - loss: 0.1277 - acc: 0.952 - ETA: 4s - loss: 0.1290 - acc: 0.952 - ETA: 3s - loss: 0.1239 - acc: 0.954 - ETA: 3s - loss: 0.1218 - acc: 0.953 - ETA: 3s - loss: 0.1272 - acc: 0.952 - ETA: 3s - loss: 0.1292 - acc: 0.951 - ETA: 3s - loss: 0.1362 - acc: 0.949 - ETA: 3s - loss: 0.1390 - acc: 0.948 - ETA: 3s - loss: 0.1457 - acc: 0.945 - ETA: 3s - loss: 0.1439 - acc: 0.947 - ETA: 3s - loss: 0.1414 - acc: 0.948 - ETA: 2s - loss: 0.1389 - acc: 0.950 - ETA: 2s - loss: 0.1374 - acc: 0.950 - ETA: 2s - loss: 0.1454 - acc: 0.948 - ETA: 2s - loss: 0.1435 - acc: 0.949 - ETA: 2s - loss: 0.1440 - acc: 0.949 - ETA: 2s - loss: 0.1408 - acc: 0.951 - ETA: 2s - loss: 0.1443 - acc: 0.948 - ETA: 2s - loss: 0.1440 - acc: 0.948 - ETA: 2s - loss: 0.1418 - acc: 0.949 - ETA: 2s - loss: 0.1395 - acc: 0.950 - ETA: 1s - loss: 0.1381 - acc: 0.951 - ETA: 1s - loss: 0.1355 - acc: 0.952 - ETA: 1s - loss: 0.1330 - acc: 0.953 - ETA: 1s - loss: 0.1322 - acc: 0.953 - ETA: 1s - loss: 0.1352 - acc: 0.952 - ETA: 1s - loss: 0.1354 - acc: 0.951 - ETA: 1s - loss: 0.1368 - acc: 0.951 - ETA: 1s - loss: 0.1378 - acc: 0.950 - ETA: 1s - loss: 0.1385 - acc: 0.950 - ETA: 1s - loss: 0.1408 - acc: 0.950 - ETA: 0s - loss: 0.1414 - acc: 0.950 - ETA: 0s - loss: 0.1406 - acc: 0.951 - ETA: 0s - loss: 0.1400 - acc: 0.951 - ETA: 0s - loss: 0.1401 - acc: 0.951 - ETA: 0s - loss: 0.1387 - acc: 0.952 - ETA: 0s - loss: 0.1376 - acc: 0.953 - ETA: 0s - loss: 0.1368 - acc: 0.953 - ETA: 0s - loss: 0.1371 - acc: 0.953 - ETA: 0s - loss: 0.1381 - acc: 0.953 - 5s 103ms/step - loss: 0.1370 - acc: 0.9535\n",
      "Epoch 96/10000\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1820 - acc: 0.920 - ETA: 4s - loss: 0.1519 - acc: 0.943 - ETA: 4s - loss: 0.1519 - acc: 0.939 - ETA: 4s - loss: 0.1567 - acc: 0.934 - ETA: 4s - loss: 0.1370 - acc: 0.943 - ETA: 4s - loss: 0.1327 - acc: 0.945 - ETA: 4s - loss: 0.1212 - acc: 0.949 - ETA: 4s - loss: 0.1253 - acc: 0.948 - ETA: 4s - loss: 0.1181 - acc: 0.952 - ETA: 4s - loss: 0.1157 - acc: 0.952 - ETA: 3s - loss: 0.1185 - acc: 0.951 - ETA: 3s - loss: 0.1204 - acc: 0.950 - ETA: 3s - loss: 0.1195 - acc: 0.950 - ETA: 3s - loss: 0.1201 - acc: 0.949 - ETA: 3s - loss: 0.1342 - acc: 0.944 - ETA: 3s - loss: 0.1304 - acc: 0.946 - ETA: 3s - loss: 0.1329 - acc: 0.945 - ETA: 3s - loss: 0.1357 - acc: 0.945 - ETA: 3s - loss: 0.1406 - acc: 0.943 - ETA: 3s - loss: 0.1404 - acc: 0.944 - ETA: 2s - loss: 0.1390 - acc: 0.945 - ETA: 2s - loss: 0.1391 - acc: 0.944 - ETA: 2s - loss: 0.1373 - acc: 0.944 - ETA: 2s - loss: 0.1365 - acc: 0.945 - ETA: 2s - loss: 0.1359 - acc: 0.945 - ETA: 2s - loss: 0.1368 - acc: 0.945 - ETA: 2s - loss: 0.1377 - acc: 0.944 - ETA: 2s - loss: 0.1354 - acc: 0.945 - ETA: 2s - loss: 0.1353 - acc: 0.945 - ETA: 1s - loss: 0.1336 - acc: 0.946 - ETA: 1s - loss: 0.1318 - acc: 0.947 - ETA: 1s - loss: 0.1293 - acc: 0.948 - ETA: 1s - loss: 0.1289 - acc: 0.949 - ETA: 1s - loss: 0.1291 - acc: 0.949 - ETA: 1s - loss: 0.1279 - acc: 0.950 - ETA: 1s - loss: 0.1292 - acc: 0.948 - ETA: 1s - loss: 0.1331 - acc: 0.947 - ETA: 1s - loss: 0.1342 - acc: 0.946 - ETA: 1s - loss: 0.1339 - acc: 0.947 - ETA: 0s - loss: 0.1330 - acc: 0.947 - ETA: 0s - loss: 0.1334 - acc: 0.948 - ETA: 0s - loss: 0.1365 - acc: 0.946 - ETA: 0s - loss: 0.1375 - acc: 0.946 - ETA: 0s - loss: 0.1357 - acc: 0.947 - ETA: 0s - loss: 0.1349 - acc: 0.947 - ETA: 0s - loss: 0.1348 - acc: 0.947 - ETA: 0s - loss: 0.1342 - acc: 0.947 - ETA: 0s - loss: 0.1361 - acc: 0.946 - 5s 104ms/step - loss: 0.1360 - acc: 0.9470\n",
      "Epoch 97/10000\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1408 - acc: 0.920 - ETA: 4s - loss: 0.1961 - acc: 0.914 - ETA: 4s - loss: 0.1665 - acc: 0.931 - ETA: 4s - loss: 0.1691 - acc: 0.926 - ETA: 4s - loss: 0.1524 - acc: 0.931 - ETA: 4s - loss: 0.1450 - acc: 0.935 - ETA: 4s - loss: 0.1532 - acc: 0.933 - ETA: 4s - loss: 0.1539 - acc: 0.931 - ETA: 4s - loss: 0.1425 - acc: 0.938 - ETA: 4s - loss: 0.1447 - acc: 0.937 - ETA: 3s - loss: 0.1390 - acc: 0.941 - ETA: 3s - loss: 0.1389 - acc: 0.942 - ETA: 3s - loss: 0.1426 - acc: 0.944 - ETA: 3s - loss: 0.1453 - acc: 0.944 - ETA: 3s - loss: 0.1422 - acc: 0.947 - ETA: 3s - loss: 0.1466 - acc: 0.943 - ETA: 3s - loss: 0.1482 - acc: 0.942 - ETA: 3s - loss: 0.1484 - acc: 0.942 - ETA: 3s - loss: 0.1506 - acc: 0.942 - ETA: 3s - loss: 0.1502 - acc: 0.942 - ETA: 2s - loss: 0.1473 - acc: 0.943 - ETA: 2s - loss: 0.1472 - acc: 0.944 - ETA: 2s - loss: 0.1444 - acc: 0.944 - ETA: 2s - loss: 0.1433 - acc: 0.944 - ETA: 2s - loss: 0.1471 - acc: 0.942 - ETA: 2s - loss: 0.1495 - acc: 0.941 - ETA: 2s - loss: 0.1469 - acc: 0.942 - ETA: 2s - loss: 0.1491 - acc: 0.942 - ETA: 2s - loss: 0.1518 - acc: 0.941 - ETA: 1s - loss: 0.1505 - acc: 0.942 - ETA: 1s - loss: 0.1485 - acc: 0.943 - ETA: 1s - loss: 0.1481 - acc: 0.944 - ETA: 1s - loss: 0.1473 - acc: 0.945 - ETA: 1s - loss: 0.1461 - acc: 0.945 - ETA: 1s - loss: 0.1445 - acc: 0.946 - ETA: 1s - loss: 0.1445 - acc: 0.945 - ETA: 1s - loss: 0.1440 - acc: 0.945 - ETA: 1s - loss: 0.1434 - acc: 0.945 - ETA: 1s - loss: 0.1419 - acc: 0.946 - ETA: 0s - loss: 0.1431 - acc: 0.945 - ETA: 0s - loss: 0.1417 - acc: 0.946 - ETA: 0s - loss: 0.1405 - acc: 0.947 - ETA: 0s - loss: 0.1400 - acc: 0.947 - ETA: 0s - loss: 0.1401 - acc: 0.947 - ETA: 0s - loss: 0.1424 - acc: 0.947 - ETA: 0s - loss: 0.1433 - acc: 0.947 - ETA: 0s - loss: 0.1446 - acc: 0.947 - ETA: 0s - loss: 0.1429 - acc: 0.947 - 5s 104ms/step - loss: 0.1420 - acc: 0.9482\n",
      "Epoch 98/10000\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.0590 - acc: 0.988 - ETA: 4s - loss: 0.1054 - acc: 0.971 - ETA: 4s - loss: 0.1494 - acc: 0.943 - ETA: 4s - loss: 0.1610 - acc: 0.940 - ETA: 4s - loss: 0.1550 - acc: 0.940 - ETA: 4s - loss: 0.1547 - acc: 0.939 - ETA: 4s - loss: 0.1530 - acc: 0.942 - ETA: 4s - loss: 0.1495 - acc: 0.942 - ETA: 4s - loss: 0.1472 - acc: 0.943 - ETA: 3s - loss: 0.1549 - acc: 0.943 - ETA: 3s - loss: 0.1471 - acc: 0.947 - ETA: 3s - loss: 0.1422 - acc: 0.950 - ETA: 3s - loss: 0.1380 - acc: 0.952 - ETA: 3s - loss: 0.1388 - acc: 0.954 - ETA: 3s - loss: 0.1339 - acc: 0.956 - ETA: 3s - loss: 0.1341 - acc: 0.955 - ETA: 3s - loss: 0.1345 - acc: 0.955 - ETA: 3s - loss: 0.1326 - acc: 0.956 - ETA: 3s - loss: 0.1348 - acc: 0.955 - ETA: 2s - loss: 0.1347 - acc: 0.954 - ETA: 2s - loss: 0.1366 - acc: 0.953 - ETA: 2s - loss: 0.1384 - acc: 0.952 - ETA: 2s - loss: 0.1386 - acc: 0.952 - ETA: 2s - loss: 0.1364 - acc: 0.953 - ETA: 2s - loss: 0.1347 - acc: 0.953 - ETA: 2s - loss: 0.1366 - acc: 0.952 - ETA: 2s - loss: 0.1338 - acc: 0.953 - ETA: 2s - loss: 0.1338 - acc: 0.953 - ETA: 2s - loss: 0.1344 - acc: 0.952 - ETA: 1s - loss: 0.1349 - acc: 0.952 - ETA: 1s - loss: 0.1362 - acc: 0.952 - ETA: 1s - loss: 0.1356 - acc: 0.952 - ETA: 1s - loss: 0.1366 - acc: 0.952 - ETA: 1s - loss: 0.1371 - acc: 0.952 - ETA: 1s - loss: 0.1372 - acc: 0.952 - ETA: 1s - loss: 0.1360 - acc: 0.952 - ETA: 1s - loss: 0.1354 - acc: 0.953 - ETA: 1s - loss: 0.1370 - acc: 0.952 - ETA: 1s - loss: 0.1409 - acc: 0.951 - ETA: 0s - loss: 0.1401 - acc: 0.951 - ETA: 0s - loss: 0.1387 - acc: 0.951 - ETA: 0s - loss: 0.1408 - acc: 0.951 - ETA: 0s - loss: 0.1422 - acc: 0.950 - ETA: 0s - loss: 0.1427 - acc: 0.949 - ETA: 0s - loss: 0.1450 - acc: 0.949 - ETA: 0s - loss: 0.1444 - acc: 0.949 - ETA: 0s - loss: 0.1446 - acc: 0.949 - ETA: 0s - loss: 0.1435 - acc: 0.949 - 5s 103ms/step - loss: 0.1454 - acc: 0.9493\n",
      "Epoch 99/10000\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0772 - acc: 0.965 - ETA: 4s - loss: 0.1081 - acc: 0.965 - ETA: 4s - loss: 0.1174 - acc: 0.962 - ETA: 4s - loss: 0.1208 - acc: 0.954 - ETA: 4s - loss: 0.1222 - acc: 0.954 - ETA: 4s - loss: 0.1339 - acc: 0.945 - ETA: 4s - loss: 0.1344 - acc: 0.944 - ETA: 4s - loss: 0.1336 - acc: 0.947 - ETA: 4s - loss: 0.1299 - acc: 0.948 - ETA: 4s - loss: 0.1342 - acc: 0.944 - ETA: 3s - loss: 0.1348 - acc: 0.945 - ETA: 3s - loss: 0.1283 - acc: 0.948 - ETA: 3s - loss: 0.1344 - acc: 0.946 - ETA: 3s - loss: 0.1367 - acc: 0.947 - ETA: 3s - loss: 0.1303 - acc: 0.951 - ETA: 3s - loss: 0.1266 - acc: 0.952 - ETA: 3s - loss: 0.1223 - acc: 0.955 - ETA: 3s - loss: 0.1252 - acc: 0.953 - ETA: 3s - loss: 0.1266 - acc: 0.953 - ETA: 2s - loss: 0.1314 - acc: 0.950 - ETA: 2s - loss: 0.1328 - acc: 0.950 - ETA: 2s - loss: 0.1333 - acc: 0.949 - ETA: 2s - loss: 0.1308 - acc: 0.950 - ETA: 2s - loss: 0.1328 - acc: 0.950 - ETA: 2s - loss: 0.1342 - acc: 0.949 - ETA: 2s - loss: 0.1356 - acc: 0.949 - ETA: 2s - loss: 0.1347 - acc: 0.950 - ETA: 2s - loss: 0.1378 - acc: 0.949 - ETA: 2s - loss: 0.1357 - acc: 0.951 - ETA: 1s - loss: 0.1345 - acc: 0.951 - ETA: 1s - loss: 0.1340 - acc: 0.950 - ETA: 1s - loss: 0.1325 - acc: 0.950 - ETA: 1s - loss: 0.1322 - acc: 0.949 - ETA: 1s - loss: 0.1321 - acc: 0.950 - ETA: 1s - loss: 0.1315 - acc: 0.950 - ETA: 1s - loss: 0.1314 - acc: 0.950 - ETA: 1s - loss: 0.1345 - acc: 0.948 - ETA: 1s - loss: 0.1401 - acc: 0.947 - ETA: 1s - loss: 0.1393 - acc: 0.947 - ETA: 0s - loss: 0.1373 - acc: 0.948 - ETA: 0s - loss: 0.1362 - acc: 0.948 - ETA: 0s - loss: 0.1370 - acc: 0.948 - ETA: 0s - loss: 0.1361 - acc: 0.949 - ETA: 0s - loss: 0.1382 - acc: 0.948 - ETA: 0s - loss: 0.1371 - acc: 0.948 - ETA: 0s - loss: 0.1361 - acc: 0.949 - ETA: 0s - loss: 0.1382 - acc: 0.948 - ETA: 0s - loss: 0.1388 - acc: 0.948 - 5s 103ms/step - loss: 0.1392 - acc: 0.9482\n",
      "Epoch 100/10000\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.00032768002711236477.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1107 - acc: 0.954 - ETA: 4s - loss: 0.0844 - acc: 0.971 - ETA: 4s - loss: 0.1134 - acc: 0.958 - ETA: 4s - loss: 0.1088 - acc: 0.960 - ETA: 4s - loss: 0.1150 - acc: 0.954 - ETA: 4s - loss: 0.1144 - acc: 0.958 - ETA: 4s - loss: 0.1050 - acc: 0.964 - ETA: 4s - loss: 0.1125 - acc: 0.961 - ETA: 4s - loss: 0.1188 - acc: 0.960 - ETA: 4s - loss: 0.1254 - acc: 0.959 - ETA: 3s - loss: 0.1253 - acc: 0.958 - ETA: 3s - loss: 0.1288 - acc: 0.957 - ETA: 3s - loss: 0.1359 - acc: 0.953 - ETA: 3s - loss: 0.1330 - acc: 0.954 - ETA: 3s - loss: 0.1402 - acc: 0.953 - ETA: 3s - loss: 0.1359 - acc: 0.956 - ETA: 3s - loss: 0.1380 - acc: 0.954 - ETA: 3s - loss: 0.1412 - acc: 0.953 - ETA: 3s - loss: 0.1435 - acc: 0.951 - ETA: 2s - loss: 0.1460 - acc: 0.951 - ETA: 2s - loss: 0.1418 - acc: 0.953 - ETA: 2s - loss: 0.1409 - acc: 0.953 - ETA: 2s - loss: 0.1447 - acc: 0.952 - ETA: 2s - loss: 0.1441 - acc: 0.953 - ETA: 2s - loss: 0.1428 - acc: 0.954 - ETA: 2s - loss: 0.1426 - acc: 0.953 - ETA: 2s - loss: 0.1416 - acc: 0.953 - ETA: 2s - loss: 0.1404 - acc: 0.954 - ETA: 2s - loss: 0.1388 - acc: 0.954 - ETA: 1s - loss: 0.1376 - acc: 0.955 - ETA: 1s - loss: 0.1354 - acc: 0.956 - ETA: 1s - loss: 0.1366 - acc: 0.956 - ETA: 1s - loss: 0.1346 - acc: 0.957 - ETA: 1s - loss: 0.1368 - acc: 0.955 - ETA: 1s - loss: 0.1349 - acc: 0.956 - ETA: 1s - loss: 0.1369 - acc: 0.955 - ETA: 1s - loss: 0.1356 - acc: 0.956 - ETA: 1s - loss: 0.1338 - acc: 0.957 - ETA: 1s - loss: 0.1363 - acc: 0.955 - ETA: 0s - loss: 0.1382 - acc: 0.954 - ETA: 0s - loss: 0.1389 - acc: 0.954 - ETA: 0s - loss: 0.1396 - acc: 0.954 - ETA: 0s - loss: 0.1403 - acc: 0.953 - ETA: 0s - loss: 0.1405 - acc: 0.953 - ETA: 0s - loss: 0.1396 - acc: 0.953 - ETA: 0s - loss: 0.1392 - acc: 0.953 - ETA: 0s - loss: 0.1398 - acc: 0.953 - ETA: 0s - loss: 0.1391 - acc: 0.954 - 5s 102ms/step - loss: 0.1398 - acc: 0.9537\n",
      "Epoch 101/10000\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0940 - acc: 0.977 - ETA: 5s - loss: 0.1032 - acc: 0.971 - ETA: 4s - loss: 0.1005 - acc: 0.969 - ETA: 4s - loss: 0.1528 - acc: 0.946 - ETA: 4s - loss: 0.1711 - acc: 0.938 - ETA: 4s - loss: 0.1660 - acc: 0.939 - ETA: 4s - loss: 0.1633 - acc: 0.938 - ETA: 4s - loss: 0.1516 - acc: 0.943 - ETA: 4s - loss: 0.1399 - acc: 0.949 - ETA: 4s - loss: 0.1382 - acc: 0.952 - ETA: 3s - loss: 0.1415 - acc: 0.950 - ETA: 3s - loss: 0.1385 - acc: 0.950 - ETA: 3s - loss: 0.1433 - acc: 0.950 - ETA: 3s - loss: 0.1457 - acc: 0.949 - ETA: 3s - loss: 0.1457 - acc: 0.950 - ETA: 3s - loss: 0.1434 - acc: 0.948 - ETA: 3s - loss: 0.1503 - acc: 0.945 - ETA: 3s - loss: 0.1508 - acc: 0.944 - ETA: 3s - loss: 0.1484 - acc: 0.945 - ETA: 3s - loss: 0.1473 - acc: 0.945 - ETA: 2s - loss: 0.1573 - acc: 0.942 - ETA: 2s - loss: 0.1556 - acc: 0.942 - ETA: 2s - loss: 0.1582 - acc: 0.940 - ETA: 2s - loss: 0.1592 - acc: 0.939 - ETA: 2s - loss: 0.1550 - acc: 0.941 - ETA: 2s - loss: 0.1529 - acc: 0.943 - ETA: 2s - loss: 0.1513 - acc: 0.943 - ETA: 2s - loss: 0.1496 - acc: 0.944 - ETA: 2s - loss: 0.1471 - acc: 0.945 - ETA: 1s - loss: 0.1469 - acc: 0.945 - ETA: 1s - loss: 0.1468 - acc: 0.945 - ETA: 1s - loss: 0.1434 - acc: 0.947 - ETA: 1s - loss: 0.1462 - acc: 0.947 - ETA: 1s - loss: 0.1443 - acc: 0.947 - ETA: 1s - loss: 0.1441 - acc: 0.948 - ETA: 1s - loss: 0.1450 - acc: 0.947 - ETA: 1s - loss: 0.1450 - acc: 0.948 - ETA: 1s - loss: 0.1430 - acc: 0.949 - ETA: 1s - loss: 0.1410 - acc: 0.950 - ETA: 0s - loss: 0.1410 - acc: 0.950 - ETA: 0s - loss: 0.1387 - acc: 0.950 - ETA: 0s - loss: 0.1405 - acc: 0.950 - ETA: 0s - loss: 0.1388 - acc: 0.951 - ETA: 0s - loss: 0.1375 - acc: 0.952 - ETA: 0s - loss: 0.1377 - acc: 0.952 - ETA: 0s - loss: 0.1377 - acc: 0.952 - ETA: 0s - loss: 0.1391 - acc: 0.951 - ETA: 0s - loss: 0.1371 - acc: 0.952 - 5s 103ms/step - loss: 0.1374 - acc: 0.9526\n",
      "Epoch 102/10000\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1059 - acc: 0.954 - ETA: 4s - loss: 0.1395 - acc: 0.951 - ETA: 4s - loss: 0.1295 - acc: 0.952 - ETA: 4s - loss: 0.1456 - acc: 0.938 - ETA: 4s - loss: 0.1419 - acc: 0.946 - ETA: 4s - loss: 0.1329 - acc: 0.951 - ETA: 4s - loss: 0.1354 - acc: 0.948 - ETA: 4s - loss: 0.1440 - acc: 0.946 - ETA: 4s - loss: 0.1415 - acc: 0.948 - ETA: 4s - loss: 0.1401 - acc: 0.949 - ETA: 3s - loss: 0.1407 - acc: 0.949 - ETA: 3s - loss: 0.1390 - acc: 0.951 - ETA: 3s - loss: 0.1380 - acc: 0.951 - ETA: 3s - loss: 0.1336 - acc: 0.953 - ETA: 3s - loss: 0.1338 - acc: 0.951 - ETA: 3s - loss: 0.1331 - acc: 0.951 - ETA: 3s - loss: 0.1303 - acc: 0.953 - ETA: 3s - loss: 0.1351 - acc: 0.951 - ETA: 3s - loss: 0.1322 - acc: 0.952 - ETA: 2s - loss: 0.1278 - acc: 0.954 - ETA: 2s - loss: 0.1264 - acc: 0.954 - ETA: 2s - loss: 0.1261 - acc: 0.954 - ETA: 2s - loss: 0.1259 - acc: 0.954 - ETA: 2s - loss: 0.1262 - acc: 0.953 - ETA: 2s - loss: 0.1244 - acc: 0.953 - ETA: 2s - loss: 0.1225 - acc: 0.954 - ETA: 2s - loss: 0.1222 - acc: 0.954 - ETA: 2s - loss: 0.1240 - acc: 0.954 - ETA: 2s - loss: 0.1296 - acc: 0.952 - ETA: 1s - loss: 0.1303 - acc: 0.952 - ETA: 1s - loss: 0.1307 - acc: 0.952 - ETA: 1s - loss: 0.1311 - acc: 0.952 - ETA: 1s - loss: 0.1309 - acc: 0.952 - ETA: 1s - loss: 0.1280 - acc: 0.954 - ETA: 1s - loss: 0.1296 - acc: 0.953 - ETA: 1s - loss: 0.1302 - acc: 0.952 - ETA: 1s - loss: 0.1285 - acc: 0.953 - ETA: 1s - loss: 0.1261 - acc: 0.954 - ETA: 1s - loss: 0.1256 - acc: 0.954 - ETA: 0s - loss: 0.1253 - acc: 0.954 - ETA: 0s - loss: 0.1269 - acc: 0.954 - ETA: 0s - loss: 0.1250 - acc: 0.955 - ETA: 0s - loss: 0.1284 - acc: 0.954 - ETA: 0s - loss: 0.1275 - acc: 0.954 - ETA: 0s - loss: 0.1283 - acc: 0.955 - ETA: 0s - loss: 0.1285 - acc: 0.954 - ETA: 0s - loss: 0.1307 - acc: 0.953 - ETA: 0s - loss: 0.1333 - acc: 0.953 - 5s 103ms/step - loss: 0.1320 - acc: 0.9537\n",
      "Epoch 103/10000\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1431 - acc: 0.920 - ETA: 4s - loss: 0.1043 - acc: 0.948 - ETA: 4s - loss: 0.1115 - acc: 0.943 - ETA: 4s - loss: 0.0977 - acc: 0.957 - ETA: 4s - loss: 0.1063 - acc: 0.959 - ETA: 4s - loss: 0.1036 - acc: 0.958 - ETA: 4s - loss: 0.1002 - acc: 0.959 - ETA: 4s - loss: 0.1102 - acc: 0.956 - ETA: 4s - loss: 0.1097 - acc: 0.955 - ETA: 4s - loss: 0.1111 - acc: 0.954 - ETA: 3s - loss: 0.1174 - acc: 0.952 - ETA: 3s - loss: 0.1191 - acc: 0.952 - ETA: 3s - loss: 0.1254 - acc: 0.951 - ETA: 3s - loss: 0.1260 - acc: 0.952 - ETA: 3s - loss: 0.1277 - acc: 0.952 - ETA: 3s - loss: 0.1290 - acc: 0.950 - ETA: 3s - loss: 0.1293 - acc: 0.951 - ETA: 3s - loss: 0.1350 - acc: 0.949 - ETA: 3s - loss: 0.1322 - acc: 0.951 - ETA: 3s - loss: 0.1339 - acc: 0.950 - ETA: 2s - loss: 0.1316 - acc: 0.950 - ETA: 2s - loss: 0.1307 - acc: 0.950 - ETA: 2s - loss: 0.1340 - acc: 0.949 - ETA: 2s - loss: 0.1334 - acc: 0.950 - ETA: 2s - loss: 0.1311 - acc: 0.950 - ETA: 2s - loss: 0.1325 - acc: 0.949 - ETA: 2s - loss: 0.1363 - acc: 0.947 - ETA: 2s - loss: 0.1392 - acc: 0.946 - ETA: 2s - loss: 0.1393 - acc: 0.946 - ETA: 1s - loss: 0.1392 - acc: 0.947 - ETA: 1s - loss: 0.1397 - acc: 0.946 - ETA: 1s - loss: 0.1386 - acc: 0.947 - ETA: 1s - loss: 0.1383 - acc: 0.948 - ETA: 1s - loss: 0.1394 - acc: 0.947 - ETA: 1s - loss: 0.1421 - acc: 0.946 - ETA: 1s - loss: 0.1413 - acc: 0.947 - ETA: 1s - loss: 0.1405 - acc: 0.946 - ETA: 1s - loss: 0.1393 - acc: 0.947 - ETA: 1s - loss: 0.1372 - acc: 0.948 - ETA: 0s - loss: 0.1380 - acc: 0.948 - ETA: 0s - loss: 0.1375 - acc: 0.948 - ETA: 0s - loss: 0.1363 - acc: 0.949 - ETA: 0s - loss: 0.1349 - acc: 0.950 - ETA: 0s - loss: 0.1331 - acc: 0.950 - ETA: 0s - loss: 0.1333 - acc: 0.950 - ETA: 0s - loss: 0.1349 - acc: 0.950 - ETA: 0s - loss: 0.1338 - acc: 0.950 - ETA: 0s - loss: 0.1337 - acc: 0.950 - 5s 103ms/step - loss: 0.1326 - acc: 0.9514\n",
      "Epoch 104/10000\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1599 - acc: 0.931 - ETA: 4s - loss: 0.1296 - acc: 0.948 - ETA: 4s - loss: 0.1152 - acc: 0.954 - ETA: 4s - loss: 0.1240 - acc: 0.954 - ETA: 4s - loss: 0.1204 - acc: 0.954 - ETA: 4s - loss: 0.1331 - acc: 0.950 - ETA: 4s - loss: 0.1290 - acc: 0.952 - ETA: 4s - loss: 0.1245 - acc: 0.956 - ETA: 4s - loss: 0.1362 - acc: 0.953 - ETA: 4s - loss: 0.1305 - acc: 0.955 - ETA: 4s - loss: 0.1272 - acc: 0.957 - ETA: 3s - loss: 0.1264 - acc: 0.957 - ETA: 3s - loss: 0.1282 - acc: 0.958 - ETA: 3s - loss: 0.1263 - acc: 0.957 - ETA: 3s - loss: 0.1295 - acc: 0.954 - ETA: 3s - loss: 0.1255 - acc: 0.955 - ETA: 3s - loss: 0.1225 - acc: 0.957 - ETA: 3s - loss: 0.1212 - acc: 0.957 - ETA: 3s - loss: 0.1199 - acc: 0.956 - ETA: 3s - loss: 0.1207 - acc: 0.956 - ETA: 2s - loss: 0.1263 - acc: 0.955 - ETA: 2s - loss: 0.1326 - acc: 0.953 - ETA: 2s - loss: 0.1305 - acc: 0.954 - ETA: 2s - loss: 0.1298 - acc: 0.954 - ETA: 2s - loss: 0.1321 - acc: 0.954 - ETA: 2s - loss: 0.1315 - acc: 0.954 - ETA: 2s - loss: 0.1285 - acc: 0.955 - ETA: 2s - loss: 0.1294 - acc: 0.956 - ETA: 2s - loss: 0.1284 - acc: 0.956 - ETA: 1s - loss: 0.1284 - acc: 0.956 - ETA: 1s - loss: 0.1283 - acc: 0.956 - ETA: 1s - loss: 0.1294 - acc: 0.956 - ETA: 1s - loss: 0.1313 - acc: 0.955 - ETA: 1s - loss: 0.1309 - acc: 0.955 - ETA: 1s - loss: 0.1297 - acc: 0.955 - ETA: 1s - loss: 0.1330 - acc: 0.954 - ETA: 1s - loss: 0.1329 - acc: 0.953 - ETA: 1s - loss: 0.1319 - acc: 0.953 - ETA: 1s - loss: 0.1309 - acc: 0.953 - ETA: 0s - loss: 0.1331 - acc: 0.952 - ETA: 0s - loss: 0.1321 - acc: 0.952 - ETA: 0s - loss: 0.1317 - acc: 0.952 - ETA: 0s - loss: 0.1335 - acc: 0.952 - ETA: 0s - loss: 0.1324 - acc: 0.952 - ETA: 0s - loss: 0.1315 - acc: 0.952 - ETA: 0s - loss: 0.1310 - acc: 0.952 - ETA: 0s - loss: 0.1330 - acc: 0.952 - ETA: 0s - loss: 0.1325 - acc: 0.952 - 5s 104ms/step - loss: 0.1318 - acc: 0.9523\n",
      "Epoch 105/10000\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.0960 - acc: 0.977 - ETA: 4s - loss: 0.0961 - acc: 0.965 - ETA: 4s - loss: 0.0905 - acc: 0.965 - ETA: 4s - loss: 0.0935 - acc: 0.965 - ETA: 4s - loss: 0.0853 - acc: 0.970 - ETA: 4s - loss: 0.0963 - acc: 0.964 - ETA: 4s - loss: 0.1003 - acc: 0.964 - ETA: 4s - loss: 0.1006 - acc: 0.963 - ETA: 4s - loss: 0.1007 - acc: 0.963 - ETA: 4s - loss: 0.1048 - acc: 0.963 - ETA: 3s - loss: 0.1046 - acc: 0.964 - ETA: 3s - loss: 0.1108 - acc: 0.961 - ETA: 3s - loss: 0.1124 - acc: 0.959 - ETA: 3s - loss: 0.1194 - acc: 0.957 - ETA: 3s - loss: 0.1210 - acc: 0.958 - ETA: 3s - loss: 0.1255 - acc: 0.955 - ETA: 3s - loss: 0.1232 - acc: 0.957 - ETA: 3s - loss: 0.1185 - acc: 0.958 - ETA: 3s - loss: 0.1188 - acc: 0.957 - ETA: 3s - loss: 0.1287 - acc: 0.955 - ETA: 2s - loss: 0.1286 - acc: 0.954 - ETA: 2s - loss: 0.1292 - acc: 0.955 - ETA: 2s - loss: 0.1261 - acc: 0.956 - ETA: 2s - loss: 0.1296 - acc: 0.954 - ETA: 2s - loss: 0.1273 - acc: 0.955 - ETA: 2s - loss: 0.1280 - acc: 0.954 - ETA: 2s - loss: 0.1303 - acc: 0.953 - ETA: 2s - loss: 0.1300 - acc: 0.952 - ETA: 2s - loss: 0.1297 - acc: 0.953 - ETA: 1s - loss: 0.1302 - acc: 0.952 - ETA: 1s - loss: 0.1303 - acc: 0.951 - ETA: 1s - loss: 0.1289 - acc: 0.952 - ETA: 1s - loss: 0.1291 - acc: 0.952 - ETA: 1s - loss: 0.1276 - acc: 0.952 - ETA: 1s - loss: 0.1260 - acc: 0.952 - ETA: 1s - loss: 0.1279 - acc: 0.952 - ETA: 1s - loss: 0.1294 - acc: 0.951 - ETA: 1s - loss: 0.1314 - acc: 0.950 - ETA: 1s - loss: 0.1306 - acc: 0.951 - ETA: 0s - loss: 0.1310 - acc: 0.951 - ETA: 0s - loss: 0.1307 - acc: 0.951 - ETA: 0s - loss: 0.1312 - acc: 0.951 - ETA: 0s - loss: 0.1337 - acc: 0.950 - ETA: 0s - loss: 0.1323 - acc: 0.951 - ETA: 0s - loss: 0.1327 - acc: 0.950 - ETA: 0s - loss: 0.1311 - acc: 0.951 - ETA: 0s - loss: 0.1309 - acc: 0.951 - ETA: 0s - loss: 0.1322 - acc: 0.950 - 5s 103ms/step - loss: 0.1332 - acc: 0.9510\n",
      "Epoch 106/10000\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1446 - acc: 0.943 - ETA: 4s - loss: 0.1086 - acc: 0.971 - ETA: 4s - loss: 0.1335 - acc: 0.958 - ETA: 4s - loss: 0.1344 - acc: 0.963 - ETA: 4s - loss: 0.1204 - acc: 0.968 - ETA: 4s - loss: 0.1291 - acc: 0.956 - ETA: 4s - loss: 0.1312 - acc: 0.951 - ETA: 4s - loss: 0.1299 - acc: 0.950 - ETA: 4s - loss: 0.1379 - acc: 0.944 - ETA: 4s - loss: 0.1386 - acc: 0.944 - ETA: 3s - loss: 0.1372 - acc: 0.945 - ETA: 3s - loss: 0.1447 - acc: 0.940 - ETA: 3s - loss: 0.1459 - acc: 0.939 - ETA: 3s - loss: 0.1405 - acc: 0.942 - ETA: 3s - loss: 0.1401 - acc: 0.943 - ETA: 3s - loss: 0.1430 - acc: 0.943 - ETA: 3s - loss: 0.1422 - acc: 0.942 - ETA: 3s - loss: 0.1410 - acc: 0.943 - ETA: 3s - loss: 0.1393 - acc: 0.944 - ETA: 2s - loss: 0.1361 - acc: 0.945 - ETA: 2s - loss: 0.1365 - acc: 0.945 - ETA: 2s - loss: 0.1375 - acc: 0.945 - ETA: 2s - loss: 0.1389 - acc: 0.945 - ETA: 2s - loss: 0.1369 - acc: 0.947 - ETA: 2s - loss: 0.1388 - acc: 0.947 - ETA: 2s - loss: 0.1360 - acc: 0.948 - ETA: 2s - loss: 0.1373 - acc: 0.948 - ETA: 2s - loss: 0.1362 - acc: 0.949 - ETA: 2s - loss: 0.1357 - acc: 0.949 - ETA: 1s - loss: 0.1346 - acc: 0.950 - ETA: 1s - loss: 0.1325 - acc: 0.950 - ETA: 1s - loss: 0.1309 - acc: 0.951 - ETA: 1s - loss: 0.1310 - acc: 0.951 - ETA: 1s - loss: 0.1312 - acc: 0.951 - ETA: 1s - loss: 0.1297 - acc: 0.952 - ETA: 1s - loss: 0.1307 - acc: 0.951 - ETA: 1s - loss: 0.1340 - acc: 0.950 - ETA: 1s - loss: 0.1335 - acc: 0.950 - ETA: 1s - loss: 0.1317 - acc: 0.951 - ETA: 0s - loss: 0.1317 - acc: 0.950 - ETA: 0s - loss: 0.1297 - acc: 0.951 - ETA: 0s - loss: 0.1310 - acc: 0.951 - ETA: 0s - loss: 0.1322 - acc: 0.950 - ETA: 0s - loss: 0.1306 - acc: 0.951 - ETA: 0s - loss: 0.1293 - acc: 0.952 - ETA: 0s - loss: 0.1289 - acc: 0.952 - ETA: 0s - loss: 0.1295 - acc: 0.952 - ETA: 0s - loss: 0.1306 - acc: 0.951 - 5s 103ms/step - loss: 0.1292 - acc: 0.9518\n",
      "Epoch 107/10000\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1054 - acc: 0.965 - ETA: 4s - loss: 0.0939 - acc: 0.971 - ETA: 4s - loss: 0.1417 - acc: 0.958 - ETA: 4s - loss: 0.1301 - acc: 0.960 - ETA: 4s - loss: 0.1268 - acc: 0.961 - ETA: 4s - loss: 0.1376 - acc: 0.950 - ETA: 4s - loss: 0.1434 - acc: 0.951 - ETA: 4s - loss: 0.1548 - acc: 0.947 - ETA: 4s - loss: 0.1463 - acc: 0.950 - ETA: 4s - loss: 0.1421 - acc: 0.952 - ETA: 3s - loss: 0.1414 - acc: 0.952 - ETA: 3s - loss: 0.1380 - acc: 0.952 - ETA: 3s - loss: 0.1440 - acc: 0.950 - ETA: 3s - loss: 0.1396 - acc: 0.952 - ETA: 3s - loss: 0.1451 - acc: 0.950 - ETA: 3s - loss: 0.1450 - acc: 0.951 - ETA: 3s - loss: 0.1423 - acc: 0.951 - ETA: 3s - loss: 0.1418 - acc: 0.951 - ETA: 3s - loss: 0.1411 - acc: 0.952 - ETA: 3s - loss: 0.1365 - acc: 0.955 - ETA: 2s - loss: 0.1398 - acc: 0.953 - ETA: 2s - loss: 0.1381 - acc: 0.953 - ETA: 2s - loss: 0.1374 - acc: 0.954 - ETA: 2s - loss: 0.1350 - acc: 0.955 - ETA: 2s - loss: 0.1322 - acc: 0.957 - ETA: 2s - loss: 0.1352 - acc: 0.954 - ETA: 2s - loss: 0.1331 - acc: 0.954 - ETA: 2s - loss: 0.1326 - acc: 0.954 - ETA: 2s - loss: 0.1331 - acc: 0.953 - ETA: 1s - loss: 0.1380 - acc: 0.952 - ETA: 1s - loss: 0.1372 - acc: 0.952 - ETA: 1s - loss: 0.1359 - acc: 0.953 - ETA: 1s - loss: 0.1364 - acc: 0.953 - ETA: 1s - loss: 0.1355 - acc: 0.953 - ETA: 1s - loss: 0.1338 - acc: 0.954 - ETA: 1s - loss: 0.1345 - acc: 0.953 - ETA: 1s - loss: 0.1337 - acc: 0.952 - ETA: 1s - loss: 0.1337 - acc: 0.952 - ETA: 1s - loss: 0.1338 - acc: 0.952 - ETA: 0s - loss: 0.1340 - acc: 0.951 - ETA: 0s - loss: 0.1358 - acc: 0.951 - ETA: 0s - loss: 0.1344 - acc: 0.952 - ETA: 0s - loss: 0.1354 - acc: 0.951 - ETA: 0s - loss: 0.1361 - acc: 0.951 - ETA: 0s - loss: 0.1356 - acc: 0.951 - ETA: 0s - loss: 0.1335 - acc: 0.952 - ETA: 0s - loss: 0.1336 - acc: 0.953 - ETA: 0s - loss: 0.1336 - acc: 0.953 - 5s 103ms/step - loss: 0.1337 - acc: 0.9526\n",
      "Epoch 108/10000\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1791 - acc: 0.943 - ETA: 4s - loss: 0.1472 - acc: 0.937 - ETA: 4s - loss: 0.1644 - acc: 0.928 - ETA: 4s - loss: 0.1664 - acc: 0.934 - ETA: 4s - loss: 0.1585 - acc: 0.938 - ETA: 4s - loss: 0.1622 - acc: 0.931 - ETA: 4s - loss: 0.1614 - acc: 0.935 - ETA: 4s - loss: 0.1570 - acc: 0.937 - ETA: 4s - loss: 0.1536 - acc: 0.941 - ETA: 4s - loss: 0.1480 - acc: 0.944 - ETA: 3s - loss: 0.1446 - acc: 0.945 - ETA: 3s - loss: 0.1479 - acc: 0.945 - ETA: 3s - loss: 0.1436 - acc: 0.946 - ETA: 3s - loss: 0.1398 - acc: 0.948 - ETA: 3s - loss: 0.1395 - acc: 0.949 - ETA: 3s - loss: 0.1350 - acc: 0.951 - ETA: 3s - loss: 0.1300 - acc: 0.953 - ETA: 3s - loss: 0.1290 - acc: 0.953 - ETA: 3s - loss: 0.1278 - acc: 0.953 - ETA: 2s - loss: 0.1302 - acc: 0.952 - ETA: 2s - loss: 0.1283 - acc: 0.953 - ETA: 2s - loss: 0.1305 - acc: 0.952 - ETA: 2s - loss: 0.1287 - acc: 0.954 - ETA: 2s - loss: 0.1275 - acc: 0.954 - ETA: 2s - loss: 0.1294 - acc: 0.954 - ETA: 2s - loss: 0.1265 - acc: 0.955 - ETA: 2s - loss: 0.1261 - acc: 0.955 - ETA: 2s - loss: 0.1284 - acc: 0.954 - ETA: 2s - loss: 0.1301 - acc: 0.953 - ETA: 1s - loss: 0.1301 - acc: 0.953 - ETA: 1s - loss: 0.1284 - acc: 0.953 - ETA: 1s - loss: 0.1322 - acc: 0.951 - ETA: 1s - loss: 0.1351 - acc: 0.950 - ETA: 1s - loss: 0.1342 - acc: 0.950 - ETA: 1s - loss: 0.1385 - acc: 0.949 - ETA: 1s - loss: 0.1376 - acc: 0.949 - ETA: 1s - loss: 0.1404 - acc: 0.948 - ETA: 1s - loss: 0.1406 - acc: 0.948 - ETA: 1s - loss: 0.1415 - acc: 0.948 - ETA: 0s - loss: 0.1425 - acc: 0.948 - ETA: 0s - loss: 0.1415 - acc: 0.948 - ETA: 0s - loss: 0.1447 - acc: 0.947 - ETA: 0s - loss: 0.1425 - acc: 0.949 - ETA: 0s - loss: 0.1437 - acc: 0.948 - ETA: 0s - loss: 0.1426 - acc: 0.949 - ETA: 0s - loss: 0.1415 - acc: 0.948 - ETA: 0s - loss: 0.1402 - acc: 0.949 - ETA: 0s - loss: 0.1398 - acc: 0.949 - 5s 103ms/step - loss: 0.1379 - acc: 0.9508\n",
      "Epoch 109/10000\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.0903 - acc: 0.977 - ETA: 4s - loss: 0.0816 - acc: 0.977 - ETA: 4s - loss: 0.0986 - acc: 0.973 - ETA: 4s - loss: 0.1043 - acc: 0.968 - ETA: 4s - loss: 0.1080 - acc: 0.965 - ETA: 4s - loss: 0.1025 - acc: 0.967 - ETA: 4s - loss: 0.1238 - acc: 0.961 - ETA: 4s - loss: 0.1257 - acc: 0.958 - ETA: 4s - loss: 0.1384 - acc: 0.952 - ETA: 4s - loss: 0.1341 - acc: 0.953 - ETA: 3s - loss: 0.1279 - acc: 0.955 - ETA: 3s - loss: 0.1278 - acc: 0.955 - ETA: 3s - loss: 0.1249 - acc: 0.957 - ETA: 3s - loss: 0.1210 - acc: 0.957 - ETA: 3s - loss: 0.1205 - acc: 0.957 - ETA: 3s - loss: 0.1169 - acc: 0.959 - ETA: 3s - loss: 0.1211 - acc: 0.957 - ETA: 3s - loss: 0.1211 - acc: 0.957 - ETA: 3s - loss: 0.1223 - acc: 0.956 - ETA: 3s - loss: 0.1214 - acc: 0.956 - ETA: 2s - loss: 0.1216 - acc: 0.956 - ETA: 2s - loss: 0.1197 - acc: 0.956 - ETA: 2s - loss: 0.1224 - acc: 0.956 - ETA: 2s - loss: 0.1207 - acc: 0.956 - ETA: 2s - loss: 0.1257 - acc: 0.954 - ETA: 2s - loss: 0.1268 - acc: 0.954 - ETA: 2s - loss: 0.1283 - acc: 0.953 - ETA: 2s - loss: 0.1271 - acc: 0.954 - ETA: 2s - loss: 0.1278 - acc: 0.954 - ETA: 1s - loss: 0.1311 - acc: 0.952 - ETA: 1s - loss: 0.1301 - acc: 0.952 - ETA: 1s - loss: 0.1308 - acc: 0.952 - ETA: 1s - loss: 0.1288 - acc: 0.953 - ETA: 1s - loss: 0.1271 - acc: 0.954 - ETA: 1s - loss: 0.1281 - acc: 0.953 - ETA: 1s - loss: 0.1281 - acc: 0.954 - ETA: 1s - loss: 0.1269 - acc: 0.954 - ETA: 1s - loss: 0.1298 - acc: 0.953 - ETA: 1s - loss: 0.1305 - acc: 0.953 - ETA: 0s - loss: 0.1309 - acc: 0.952 - ETA: 0s - loss: 0.1296 - acc: 0.952 - ETA: 0s - loss: 0.1299 - acc: 0.952 - ETA: 0s - loss: 0.1288 - acc: 0.953 - ETA: 0s - loss: 0.1305 - acc: 0.952 - ETA: 0s - loss: 0.1314 - acc: 0.952 - ETA: 0s - loss: 0.1305 - acc: 0.953 - ETA: 0s - loss: 0.1312 - acc: 0.952 - ETA: 0s - loss: 0.1298 - acc: 0.953 - 5s 103ms/step - loss: 0.1295 - acc: 0.9537\n",
      "Epoch 110/10000\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1897 - acc: 0.920 - ETA: 4s - loss: 0.1740 - acc: 0.920 - ETA: 4s - loss: 0.1613 - acc: 0.931 - ETA: 4s - loss: 0.1348 - acc: 0.946 - ETA: 4s - loss: 0.1421 - acc: 0.943 - ETA: 4s - loss: 0.1258 - acc: 0.950 - ETA: 4s - loss: 0.1212 - acc: 0.952 - ETA: 4s - loss: 0.1291 - acc: 0.951 - ETA: 4s - loss: 0.1293 - acc: 0.953 - ETA: 4s - loss: 0.1248 - acc: 0.955 - ETA: 3s - loss: 0.1289 - acc: 0.954 - ETA: 3s - loss: 0.1296 - acc: 0.953 - ETA: 3s - loss: 0.1336 - acc: 0.951 - ETA: 3s - loss: 0.1316 - acc: 0.951 - ETA: 3s - loss: 0.1342 - acc: 0.950 - ETA: 3s - loss: 0.1324 - acc: 0.951 - ETA: 3s - loss: 0.1342 - acc: 0.950 - ETA: 3s - loss: 0.1305 - acc: 0.951 - ETA: 3s - loss: 0.1384 - acc: 0.948 - ETA: 2s - loss: 0.1376 - acc: 0.950 - ETA: 2s - loss: 0.1406 - acc: 0.949 - ETA: 2s - loss: 0.1420 - acc: 0.948 - ETA: 2s - loss: 0.1377 - acc: 0.950 - ETA: 2s - loss: 0.1367 - acc: 0.949 - ETA: 2s - loss: 0.1374 - acc: 0.949 - ETA: 2s - loss: 0.1346 - acc: 0.950 - ETA: 2s - loss: 0.1352 - acc: 0.950 - ETA: 2s - loss: 0.1352 - acc: 0.950 - ETA: 2s - loss: 0.1346 - acc: 0.950 - ETA: 1s - loss: 0.1339 - acc: 0.951 - ETA: 1s - loss: 0.1358 - acc: 0.950 - ETA: 1s - loss: 0.1357 - acc: 0.950 - ETA: 1s - loss: 0.1370 - acc: 0.950 - ETA: 1s - loss: 0.1380 - acc: 0.949 - ETA: 1s - loss: 0.1348 - acc: 0.950 - ETA: 1s - loss: 0.1336 - acc: 0.951 - ETA: 1s - loss: 0.1345 - acc: 0.950 - ETA: 1s - loss: 0.1336 - acc: 0.951 - ETA: 1s - loss: 0.1358 - acc: 0.950 - ETA: 0s - loss: 0.1359 - acc: 0.950 - ETA: 0s - loss: 0.1357 - acc: 0.951 - ETA: 0s - loss: 0.1387 - acc: 0.950 - ETA: 0s - loss: 0.1381 - acc: 0.950 - ETA: 0s - loss: 0.1360 - acc: 0.951 - ETA: 0s - loss: 0.1360 - acc: 0.951 - ETA: 0s - loss: 0.1359 - acc: 0.950 - ETA: 0s - loss: 0.1345 - acc: 0.951 - ETA: 0s - loss: 0.1352 - acc: 0.951 - 5s 103ms/step - loss: 0.1340 - acc: 0.9518\n",
      "Epoch 111/10000\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1601 - acc: 0.920 - ETA: 4s - loss: 0.2080 - acc: 0.914 - ETA: 4s - loss: 0.2165 - acc: 0.909 - ETA: 4s - loss: 0.1882 - acc: 0.923 - ETA: 4s - loss: 0.1759 - acc: 0.931 - ETA: 4s - loss: 0.1666 - acc: 0.933 - ETA: 4s - loss: 0.1559 - acc: 0.935 - ETA: 4s - loss: 0.1514 - acc: 0.937 - ETA: 4s - loss: 0.1572 - acc: 0.935 - ETA: 4s - loss: 0.1606 - acc: 0.936 - ETA: 3s - loss: 0.1559 - acc: 0.941 - ETA: 3s - loss: 0.1580 - acc: 0.942 - ETA: 3s - loss: 0.1639 - acc: 0.942 - ETA: 3s - loss: 0.1605 - acc: 0.942 - ETA: 3s - loss: 0.1524 - acc: 0.945 - ETA: 3s - loss: 0.1534 - acc: 0.945 - ETA: 3s - loss: 0.1543 - acc: 0.945 - ETA: 3s - loss: 0.1542 - acc: 0.945 - ETA: 3s - loss: 0.1522 - acc: 0.945 - ETA: 3s - loss: 0.1490 - acc: 0.947 - ETA: 2s - loss: 0.1483 - acc: 0.947 - ETA: 2s - loss: 0.1534 - acc: 0.946 - ETA: 2s - loss: 0.1507 - acc: 0.948 - ETA: 2s - loss: 0.1502 - acc: 0.947 - ETA: 2s - loss: 0.1500 - acc: 0.948 - ETA: 2s - loss: 0.1472 - acc: 0.949 - ETA: 2s - loss: 0.1464 - acc: 0.949 - ETA: 2s - loss: 0.1458 - acc: 0.950 - ETA: 2s - loss: 0.1440 - acc: 0.951 - ETA: 1s - loss: 0.1423 - acc: 0.951 - ETA: 1s - loss: 0.1437 - acc: 0.951 - ETA: 1s - loss: 0.1414 - acc: 0.952 - ETA: 1s - loss: 0.1421 - acc: 0.951 - ETA: 1s - loss: 0.1407 - acc: 0.951 - ETA: 1s - loss: 0.1392 - acc: 0.952 - ETA: 1s - loss: 0.1381 - acc: 0.952 - ETA: 1s - loss: 0.1367 - acc: 0.953 - ETA: 1s - loss: 0.1357 - acc: 0.953 - ETA: 1s - loss: 0.1352 - acc: 0.953 - ETA: 0s - loss: 0.1383 - acc: 0.952 - ETA: 0s - loss: 0.1370 - acc: 0.952 - ETA: 0s - loss: 0.1378 - acc: 0.952 - ETA: 0s - loss: 0.1368 - acc: 0.952 - ETA: 0s - loss: 0.1363 - acc: 0.953 - ETA: 0s - loss: 0.1365 - acc: 0.953 - ETA: 0s - loss: 0.1352 - acc: 0.954 - ETA: 0s - loss: 0.1364 - acc: 0.953 - ETA: 0s - loss: 0.1355 - acc: 0.953 - 5s 103ms/step - loss: 0.1339 - acc: 0.9536\n",
      "Epoch 112/10000\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1648 - acc: 0.931 - ETA: 4s - loss: 0.1480 - acc: 0.937 - ETA: 4s - loss: 0.1972 - acc: 0.928 - ETA: 4s - loss: 0.1751 - acc: 0.931 - ETA: 4s - loss: 0.1898 - acc: 0.929 - ETA: 4s - loss: 0.1803 - acc: 0.933 - ETA: 4s - loss: 0.1797 - acc: 0.933 - ETA: 4s - loss: 0.1708 - acc: 0.938 - ETA: 4s - loss: 0.1754 - acc: 0.938 - ETA: 3s - loss: 0.1699 - acc: 0.939 - ETA: 3s - loss: 0.1663 - acc: 0.941 - ETA: 3s - loss: 0.1671 - acc: 0.939 - ETA: 3s - loss: 0.1580 - acc: 0.944 - ETA: 3s - loss: 0.1553 - acc: 0.944 - ETA: 3s - loss: 0.1565 - acc: 0.943 - ETA: 3s - loss: 0.1543 - acc: 0.943 - ETA: 3s - loss: 0.1562 - acc: 0.942 - ETA: 3s - loss: 0.1529 - acc: 0.943 - ETA: 3s - loss: 0.1509 - acc: 0.945 - ETA: 3s - loss: 0.1500 - acc: 0.944 - ETA: 2s - loss: 0.1455 - acc: 0.947 - ETA: 2s - loss: 0.1489 - acc: 0.946 - ETA: 2s - loss: 0.1500 - acc: 0.946 - ETA: 2s - loss: 0.1479 - acc: 0.947 - ETA: 2s - loss: 0.1447 - acc: 0.948 - ETA: 2s - loss: 0.1497 - acc: 0.947 - ETA: 2s - loss: 0.1503 - acc: 0.947 - ETA: 2s - loss: 0.1513 - acc: 0.946 - ETA: 2s - loss: 0.1536 - acc: 0.944 - ETA: 1s - loss: 0.1551 - acc: 0.944 - ETA: 1s - loss: 0.1527 - acc: 0.945 - ETA: 1s - loss: 0.1523 - acc: 0.945 - ETA: 1s - loss: 0.1492 - acc: 0.946 - ETA: 1s - loss: 0.1481 - acc: 0.947 - ETA: 1s - loss: 0.1475 - acc: 0.946 - ETA: 1s - loss: 0.1456 - acc: 0.947 - ETA: 1s - loss: 0.1450 - acc: 0.947 - ETA: 1s - loss: 0.1433 - acc: 0.948 - ETA: 1s - loss: 0.1407 - acc: 0.949 - ETA: 0s - loss: 0.1404 - acc: 0.949 - ETA: 0s - loss: 0.1417 - acc: 0.948 - ETA: 0s - loss: 0.1415 - acc: 0.948 - ETA: 0s - loss: 0.1417 - acc: 0.948 - ETA: 0s - loss: 0.1418 - acc: 0.948 - ETA: 0s - loss: 0.1410 - acc: 0.948 - ETA: 0s - loss: 0.1414 - acc: 0.947 - ETA: 0s - loss: 0.1412 - acc: 0.948 - ETA: 0s - loss: 0.1407 - acc: 0.948 - 5s 104ms/step - loss: 0.1395 - acc: 0.9487\n",
      "Epoch 113/10000\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0911 - acc: 0.965 - ETA: 4s - loss: 0.1489 - acc: 0.954 - ETA: 4s - loss: 0.1447 - acc: 0.958 - ETA: 4s - loss: 0.1308 - acc: 0.963 - ETA: 4s - loss: 0.1182 - acc: 0.965 - ETA: 4s - loss: 0.1290 - acc: 0.956 - ETA: 4s - loss: 0.1240 - acc: 0.957 - ETA: 4s - loss: 0.1180 - acc: 0.960 - ETA: 4s - loss: 0.1267 - acc: 0.953 - ETA: 4s - loss: 0.1334 - acc: 0.951 - ETA: 3s - loss: 0.1314 - acc: 0.952 - ETA: 3s - loss: 0.1274 - acc: 0.954 - ETA: 3s - loss: 0.1303 - acc: 0.954 - ETA: 3s - loss: 0.1306 - acc: 0.953 - ETA: 3s - loss: 0.1290 - acc: 0.953 - ETA: 3s - loss: 0.1303 - acc: 0.953 - ETA: 3s - loss: 0.1263 - acc: 0.955 - ETA: 3s - loss: 0.1259 - acc: 0.955 - ETA: 3s - loss: 0.1244 - acc: 0.955 - ETA: 3s - loss: 0.1314 - acc: 0.951 - ETA: 2s - loss: 0.1327 - acc: 0.950 - ETA: 2s - loss: 0.1307 - acc: 0.950 - ETA: 2s - loss: 0.1302 - acc: 0.951 - ETA: 2s - loss: 0.1291 - acc: 0.951 - ETA: 2s - loss: 0.1287 - acc: 0.951 - ETA: 2s - loss: 0.1292 - acc: 0.951 - ETA: 2s - loss: 0.1274 - acc: 0.952 - ETA: 2s - loss: 0.1272 - acc: 0.952 - ETA: 2s - loss: 0.1280 - acc: 0.951 - ETA: 1s - loss: 0.1295 - acc: 0.950 - ETA: 1s - loss: 0.1290 - acc: 0.950 - ETA: 1s - loss: 0.1308 - acc: 0.950 - ETA: 1s - loss: 0.1309 - acc: 0.951 - ETA: 1s - loss: 0.1299 - acc: 0.951 - ETA: 1s - loss: 0.1294 - acc: 0.952 - ETA: 1s - loss: 0.1278 - acc: 0.953 - ETA: 1s - loss: 0.1261 - acc: 0.953 - ETA: 1s - loss: 0.1268 - acc: 0.953 - ETA: 1s - loss: 0.1280 - acc: 0.953 - ETA: 0s - loss: 0.1260 - acc: 0.954 - ETA: 0s - loss: 0.1256 - acc: 0.954 - ETA: 0s - loss: 0.1268 - acc: 0.953 - ETA: 0s - loss: 0.1283 - acc: 0.953 - ETA: 0s - loss: 0.1287 - acc: 0.953 - ETA: 0s - loss: 0.1297 - acc: 0.952 - ETA: 0s - loss: 0.1295 - acc: 0.952 - ETA: 0s - loss: 0.1300 - acc: 0.952 - ETA: 0s - loss: 0.1305 - acc: 0.952 - 5s 105ms/step - loss: 0.1286 - acc: 0.9537\n",
      "Epoch 114/10000\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.0526 - acc: 0.988 - ETA: 4s - loss: 0.0591 - acc: 0.977 - ETA: 4s - loss: 0.1031 - acc: 0.954 - ETA: 4s - loss: 0.1481 - acc: 0.943 - ETA: 4s - loss: 0.1696 - acc: 0.943 - ETA: 4s - loss: 0.1611 - acc: 0.941 - ETA: 4s - loss: 0.1467 - acc: 0.948 - ETA: 4s - loss: 0.1490 - acc: 0.944 - ETA: 4s - loss: 0.1499 - acc: 0.945 - ETA: 4s - loss: 0.1422 - acc: 0.948 - ETA: 3s - loss: 0.1368 - acc: 0.951 - ETA: 3s - loss: 0.1298 - acc: 0.955 - ETA: 3s - loss: 0.1301 - acc: 0.956 - ETA: 3s - loss: 0.1285 - acc: 0.957 - ETA: 3s - loss: 0.1337 - acc: 0.956 - ETA: 3s - loss: 0.1301 - acc: 0.957 - ETA: 3s - loss: 0.1259 - acc: 0.959 - ETA: 3s - loss: 0.1254 - acc: 0.960 - ETA: 3s - loss: 0.1257 - acc: 0.959 - ETA: 3s - loss: 0.1220 - acc: 0.960 - ETA: 2s - loss: 0.1185 - acc: 0.962 - ETA: 2s - loss: 0.1197 - acc: 0.962 - ETA: 2s - loss: 0.1176 - acc: 0.962 - ETA: 2s - loss: 0.1153 - acc: 0.962 - ETA: 2s - loss: 0.1150 - acc: 0.962 - ETA: 2s - loss: 0.1174 - acc: 0.961 - ETA: 2s - loss: 0.1214 - acc: 0.960 - ETA: 2s - loss: 0.1231 - acc: 0.959 - ETA: 2s - loss: 0.1227 - acc: 0.958 - ETA: 1s - loss: 0.1233 - acc: 0.959 - ETA: 1s - loss: 0.1253 - acc: 0.958 - ETA: 1s - loss: 0.1279 - acc: 0.958 - ETA: 1s - loss: 0.1252 - acc: 0.959 - ETA: 1s - loss: 0.1234 - acc: 0.960 - ETA: 1s - loss: 0.1247 - acc: 0.959 - ETA: 1s - loss: 0.1227 - acc: 0.960 - ETA: 1s - loss: 0.1226 - acc: 0.960 - ETA: 1s - loss: 0.1236 - acc: 0.959 - ETA: 1s - loss: 0.1274 - acc: 0.957 - ETA: 0s - loss: 0.1322 - acc: 0.956 - ETA: 0s - loss: 0.1315 - acc: 0.955 - ETA: 0s - loss: 0.1341 - acc: 0.953 - ETA: 0s - loss: 0.1340 - acc: 0.953 - ETA: 0s - loss: 0.1335 - acc: 0.953 - ETA: 0s - loss: 0.1320 - acc: 0.953 - ETA: 0s - loss: 0.1305 - acc: 0.954 - ETA: 0s - loss: 0.1310 - acc: 0.953 - ETA: 0s - loss: 0.1299 - acc: 0.954 - 5s 103ms/step - loss: 0.1305 - acc: 0.9534\n",
      "Epoch 115/10000\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1312 - acc: 0.954 - ETA: 4s - loss: 0.1210 - acc: 0.960 - ETA: 4s - loss: 0.1351 - acc: 0.954 - ETA: 4s - loss: 0.1315 - acc: 0.951 - ETA: 4s - loss: 0.1307 - acc: 0.952 - ETA: 4s - loss: 0.1275 - acc: 0.952 - ETA: 4s - loss: 0.1278 - acc: 0.949 - ETA: 4s - loss: 0.1437 - acc: 0.946 - ETA: 4s - loss: 0.1452 - acc: 0.943 - ETA: 4s - loss: 0.1449 - acc: 0.942 - ETA: 3s - loss: 0.1454 - acc: 0.941 - ETA: 3s - loss: 0.1455 - acc: 0.941 - ETA: 3s - loss: 0.1453 - acc: 0.943 - ETA: 3s - loss: 0.1419 - acc: 0.944 - ETA: 3s - loss: 0.1441 - acc: 0.944 - ETA: 3s - loss: 0.1447 - acc: 0.945 - ETA: 3s - loss: 0.1423 - acc: 0.946 - ETA: 3s - loss: 0.1419 - acc: 0.947 - ETA: 3s - loss: 0.1509 - acc: 0.944 - ETA: 3s - loss: 0.1498 - acc: 0.945 - ETA: 2s - loss: 0.1474 - acc: 0.945 - ETA: 2s - loss: 0.1454 - acc: 0.945 - ETA: 2s - loss: 0.1417 - acc: 0.947 - ETA: 2s - loss: 0.1442 - acc: 0.946 - ETA: 2s - loss: 0.1473 - acc: 0.945 - ETA: 2s - loss: 0.1449 - acc: 0.946 - ETA: 2s - loss: 0.1458 - acc: 0.946 - ETA: 2s - loss: 0.1473 - acc: 0.946 - ETA: 2s - loss: 0.1462 - acc: 0.946 - ETA: 1s - loss: 0.1490 - acc: 0.945 - ETA: 1s - loss: 0.1500 - acc: 0.944 - ETA: 1s - loss: 0.1488 - acc: 0.944 - ETA: 1s - loss: 0.1499 - acc: 0.944 - ETA: 1s - loss: 0.1488 - acc: 0.944 - ETA: 1s - loss: 0.1463 - acc: 0.946 - ETA: 1s - loss: 0.1447 - acc: 0.946 - ETA: 1s - loss: 0.1421 - acc: 0.948 - ETA: 1s - loss: 0.1413 - acc: 0.948 - ETA: 1s - loss: 0.1410 - acc: 0.949 - ETA: 0s - loss: 0.1412 - acc: 0.948 - ETA: 0s - loss: 0.1391 - acc: 0.949 - ETA: 0s - loss: 0.1372 - acc: 0.950 - ETA: 0s - loss: 0.1379 - acc: 0.950 - ETA: 0s - loss: 0.1374 - acc: 0.950 - ETA: 0s - loss: 0.1374 - acc: 0.950 - ETA: 0s - loss: 0.1352 - acc: 0.951 - ETA: 0s - loss: 0.1345 - acc: 0.951 - ETA: 0s - loss: 0.1346 - acc: 0.951 - 5s 104ms/step - loss: 0.1340 - acc: 0.9517\n",
      "Epoch 116/10000\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0650 - acc: 0.977 - ETA: 4s - loss: 0.0943 - acc: 0.971 - ETA: 4s - loss: 0.0838 - acc: 0.973 - ETA: 4s - loss: 0.0922 - acc: 0.971 - ETA: 4s - loss: 0.1091 - acc: 0.959 - ETA: 4s - loss: 0.1138 - acc: 0.958 - ETA: 4s - loss: 0.1155 - acc: 0.956 - ETA: 4s - loss: 0.1241 - acc: 0.951 - ETA: 4s - loss: 0.1211 - acc: 0.953 - ETA: 4s - loss: 0.1255 - acc: 0.952 - ETA: 4s - loss: 0.1213 - acc: 0.954 - ETA: 3s - loss: 0.1275 - acc: 0.952 - ETA: 3s - loss: 0.1314 - acc: 0.952 - ETA: 3s - loss: 0.1302 - acc: 0.952 - ETA: 3s - loss: 0.1361 - acc: 0.950 - ETA: 3s - loss: 0.1388 - acc: 0.948 - ETA: 3s - loss: 0.1386 - acc: 0.946 - ETA: 3s - loss: 0.1355 - acc: 0.948 - ETA: 3s - loss: 0.1315 - acc: 0.950 - ETA: 3s - loss: 0.1284 - acc: 0.952 - ETA: 2s - loss: 0.1264 - acc: 0.953 - ETA: 2s - loss: 0.1230 - acc: 0.955 - ETA: 2s - loss: 0.1238 - acc: 0.954 - ETA: 2s - loss: 0.1294 - acc: 0.952 - ETA: 2s - loss: 0.1292 - acc: 0.951 - ETA: 2s - loss: 0.1255 - acc: 0.953 - ETA: 2s - loss: 0.1275 - acc: 0.953 - ETA: 2s - loss: 0.1305 - acc: 0.951 - ETA: 2s - loss: 0.1307 - acc: 0.951 - ETA: 1s - loss: 0.1307 - acc: 0.951 - ETA: 1s - loss: 0.1309 - acc: 0.951 - ETA: 1s - loss: 0.1343 - acc: 0.951 - ETA: 1s - loss: 0.1343 - acc: 0.951 - ETA: 1s - loss: 0.1319 - acc: 0.952 - ETA: 1s - loss: 0.1311 - acc: 0.952 - ETA: 1s - loss: 0.1296 - acc: 0.953 - ETA: 1s - loss: 0.1275 - acc: 0.954 - ETA: 1s - loss: 0.1308 - acc: 0.953 - ETA: 1s - loss: 0.1335 - acc: 0.952 - ETA: 0s - loss: 0.1329 - acc: 0.953 - ETA: 0s - loss: 0.1330 - acc: 0.952 - ETA: 0s - loss: 0.1335 - acc: 0.952 - ETA: 0s - loss: 0.1313 - acc: 0.953 - ETA: 0s - loss: 0.1306 - acc: 0.953 - ETA: 0s - loss: 0.1297 - acc: 0.953 - ETA: 0s - loss: 0.1296 - acc: 0.953 - ETA: 0s - loss: 0.1292 - acc: 0.954 - ETA: 0s - loss: 0.1297 - acc: 0.954 - 5s 104ms/step - loss: 0.1303 - acc: 0.9536\n",
      "Epoch 117/10000\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1553 - acc: 0.954 - ETA: 5s - loss: 0.1442 - acc: 0.954 - ETA: 4s - loss: 0.1739 - acc: 0.943 - ETA: 4s - loss: 0.1423 - acc: 0.957 - ETA: 4s - loss: 0.1495 - acc: 0.950 - ETA: 4s - loss: 0.1644 - acc: 0.947 - ETA: 4s - loss: 0.1531 - acc: 0.951 - ETA: 4s - loss: 0.1520 - acc: 0.950 - ETA: 4s - loss: 0.1525 - acc: 0.948 - ETA: 4s - loss: 0.1494 - acc: 0.947 - ETA: 4s - loss: 0.1448 - acc: 0.949 - ETA: 3s - loss: 0.1396 - acc: 0.951 - ETA: 3s - loss: 0.1364 - acc: 0.951 - ETA: 3s - loss: 0.1388 - acc: 0.949 - ETA: 3s - loss: 0.1425 - acc: 0.947 - ETA: 3s - loss: 0.1422 - acc: 0.948 - ETA: 3s - loss: 0.1417 - acc: 0.948 - ETA: 3s - loss: 0.1397 - acc: 0.949 - ETA: 3s - loss: 0.1390 - acc: 0.948 - ETA: 3s - loss: 0.1389 - acc: 0.948 - ETA: 2s - loss: 0.1385 - acc: 0.947 - ETA: 2s - loss: 0.1355 - acc: 0.948 - ETA: 2s - loss: 0.1363 - acc: 0.948 - ETA: 2s - loss: 0.1343 - acc: 0.949 - ETA: 2s - loss: 0.1301 - acc: 0.951 - ETA: 2s - loss: 0.1352 - acc: 0.949 - ETA: 2s - loss: 0.1322 - acc: 0.950 - ETA: 2s - loss: 0.1298 - acc: 0.952 - ETA: 2s - loss: 0.1303 - acc: 0.951 - ETA: 2s - loss: 0.1292 - acc: 0.952 - ETA: 1s - loss: 0.1283 - acc: 0.953 - ETA: 1s - loss: 0.1262 - acc: 0.954 - ETA: 1s - loss: 0.1260 - acc: 0.954 - ETA: 1s - loss: 0.1280 - acc: 0.954 - ETA: 1s - loss: 0.1284 - acc: 0.954 - ETA: 1s - loss: 0.1276 - acc: 0.954 - ETA: 1s - loss: 0.1288 - acc: 0.953 - ETA: 1s - loss: 0.1340 - acc: 0.951 - ETA: 1s - loss: 0.1347 - acc: 0.950 - ETA: 0s - loss: 0.1333 - acc: 0.951 - ETA: 0s - loss: 0.1330 - acc: 0.951 - ETA: 0s - loss: 0.1354 - acc: 0.950 - ETA: 0s - loss: 0.1351 - acc: 0.950 - ETA: 0s - loss: 0.1352 - acc: 0.950 - ETA: 0s - loss: 0.1340 - acc: 0.951 - ETA: 0s - loss: 0.1345 - acc: 0.950 - ETA: 0s - loss: 0.1354 - acc: 0.950 - ETA: 0s - loss: 0.1367 - acc: 0.950 - 5s 105ms/step - loss: 0.1378 - acc: 0.9504\n",
      "Epoch 118/10000\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1109 - acc: 0.954 - ETA: 5s - loss: 0.1291 - acc: 0.954 - ETA: 5s - loss: 0.1454 - acc: 0.935 - ETA: 5s - loss: 0.1728 - acc: 0.931 - ETA: 5s - loss: 0.1665 - acc: 0.934 - ETA: 4s - loss: 0.1651 - acc: 0.933 - ETA: 4s - loss: 0.1552 - acc: 0.939 - ETA: 4s - loss: 0.1538 - acc: 0.941 - ETA: 4s - loss: 0.1550 - acc: 0.941 - ETA: 4s - loss: 0.1560 - acc: 0.939 - ETA: 4s - loss: 0.1531 - acc: 0.940 - ETA: 3s - loss: 0.1497 - acc: 0.944 - ETA: 3s - loss: 0.1420 - acc: 0.947 - ETA: 3s - loss: 0.1356 - acc: 0.951 - ETA: 3s - loss: 0.1334 - acc: 0.952 - ETA: 3s - loss: 0.1320 - acc: 0.953 - ETA: 3s - loss: 0.1304 - acc: 0.955 - ETA: 3s - loss: 0.1388 - acc: 0.953 - ETA: 3s - loss: 0.1394 - acc: 0.952 - ETA: 3s - loss: 0.1393 - acc: 0.953 - ETA: 2s - loss: 0.1434 - acc: 0.951 - ETA: 2s - loss: 0.1399 - acc: 0.953 - ETA: 2s - loss: 0.1380 - acc: 0.954 - ETA: 2s - loss: 0.1370 - acc: 0.955 - ETA: 2s - loss: 0.1359 - acc: 0.955 - ETA: 2s - loss: 0.1336 - acc: 0.955 - ETA: 2s - loss: 0.1325 - acc: 0.955 - ETA: 2s - loss: 0.1324 - acc: 0.955 - ETA: 2s - loss: 0.1295 - acc: 0.956 - ETA: 2s - loss: 0.1285 - acc: 0.957 - ETA: 1s - loss: 0.1285 - acc: 0.956 - ETA: 1s - loss: 0.1273 - acc: 0.956 - ETA: 1s - loss: 0.1276 - acc: 0.956 - ETA: 1s - loss: 0.1304 - acc: 0.955 - ETA: 1s - loss: 0.1290 - acc: 0.955 - ETA: 1s - loss: 0.1311 - acc: 0.954 - ETA: 1s - loss: 0.1305 - acc: 0.954 - ETA: 1s - loss: 0.1321 - acc: 0.953 - ETA: 1s - loss: 0.1311 - acc: 0.954 - ETA: 0s - loss: 0.1318 - acc: 0.953 - ETA: 0s - loss: 0.1324 - acc: 0.953 - ETA: 0s - loss: 0.1310 - acc: 0.954 - ETA: 0s - loss: 0.1310 - acc: 0.954 - ETA: 0s - loss: 0.1306 - acc: 0.953 - ETA: 0s - loss: 0.1302 - acc: 0.954 - ETA: 0s - loss: 0.1296 - acc: 0.954 - ETA: 0s - loss: 0.1311 - acc: 0.954 - ETA: 0s - loss: 0.1315 - acc: 0.953 - 5s 105ms/step - loss: 0.1322 - acc: 0.9534\n",
      "Epoch 119/10000\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1035 - acc: 0.954 - ETA: 5s - loss: 0.0910 - acc: 0.965 - ETA: 4s - loss: 0.1005 - acc: 0.969 - ETA: 4s - loss: 0.1250 - acc: 0.960 - ETA: 4s - loss: 0.1181 - acc: 0.965 - ETA: 4s - loss: 0.1109 - acc: 0.967 - ETA: 4s - loss: 0.1311 - acc: 0.954 - ETA: 4s - loss: 0.1248 - acc: 0.956 - ETA: 4s - loss: 0.1254 - acc: 0.959 - ETA: 4s - loss: 0.1278 - acc: 0.956 - ETA: 3s - loss: 0.1394 - acc: 0.952 - ETA: 3s - loss: 0.1409 - acc: 0.951 - ETA: 3s - loss: 0.1398 - acc: 0.953 - ETA: 3s - loss: 0.1368 - acc: 0.955 - ETA: 3s - loss: 0.1342 - acc: 0.955 - ETA: 3s - loss: 0.1288 - acc: 0.957 - ETA: 3s - loss: 0.1309 - acc: 0.955 - ETA: 3s - loss: 0.1256 - acc: 0.958 - ETA: 3s - loss: 0.1225 - acc: 0.958 - ETA: 3s - loss: 0.1220 - acc: 0.959 - ETA: 2s - loss: 0.1205 - acc: 0.960 - ETA: 2s - loss: 0.1200 - acc: 0.960 - ETA: 2s - loss: 0.1212 - acc: 0.960 - ETA: 2s - loss: 0.1229 - acc: 0.957 - ETA: 2s - loss: 0.1221 - acc: 0.958 - ETA: 2s - loss: 0.1210 - acc: 0.958 - ETA: 2s - loss: 0.1205 - acc: 0.958 - ETA: 2s - loss: 0.1200 - acc: 0.958 - ETA: 2s - loss: 0.1297 - acc: 0.956 - ETA: 2s - loss: 0.1302 - acc: 0.955 - ETA: 1s - loss: 0.1284 - acc: 0.956 - ETA: 1s - loss: 0.1258 - acc: 0.957 - ETA: 1s - loss: 0.1261 - acc: 0.957 - ETA: 1s - loss: 0.1235 - acc: 0.958 - ETA: 1s - loss: 0.1230 - acc: 0.958 - ETA: 1s - loss: 0.1272 - acc: 0.957 - ETA: 1s - loss: 0.1257 - acc: 0.957 - ETA: 1s - loss: 0.1230 - acc: 0.958 - ETA: 1s - loss: 0.1251 - acc: 0.957 - ETA: 0s - loss: 0.1257 - acc: 0.957 - ETA: 0s - loss: 0.1277 - acc: 0.954 - ETA: 0s - loss: 0.1288 - acc: 0.954 - ETA: 0s - loss: 0.1277 - acc: 0.955 - ETA: 0s - loss: 0.1270 - acc: 0.955 - ETA: 0s - loss: 0.1283 - acc: 0.955 - ETA: 0s - loss: 0.1273 - acc: 0.956 - ETA: 0s - loss: 0.1263 - acc: 0.956 - ETA: 0s - loss: 0.1274 - acc: 0.956 - 5s 106ms/step - loss: 0.1277 - acc: 0.9559\n",
      "Epoch 120/10000\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0002621440216898918.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0472 - acc: 1.000 - ETA: 5s - loss: 0.1196 - acc: 0.965 - ETA: 5s - loss: 0.1152 - acc: 0.958 - ETA: 4s - loss: 0.1062 - acc: 0.965 - ETA: 4s - loss: 0.1126 - acc: 0.965 - ETA: 4s - loss: 0.1192 - acc: 0.960 - ETA: 4s - loss: 0.1166 - acc: 0.959 - ETA: 4s - loss: 0.1170 - acc: 0.958 - ETA: 4s - loss: 0.1179 - acc: 0.960 - ETA: 4s - loss: 0.1209 - acc: 0.958 - ETA: 4s - loss: 0.1220 - acc: 0.956 - ETA: 3s - loss: 0.1239 - acc: 0.955 - ETA: 3s - loss: 0.1333 - acc: 0.951 - ETA: 3s - loss: 0.1371 - acc: 0.952 - ETA: 3s - loss: 0.1337 - acc: 0.953 - ETA: 3s - loss: 0.1275 - acc: 0.956 - ETA: 3s - loss: 0.1367 - acc: 0.953 - ETA: 3s - loss: 0.1381 - acc: 0.952 - ETA: 3s - loss: 0.1432 - acc: 0.951 - ETA: 3s - loss: 0.1411 - acc: 0.951 - ETA: 2s - loss: 0.1410 - acc: 0.950 - ETA: 2s - loss: 0.1414 - acc: 0.950 - ETA: 2s - loss: 0.1383 - acc: 0.952 - ETA: 2s - loss: 0.1358 - acc: 0.952 - ETA: 2s - loss: 0.1343 - acc: 0.953 - ETA: 2s - loss: 0.1384 - acc: 0.951 - ETA: 2s - loss: 0.1403 - acc: 0.951 - ETA: 2s - loss: 0.1375 - acc: 0.952 - ETA: 2s - loss: 0.1355 - acc: 0.953 - ETA: 2s - loss: 0.1336 - acc: 0.953 - ETA: 1s - loss: 0.1336 - acc: 0.953 - ETA: 1s - loss: 0.1333 - acc: 0.954 - ETA: 1s - loss: 0.1331 - acc: 0.954 - ETA: 1s - loss: 0.1313 - acc: 0.955 - ETA: 1s - loss: 0.1298 - acc: 0.955 - ETA: 1s - loss: 0.1283 - acc: 0.955 - ETA: 1s - loss: 0.1265 - acc: 0.956 - ETA: 1s - loss: 0.1294 - acc: 0.956 - ETA: 1s - loss: 0.1285 - acc: 0.956 - ETA: 0s - loss: 0.1283 - acc: 0.956 - ETA: 0s - loss: 0.1278 - acc: 0.957 - ETA: 0s - loss: 0.1271 - acc: 0.957 - ETA: 0s - loss: 0.1279 - acc: 0.956 - ETA: 0s - loss: 0.1270 - acc: 0.956 - ETA: 0s - loss: 0.1291 - acc: 0.955 - ETA: 0s - loss: 0.1294 - acc: 0.954 - ETA: 0s - loss: 0.1287 - acc: 0.954 - ETA: 0s - loss: 0.1324 - acc: 0.954 - 5s 105ms/step - loss: 0.1340 - acc: 0.9539\n",
      "Epoch 121/10000\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 6s - loss: 0.1296 - acc: 0.954 - ETA: 5s - loss: 0.1319 - acc: 0.948 - ETA: 5s - loss: 0.1545 - acc: 0.939 - ETA: 5s - loss: 0.1501 - acc: 0.943 - ETA: 4s - loss: 0.1637 - acc: 0.936 - ETA: 4s - loss: 0.1503 - acc: 0.939 - ETA: 4s - loss: 0.1484 - acc: 0.941 - ETA: 4s - loss: 0.1397 - acc: 0.944 - ETA: 4s - loss: 0.1301 - acc: 0.950 - ETA: 4s - loss: 0.1278 - acc: 0.952 - ETA: 4s - loss: 0.1357 - acc: 0.949 - ETA: 3s - loss: 0.1420 - acc: 0.946 - ETA: 3s - loss: 0.1414 - acc: 0.944 - ETA: 3s - loss: 0.1389 - acc: 0.944 - ETA: 3s - loss: 0.1321 - acc: 0.948 - ETA: 3s - loss: 0.1292 - acc: 0.949 - ETA: 3s - loss: 0.1391 - acc: 0.946 - ETA: 3s - loss: 0.1383 - acc: 0.948 - ETA: 3s - loss: 0.1359 - acc: 0.949 - ETA: 3s - loss: 0.1339 - acc: 0.950 - ETA: 2s - loss: 0.1298 - acc: 0.952 - ETA: 2s - loss: 0.1292 - acc: 0.953 - ETA: 2s - loss: 0.1323 - acc: 0.951 - ETA: 2s - loss: 0.1342 - acc: 0.949 - ETA: 2s - loss: 0.1326 - acc: 0.950 - ETA: 2s - loss: 0.1310 - acc: 0.951 - ETA: 2s - loss: 0.1315 - acc: 0.952 - ETA: 2s - loss: 0.1282 - acc: 0.953 - ETA: 2s - loss: 0.1310 - acc: 0.952 - ETA: 2s - loss: 0.1309 - acc: 0.951 - ETA: 1s - loss: 0.1307 - acc: 0.952 - ETA: 1s - loss: 0.1311 - acc: 0.951 - ETA: 1s - loss: 0.1318 - acc: 0.951 - ETA: 1s - loss: 0.1317 - acc: 0.951 - ETA: 1s - loss: 0.1302 - acc: 0.952 - ETA: 1s - loss: 0.1298 - acc: 0.953 - ETA: 1s - loss: 0.1276 - acc: 0.954 - ETA: 1s - loss: 0.1282 - acc: 0.954 - ETA: 1s - loss: 0.1312 - acc: 0.953 - ETA: 0s - loss: 0.1293 - acc: 0.953 - ETA: 0s - loss: 0.1292 - acc: 0.953 - ETA: 0s - loss: 0.1301 - acc: 0.953 - ETA: 0s - loss: 0.1282 - acc: 0.954 - ETA: 0s - loss: 0.1274 - acc: 0.954 - ETA: 0s - loss: 0.1301 - acc: 0.953 - ETA: 0s - loss: 0.1289 - acc: 0.953 - ETA: 0s - loss: 0.1299 - acc: 0.952 - ETA: 0s - loss: 0.1318 - acc: 0.952 - 5s 106ms/step - loss: 0.1353 - acc: 0.9506\n",
      "Epoch 122/10000\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 5s - loss: 0.1361 - acc: 0.943 - ETA: 5s - loss: 0.1653 - acc: 0.937 - ETA: 4s - loss: 0.1365 - acc: 0.947 - ETA: 4s - loss: 0.1475 - acc: 0.946 - ETA: 4s - loss: 0.1398 - acc: 0.952 - ETA: 4s - loss: 0.1452 - acc: 0.948 - ETA: 4s - loss: 0.1429 - acc: 0.948 - ETA: 4s - loss: 0.1447 - acc: 0.946 - ETA: 4s - loss: 0.1372 - acc: 0.950 - ETA: 4s - loss: 0.1399 - acc: 0.948 - ETA: 4s - loss: 0.1373 - acc: 0.951 - ETA: 3s - loss: 0.1332 - acc: 0.952 - ETA: 3s - loss: 0.1300 - acc: 0.954 - ETA: 3s - loss: 0.1370 - acc: 0.952 - ETA: 3s - loss: 0.1398 - acc: 0.949 - ETA: 3s - loss: 0.1368 - acc: 0.950 - ETA: 3s - loss: 0.1336 - acc: 0.951 - ETA: 3s - loss: 0.1306 - acc: 0.951 - ETA: 3s - loss: 0.1318 - acc: 0.952 - ETA: 3s - loss: 0.1289 - acc: 0.953 - ETA: 2s - loss: 0.1262 - acc: 0.954 - ETA: 2s - loss: 0.1295 - acc: 0.952 - ETA: 2s - loss: 0.1311 - acc: 0.951 - ETA: 2s - loss: 0.1314 - acc: 0.951 - ETA: 2s - loss: 0.1304 - acc: 0.951 - ETA: 2s - loss: 0.1305 - acc: 0.950 - ETA: 2s - loss: 0.1271 - acc: 0.952 - ETA: 2s - loss: 0.1246 - acc: 0.954 - ETA: 2s - loss: 0.1231 - acc: 0.955 - ETA: 1s - loss: 0.1235 - acc: 0.955 - ETA: 1s - loss: 0.1214 - acc: 0.956 - ETA: 1s - loss: 0.1196 - acc: 0.957 - ETA: 1s - loss: 0.1203 - acc: 0.957 - ETA: 1s - loss: 0.1226 - acc: 0.955 - ETA: 1s - loss: 0.1246 - acc: 0.954 - ETA: 1s - loss: 0.1251 - acc: 0.953 - ETA: 1s - loss: 0.1239 - acc: 0.954 - ETA: 1s - loss: 0.1236 - acc: 0.954 - ETA: 1s - loss: 0.1228 - acc: 0.955 - ETA: 0s - loss: 0.1205 - acc: 0.956 - ETA: 0s - loss: 0.1199 - acc: 0.956 - ETA: 0s - loss: 0.1197 - acc: 0.956 - ETA: 0s - loss: 0.1206 - acc: 0.955 - ETA: 0s - loss: 0.1212 - acc: 0.955 - ETA: 0s - loss: 0.1214 - acc: 0.956 - ETA: 0s - loss: 0.1235 - acc: 0.955 - ETA: 0s - loss: 0.1223 - acc: 0.956 - ETA: 0s - loss: 0.1213 - acc: 0.956 - 5s 105ms/step - loss: 0.1206 - acc: 0.9565\n",
      "Epoch 123/10000\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1709 - acc: 0.965 - ETA: 4s - loss: 0.1112 - acc: 0.977 - ETA: 4s - loss: 0.0918 - acc: 0.981 - ETA: 4s - loss: 0.0923 - acc: 0.980 - ETA: 4s - loss: 0.0962 - acc: 0.977 - ETA: 4s - loss: 0.1273 - acc: 0.960 - ETA: 4s - loss: 0.1326 - acc: 0.957 - ETA: 4s - loss: 0.1330 - acc: 0.957 - ETA: 4s - loss: 0.1465 - acc: 0.953 - ETA: 4s - loss: 0.1426 - acc: 0.955 - ETA: 3s - loss: 0.1414 - acc: 0.955 - ETA: 3s - loss: 0.1389 - acc: 0.957 - ETA: 3s - loss: 0.1368 - acc: 0.958 - ETA: 3s - loss: 0.1381 - acc: 0.954 - ETA: 3s - loss: 0.1416 - acc: 0.952 - ETA: 3s - loss: 0.1467 - acc: 0.951 - ETA: 3s - loss: 0.1401 - acc: 0.953 - ETA: 3s - loss: 0.1366 - acc: 0.955 - ETA: 3s - loss: 0.1352 - acc: 0.956 - ETA: 3s - loss: 0.1324 - acc: 0.958 - ETA: 2s - loss: 0.1339 - acc: 0.957 - ETA: 2s - loss: 0.1420 - acc: 0.953 - ETA: 2s - loss: 0.1416 - acc: 0.953 - ETA: 2s - loss: 0.1386 - acc: 0.954 - ETA: 2s - loss: 0.1369 - acc: 0.955 - ETA: 2s - loss: 0.1388 - acc: 0.955 - ETA: 2s - loss: 0.1377 - acc: 0.955 - ETA: 2s - loss: 0.1372 - acc: 0.954 - ETA: 2s - loss: 0.1336 - acc: 0.956 - ETA: 1s - loss: 0.1368 - acc: 0.954 - ETA: 1s - loss: 0.1370 - acc: 0.954 - ETA: 1s - loss: 0.1383 - acc: 0.953 - ETA: 1s - loss: 0.1393 - acc: 0.952 - ETA: 1s - loss: 0.1380 - acc: 0.952 - ETA: 1s - loss: 0.1355 - acc: 0.953 - ETA: 1s - loss: 0.1348 - acc: 0.953 - ETA: 1s - loss: 0.1339 - acc: 0.953 - ETA: 1s - loss: 0.1327 - acc: 0.953 - ETA: 1s - loss: 0.1322 - acc: 0.954 - ETA: 0s - loss: 0.1306 - acc: 0.955 - ETA: 0s - loss: 0.1324 - acc: 0.954 - ETA: 0s - loss: 0.1327 - acc: 0.953 - ETA: 0s - loss: 0.1329 - acc: 0.953 - ETA: 0s - loss: 0.1318 - acc: 0.954 - ETA: 0s - loss: 0.1323 - acc: 0.954 - ETA: 0s - loss: 0.1343 - acc: 0.952 - ETA: 0s - loss: 0.1334 - acc: 0.953 - ETA: 0s - loss: 0.1357 - acc: 0.952 - 5s 104ms/step - loss: 0.1376 - acc: 0.9517\n",
      "Epoch 124/10000\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0776 - acc: 0.977 - ETA: 4s - loss: 0.0911 - acc: 0.971 - ETA: 4s - loss: 0.1493 - acc: 0.943 - ETA: 4s - loss: 0.1445 - acc: 0.943 - ETA: 4s - loss: 0.1463 - acc: 0.947 - ETA: 4s - loss: 0.1375 - acc: 0.948 - ETA: 4s - loss: 0.1261 - acc: 0.956 - ETA: 4s - loss: 0.1318 - acc: 0.956 - ETA: 4s - loss: 0.1293 - acc: 0.955 - ETA: 4s - loss: 0.1324 - acc: 0.951 - ETA: 4s - loss: 0.1344 - acc: 0.948 - ETA: 3s - loss: 0.1371 - acc: 0.946 - ETA: 3s - loss: 0.1321 - acc: 0.948 - ETA: 3s - loss: 0.1300 - acc: 0.948 - ETA: 3s - loss: 0.1323 - acc: 0.948 - ETA: 3s - loss: 0.1314 - acc: 0.948 - ETA: 3s - loss: 0.1280 - acc: 0.951 - ETA: 3s - loss: 0.1248 - acc: 0.952 - ETA: 3s - loss: 0.1226 - acc: 0.954 - ETA: 3s - loss: 0.1224 - acc: 0.954 - ETA: 2s - loss: 0.1236 - acc: 0.955 - ETA: 2s - loss: 0.1260 - acc: 0.953 - ETA: 2s - loss: 0.1229 - acc: 0.955 - ETA: 2s - loss: 0.1234 - acc: 0.955 - ETA: 2s - loss: 0.1260 - acc: 0.953 - ETA: 2s - loss: 0.1252 - acc: 0.953 - ETA: 2s - loss: 0.1237 - acc: 0.954 - ETA: 2s - loss: 0.1234 - acc: 0.954 - ETA: 2s - loss: 0.1245 - acc: 0.954 - ETA: 1s - loss: 0.1305 - acc: 0.952 - ETA: 1s - loss: 0.1305 - acc: 0.952 - ETA: 1s - loss: 0.1313 - acc: 0.952 - ETA: 1s - loss: 0.1348 - acc: 0.951 - ETA: 1s - loss: 0.1333 - acc: 0.952 - ETA: 1s - loss: 0.1321 - acc: 0.952 - ETA: 1s - loss: 0.1296 - acc: 0.954 - ETA: 1s - loss: 0.1289 - acc: 0.954 - ETA: 1s - loss: 0.1303 - acc: 0.953 - ETA: 1s - loss: 0.1308 - acc: 0.953 - ETA: 0s - loss: 0.1299 - acc: 0.953 - ETA: 0s - loss: 0.1299 - acc: 0.953 - ETA: 0s - loss: 0.1291 - acc: 0.954 - ETA: 0s - loss: 0.1282 - acc: 0.954 - ETA: 0s - loss: 0.1302 - acc: 0.954 - ETA: 0s - loss: 0.1344 - acc: 0.952 - ETA: 0s - loss: 0.1328 - acc: 0.953 - ETA: 0s - loss: 0.1313 - acc: 0.954 - ETA: 0s - loss: 0.1326 - acc: 0.953 - 5s 105ms/step - loss: 0.1342 - acc: 0.9528\n",
      "Epoch 125/10000\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.0580 - acc: 1.000 - ETA: 4s - loss: 0.0961 - acc: 0.977 - ETA: 4s - loss: 0.1097 - acc: 0.954 - ETA: 4s - loss: 0.1103 - acc: 0.954 - ETA: 4s - loss: 0.1211 - acc: 0.952 - ETA: 4s - loss: 0.1270 - acc: 0.947 - ETA: 4s - loss: 0.1369 - acc: 0.944 - ETA: 4s - loss: 0.1445 - acc: 0.943 - ETA: 4s - loss: 0.1331 - acc: 0.949 - ETA: 4s - loss: 0.1279 - acc: 0.953 - ETA: 4s - loss: 0.1229 - acc: 0.954 - ETA: 3s - loss: 0.1231 - acc: 0.952 - ETA: 3s - loss: 0.1178 - acc: 0.955 - ETA: 3s - loss: 0.1163 - acc: 0.956 - ETA: 3s - loss: 0.1204 - acc: 0.953 - ETA: 3s - loss: 0.1211 - acc: 0.953 - ETA: 3s - loss: 0.1210 - acc: 0.953 - ETA: 3s - loss: 0.1216 - acc: 0.953 - ETA: 3s - loss: 0.1210 - acc: 0.953 - ETA: 3s - loss: 0.1227 - acc: 0.954 - ETA: 2s - loss: 0.1241 - acc: 0.954 - ETA: 2s - loss: 0.1228 - acc: 0.954 - ETA: 2s - loss: 0.1224 - acc: 0.955 - ETA: 2s - loss: 0.1229 - acc: 0.955 - ETA: 2s - loss: 0.1218 - acc: 0.955 - ETA: 2s - loss: 0.1209 - acc: 0.956 - ETA: 2s - loss: 0.1193 - acc: 0.957 - ETA: 2s - loss: 0.1181 - acc: 0.958 - ETA: 2s - loss: 0.1199 - acc: 0.958 - ETA: 2s - loss: 0.1235 - acc: 0.957 - ETA: 1s - loss: 0.1231 - acc: 0.957 - ETA: 1s - loss: 0.1239 - acc: 0.956 - ETA: 1s - loss: 0.1254 - acc: 0.955 - ETA: 1s - loss: 0.1241 - acc: 0.956 - ETA: 1s - loss: 0.1235 - acc: 0.956 - ETA: 1s - loss: 0.1244 - acc: 0.955 - ETA: 1s - loss: 0.1225 - acc: 0.956 - ETA: 1s - loss: 0.1214 - acc: 0.956 - ETA: 1s - loss: 0.1242 - acc: 0.955 - ETA: 0s - loss: 0.1241 - acc: 0.956 - ETA: 0s - loss: 0.1236 - acc: 0.956 - ETA: 0s - loss: 0.1230 - acc: 0.956 - ETA: 0s - loss: 0.1225 - acc: 0.956 - ETA: 0s - loss: 0.1223 - acc: 0.957 - ETA: 0s - loss: 0.1236 - acc: 0.956 - ETA: 0s - loss: 0.1257 - acc: 0.955 - ETA: 0s - loss: 0.1257 - acc: 0.955 - ETA: 0s - loss: 0.1248 - acc: 0.956 - 5s 106ms/step - loss: 0.1248 - acc: 0.9562\n",
      "Epoch 126/10000\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.0492 - acc: 1.000 - ETA: 4s - loss: 0.0845 - acc: 0.977 - ETA: 4s - loss: 0.1104 - acc: 0.965 - ETA: 4s - loss: 0.1088 - acc: 0.960 - ETA: 4s - loss: 0.1031 - acc: 0.961 - ETA: 4s - loss: 0.1154 - acc: 0.958 - ETA: 4s - loss: 0.1176 - acc: 0.959 - ETA: 4s - loss: 0.1230 - acc: 0.957 - ETA: 4s - loss: 0.1262 - acc: 0.956 - ETA: 4s - loss: 0.1346 - acc: 0.952 - ETA: 3s - loss: 0.1289 - acc: 0.955 - ETA: 3s - loss: 0.1343 - acc: 0.951 - ETA: 3s - loss: 0.1351 - acc: 0.952 - ETA: 3s - loss: 0.1349 - acc: 0.951 - ETA: 3s - loss: 0.1381 - acc: 0.951 - ETA: 3s - loss: 0.1417 - acc: 0.950 - ETA: 3s - loss: 0.1389 - acc: 0.952 - ETA: 3s - loss: 0.1393 - acc: 0.951 - ETA: 3s - loss: 0.1394 - acc: 0.951 - ETA: 2s - loss: 0.1345 - acc: 0.953 - ETA: 2s - loss: 0.1337 - acc: 0.953 - ETA: 2s - loss: 0.1381 - acc: 0.952 - ETA: 2s - loss: 0.1364 - acc: 0.952 - ETA: 2s - loss: 0.1330 - acc: 0.953 - ETA: 2s - loss: 0.1362 - acc: 0.951 - ETA: 2s - loss: 0.1366 - acc: 0.951 - ETA: 2s - loss: 0.1408 - acc: 0.950 - ETA: 2s - loss: 0.1394 - acc: 0.950 - ETA: 2s - loss: 0.1390 - acc: 0.950 - ETA: 1s - loss: 0.1358 - acc: 0.952 - ETA: 1s - loss: 0.1369 - acc: 0.951 - ETA: 1s - loss: 0.1354 - acc: 0.951 - ETA: 1s - loss: 0.1365 - acc: 0.951 - ETA: 1s - loss: 0.1345 - acc: 0.952 - ETA: 1s - loss: 0.1356 - acc: 0.952 - ETA: 1s - loss: 0.1373 - acc: 0.951 - ETA: 1s - loss: 0.1356 - acc: 0.952 - ETA: 1s - loss: 0.1349 - acc: 0.952 - ETA: 1s - loss: 0.1345 - acc: 0.952 - ETA: 0s - loss: 0.1358 - acc: 0.952 - ETA: 0s - loss: 0.1341 - acc: 0.953 - ETA: 0s - loss: 0.1341 - acc: 0.953 - ETA: 0s - loss: 0.1364 - acc: 0.952 - ETA: 0s - loss: 0.1386 - acc: 0.951 - ETA: 0s - loss: 0.1379 - acc: 0.952 - ETA: 0s - loss: 0.1370 - acc: 0.952 - ETA: 0s - loss: 0.1353 - acc: 0.953 - ETA: 0s - loss: 0.1343 - acc: 0.954 - 5s 105ms/step - loss: 0.1340 - acc: 0.9544\n",
      "Epoch 127/10000\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 5s - loss: 0.1063 - acc: 0.977 - ETA: 4s - loss: 0.1224 - acc: 0.960 - ETA: 4s - loss: 0.1161 - acc: 0.964 - ETA: 4s - loss: 0.1075 - acc: 0.967 - ETA: 4s - loss: 0.1045 - acc: 0.967 - ETA: 4s - loss: 0.0958 - acc: 0.971 - ETA: 4s - loss: 0.0986 - acc: 0.968 - ETA: 4s - loss: 0.0946 - acc: 0.971 - ETA: 4s - loss: 0.1014 - acc: 0.965 - ETA: 4s - loss: 0.1057 - acc: 0.966 - ETA: 3s - loss: 0.1008 - acc: 0.969 - ETA: 3s - loss: 0.1019 - acc: 0.968 - ETA: 3s - loss: 0.0997 - acc: 0.970 - ETA: 3s - loss: 0.1124 - acc: 0.964 - ETA: 3s - loss: 0.1114 - acc: 0.964 - ETA: 3s - loss: 0.1187 - acc: 0.962 - ETA: 3s - loss: 0.1251 - acc: 0.958 - ETA: 3s - loss: 0.1297 - acc: 0.955 - ETA: 3s - loss: 0.1282 - acc: 0.956 - ETA: 2s - loss: 0.1260 - acc: 0.956 - ETA: 2s - loss: 0.1268 - acc: 0.954 - ETA: 2s - loss: 0.1274 - acc: 0.953 - ETA: 2s - loss: 0.1240 - acc: 0.954 - ETA: 2s - loss: 0.1255 - acc: 0.954 - ETA: 2s - loss: 0.1256 - acc: 0.954 - ETA: 2s - loss: 0.1257 - acc: 0.953 - ETA: 2s - loss: 0.1251 - acc: 0.953 - ETA: 2s - loss: 0.1254 - acc: 0.953 - ETA: 2s - loss: 0.1230 - acc: 0.954 - ETA: 1s - loss: 0.1227 - acc: 0.955 - ETA: 1s - loss: 0.1228 - acc: 0.955 - ETA: 1s - loss: 0.1215 - acc: 0.955 - ETA: 1s - loss: 0.1194 - acc: 0.956 - ETA: 1s - loss: 0.1187 - acc: 0.956 - ETA: 1s - loss: 0.1196 - acc: 0.956 - ETA: 1s - loss: 0.1197 - acc: 0.956 - ETA: 1s - loss: 0.1191 - acc: 0.956 - ETA: 1s - loss: 0.1190 - acc: 0.956 - ETA: 1s - loss: 0.1201 - acc: 0.956 - ETA: 0s - loss: 0.1193 - acc: 0.956 - ETA: 0s - loss: 0.1194 - acc: 0.956 - ETA: 0s - loss: 0.1191 - acc: 0.956 - ETA: 0s - loss: 0.1232 - acc: 0.955 - ETA: 0s - loss: 0.1223 - acc: 0.955 - ETA: 0s - loss: 0.1222 - acc: 0.955 - ETA: 0s - loss: 0.1244 - acc: 0.954 - ETA: 0s - loss: 0.1255 - acc: 0.953 - ETA: 0s - loss: 0.1263 - acc: 0.953 - 5s 105ms/step - loss: 0.1268 - acc: 0.9534\n",
      "Epoch 128/10000\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1250 - acc: 0.954 - ETA: 5s - loss: 0.0979 - acc: 0.960 - ETA: 5s - loss: 0.1036 - acc: 0.954 - ETA: 5s - loss: 0.1006 - acc: 0.954 - ETA: 4s - loss: 0.1137 - acc: 0.947 - ETA: 4s - loss: 0.1169 - acc: 0.945 - ETA: 4s - loss: 0.1133 - acc: 0.949 - ETA: 4s - loss: 0.1110 - acc: 0.950 - ETA: 4s - loss: 0.1088 - acc: 0.952 - ETA: 4s - loss: 0.1110 - acc: 0.952 - ETA: 4s - loss: 0.1102 - acc: 0.953 - ETA: 3s - loss: 0.1090 - acc: 0.954 - ETA: 3s - loss: 0.1104 - acc: 0.954 - ETA: 3s - loss: 0.1200 - acc: 0.948 - ETA: 3s - loss: 0.1314 - acc: 0.945 - ETA: 3s - loss: 0.1321 - acc: 0.946 - ETA: 3s - loss: 0.1297 - acc: 0.947 - ETA: 3s - loss: 0.1374 - acc: 0.946 - ETA: 3s - loss: 0.1378 - acc: 0.946 - ETA: 3s - loss: 0.1379 - acc: 0.947 - ETA: 2s - loss: 0.1335 - acc: 0.949 - ETA: 2s - loss: 0.1412 - acc: 0.947 - ETA: 2s - loss: 0.1394 - acc: 0.948 - ETA: 2s - loss: 0.1356 - acc: 0.949 - ETA: 2s - loss: 0.1317 - acc: 0.951 - ETA: 2s - loss: 0.1307 - acc: 0.951 - ETA: 2s - loss: 0.1286 - acc: 0.952 - ETA: 2s - loss: 0.1271 - acc: 0.953 - ETA: 2s - loss: 0.1261 - acc: 0.953 - ETA: 2s - loss: 0.1274 - acc: 0.952 - ETA: 1s - loss: 0.1276 - acc: 0.952 - ETA: 1s - loss: 0.1261 - acc: 0.952 - ETA: 1s - loss: 0.1292 - acc: 0.951 - ETA: 1s - loss: 0.1277 - acc: 0.952 - ETA: 1s - loss: 0.1275 - acc: 0.952 - ETA: 1s - loss: 0.1279 - acc: 0.952 - ETA: 1s - loss: 0.1311 - acc: 0.951 - ETA: 1s - loss: 0.1336 - acc: 0.950 - ETA: 1s - loss: 0.1324 - acc: 0.951 - ETA: 0s - loss: 0.1332 - acc: 0.950 - ETA: 0s - loss: 0.1309 - acc: 0.951 - ETA: 0s - loss: 0.1290 - acc: 0.952 - ETA: 0s - loss: 0.1285 - acc: 0.953 - ETA: 0s - loss: 0.1285 - acc: 0.953 - ETA: 0s - loss: 0.1285 - acc: 0.953 - ETA: 0s - loss: 0.1283 - acc: 0.953 - ETA: 0s - loss: 0.1280 - acc: 0.953 - ETA: 0s - loss: 0.1276 - acc: 0.954 - 5s 106ms/step - loss: 0.1288 - acc: 0.9545\n",
      "Epoch 129/10000\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "49/49 [==============================] - ETA: 4s - loss: 0.1038 - acc: 0.954 - ETA: 4s - loss: 0.1200 - acc: 0.960 - ETA: 4s - loss: 0.0975 - acc: 0.965 - ETA: 4s - loss: 0.0978 - acc: 0.963 - ETA: 4s - loss: 0.1126 - acc: 0.952 - ETA: 4s - loss: 0.1219 - acc: 0.950 - ETA: 4s - loss: 0.1121 - acc: 0.956 - ETA: 4s - loss: 0.1191 - acc: 0.954 - ETA: 4s - loss: 0.1295 - acc: 0.952 - ETA: 4s - loss: 0.1327 - acc: 0.951 - ETA: 3s - loss: 0.1298 - acc: 0.953 - ETA: 3s - loss: 0.1266 - acc: 0.955 - ETA: 3s - loss: 0.1207 - acc: 0.958 - ETA: 3s - loss: 0.1159 - acc: 0.960 - ETA: 3s - loss: 0.1117 - acc: 0.962 - ETA: 3s - loss: 0.1092 - acc: 0.963 - ETA: 3s - loss: 0.1169 - acc: 0.961 - ETA: 3s - loss: 0.1191 - acc: 0.960 - ETA: 3s - loss: 0.1218 - acc: 0.958 - ETA: 3s - loss: 0.1238 - acc: 0.958 - ETA: 2s - loss: 0.1248 - acc: 0.957 - ETA: 2s - loss: 0.1271 - acc: 0.957 - ETA: 2s - loss: 0.1245 - acc: 0.959 - ETA: 2s - loss: 0.1221 - acc: 0.958 - ETA: 2s - loss: 0.1192 - acc: 0.959 - ETA: 2s - loss: 0.1205 - acc: 0.959 - ETA: 2s - loss: 0.1288 - acc: 0.955 - ETA: 2s - loss: 0.1292 - acc: 0.955 - ETA: 2s - loss: 0.1279 - acc: 0.956 - ETA: 1s - loss: 0.1249 - acc: 0.957 - ETA: 1s - loss: 0.1247 - acc: 0.957 - ETA: 1s - loss: 0.1238 - acc: 0.958 - ETA: 1s - loss: 0.1226 - acc: 0.959 - ETA: 1s - loss: 0.1214 - acc: 0.959 - ETA: 1s - loss: 0.1210 - acc: 0.959 - ETA: 1s - loss: 0.1219 - acc: 0.958 - ETA: 1s - loss: 0.1239 - acc: 0.957 - ETA: 1s - loss: 0.1224 - acc: 0.957 - ETA: 1s - loss: 0.1222 - acc: 0.958 - ETA: 0s - loss: 0.1225 - acc: 0.957 - ETA: 0s - loss: 0.1220 - acc: 0.957 - ETA: 0s - loss: 0.1214 - acc: 0.958 - ETA: 0s - loss: 0.1197 - acc: 0.958 - ETA: 0s - loss: 0.1191 - acc: 0.958 - ETA: 0s - loss: 0.1194 - acc: 0.958 - ETA: 0s - loss: 0.1202 - acc: 0.958 - ETA: 0s - loss: 0.1217 - acc: 0.956 - ETA: 0s - loss: 0.1239 - acc: 0.956 - 5s 105ms/step - loss: 0.1259 - acc: 0.9550\n",
      "Epoch 130/10000\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - ETA: 4s - loss: 0.1284 - acc: 0.954 - ETA: 4s - loss: 0.1693 - acc: 0.948 - ETA: 4s - loss: 0.1585 - acc: 0.954 - ETA: 4s - loss: 0.1711 - acc: 0.940 - ETA: 4s - loss: 0.1641 - acc: 0.940 - ETA: 4s - loss: 0.1591 - acc: 0.939 - ETA: 4s - loss: 0.1547 - acc: 0.939 - ETA: 4s - loss: 0.1580 - acc: 0.938 - ETA: 4s - loss: 0.1516 - acc: 0.941 - ETA: 4s - loss: 0.1517 - acc: 0.940 - ETA: 3s - loss: 0.1513 - acc: 0.940 - ETA: 3s - loss: 0.1508 - acc: 0.938 - ETA: 3s - loss: 0.1466 - acc: 0.941 - ETA: 3s - loss: 0.1459 - acc: 0.938 - ETA: 3s - loss: 0.1494 - acc: 0.935 - ETA: 3s - loss: 0.1459 - acc: 0.938 - ETA: 3s - loss: 0.1452 - acc: 0.937 - ETA: 3s - loss: 0.1441 - acc: 0.937 - ETA: 3s - loss: 0.1426 - acc: 0.939 - ETA: 2s - loss: 0.1462 - acc: 0.939 - ETA: 2s - loss: 0.1470 - acc: 0.939 - ETA: 2s - loss: 0.1473 - acc: 0.939 - ETA: 2s - loss: 0.1487 - acc: 0.938 - ETA: 2s - loss: 0.1465 - acc: 0.939 - ETA: 2s - loss: 0.1460 - acc: 0.940 - ETA: 2s - loss: 0.1445 - acc: 0.940 - ETA: 2s - loss: 0.1446 - acc: 0.941 - ETA: 2s - loss: 0.1408 - acc: 0.943 - ETA: 2s - loss: 0.1412 - acc: 0.942 - ETA: 1s - loss: 0.1447 - acc: 0.941 - ETA: 1s - loss: 0.1416 - acc: 0.943 - ETA: 1s - loss: 0.1459 - acc: 0.943 - ETA: 1s - loss: 0.1421 - acc: 0.944 - ETA: 1s - loss: 0.1444 - acc: 0.943 - ETA: 1s - loss: 0.1445 - acc: 0.943 - ETA: 1s - loss: 0.1449 - acc: 0.943 - ETA: 1s - loss: 0.1439 - acc: 0.943 - ETA: 1s - loss: 0.1444 - acc: 0.943 - ETA: 1s - loss: 0.1447 - acc: 0.942 - ETA: 0s - loss: 0.1458 - acc: 0.941 - ETA: 0s - loss: 0.1450 - acc: 0.942 - ETA: 0s - loss: 0.1445 - acc: 0.942 - ETA: 0s - loss: 0.1425 - acc: 0.943 - ETA: 0s - loss: 0.1433 - acc: 0.943 - ETA: 0s - loss: 0.1419 - acc: 0.943 - ETA: 0s - loss: 0.1427 - acc: 0.942 - ETA: 0s - loss: 0.1438 - acc: 0.941 - ETA: 0s - loss: 0.1427 - acc: 0.942 - 5s 103ms/step - loss: 0.1413 - acc: 0.9430\n",
      "Epoch 131/10000\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.00020971521735191345.\n",
      "40/49 [=======================>......] - ETA: 5s - loss: 0.1634 - acc: 0.931 - ETA: 4s - loss: 0.1515 - acc: 0.943 - ETA: 4s - loss: 0.1402 - acc: 0.950 - ETA: 4s - loss: 0.1221 - acc: 0.957 - ETA: 4s - loss: 0.1297 - acc: 0.954 - ETA: 4s - loss: 0.1324 - acc: 0.952 - ETA: 4s - loss: 0.1266 - acc: 0.954 - ETA: 4s - loss: 0.1257 - acc: 0.953 - ETA: 4s - loss: 0.1185 - acc: 0.957 - ETA: 4s - loss: 0.1173 - acc: 0.956 - ETA: 3s - loss: 0.1137 - acc: 0.958 - ETA: 3s - loss: 0.1165 - acc: 0.955 - ETA: 3s - loss: 0.1148 - acc: 0.957 - ETA: 3s - loss: 0.1113 - acc: 0.959 - ETA: 3s - loss: 0.1093 - acc: 0.959 - ETA: 3s - loss: 0.1056 - acc: 0.961 - ETA: 3s - loss: 0.1099 - acc: 0.962 - ETA: 3s - loss: 0.1102 - acc: 0.962 - ETA: 3s - loss: 0.1094 - acc: 0.963 - ETA: 2s - loss: 0.1079 - acc: 0.964 - ETA: 2s - loss: 0.1056 - acc: 0.964 - ETA: 2s - loss: 0.1048 - acc: 0.964 - ETA: 2s - loss: 0.1107 - acc: 0.962 - ETA: 2s - loss: 0.1166 - acc: 0.960 - ETA: 2s - loss: 0.1162 - acc: 0.959 - ETA: 2s - loss: 0.1154 - acc: 0.960 - ETA: 2s - loss: 0.1180 - acc: 0.959 - ETA: 2s - loss: 0.1189 - acc: 0.959 - ETA: 2s - loss: 0.1193 - acc: 0.958 - ETA: 1s - loss: 0.1217 - acc: 0.958 - ETA: 1s - loss: 0.1277 - acc: 0.955 - ETA: 1s - loss: 0.1274 - acc: 0.955 - ETA: 1s - loss: 0.1292 - acc: 0.955 - ETA: 1s - loss: 0.1307 - acc: 0.955 - ETA: 1s - loss: 0.1292 - acc: 0.956 - ETA: 1s - loss: 0.1290 - acc: 0.957 - ETA: 1s - loss: 0.1269 - acc: 0.957 - ETA: 1s - loss: 0.1261 - acc: 0.958 - ETA: 1s - loss: 0.1275 - acc: 0.957 - ETA: 0s - loss: 0.1300 - acc: 0.9568"
     ]
    }
   ],
   "source": [
    "results = model.fit_generator(image_generator, epochs=10000, callbacks=[checkpointer, reduce_lr],\n",
    "                              steps_per_epoch=x.shape[0]//BATCH_SIZE, class_weight=w,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveweights('vgg_classifier_27.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.argmax(model.predict(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
